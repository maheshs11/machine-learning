 1/1: import numpy as np
 1/2: print a
 1/3: print(a)
 1/4: a = 7
 1/5: print(a)
 1/6: float(7)
 1/7: float(a)
 1/8: a.dtype
 1/9: dtype(a)
1/10: def square(a):
1/11:
def square(a):
    return a*a
1/12: sauare(2)
1/13: square(2)
1/14: age = [23,45,27,4,6]
1/15:
age = [23,45,27,4,6]
for i in age:
    if age%2==0:
        print("even")
1/16:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even")
1/17:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even",i)
1/18:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even",i)
    elif:print("odd")
1/19:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even",i)
    elif:
        print("odd")
1/20:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even",i)
    elif i%2!=0
        print("odd")
1/21:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even",i)
    elif i%2!=0:
        print("odd")
1/22:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even",i)
    elif i%2!=0:
        print("odd",i)
1/23:
age = [23,45,27,4,6]
for i in age:
    if i%2==0:
        print("even",i)
    elif i%2!=0:
        print("odd",i) 
    else:
        print("nothing")
1/24: a=7
1/25: a=8
1/26: print(a)
1/27: a=9
1/28: print("dnsdjnksjnd", a)
1/29: print("bdkss" +a)
1/30: print("ufndfn"+str(a))
1/31: print("dsfsd"a)
1/32:
print("mfmskof" a
     )
1/33: print("jjfsfs" a)
1/34:
print("kmms", a
     )
1/35:
x=1
type(x)
1/36:
x=0.1
type(x)
1/37:
b1=true
type(b1)
1/38:
b1 = true
type(b1)
1/39:
b1=True
type(b1)
1/40:
name1 = 'snsnsnsndj'
type(name1)
1/41:
u= 1-2j
j
1/42:
u=1-2j
u
1/43: u
1/44: a
1/45: x
1/46: print(x.real , x.imag)
1/47: ten = ten
1/48: ten = 'ten'
1/49: ten
1/50: ten = "ten"
1/51: ten
1/52: eat = "eat"
1/53: eat
1/54: True is True
1/55: False is False
1/56: True is False
1/57:
type(i
    )
1/58: i.numerator
1/59: i
1/60: i.imag
1/61: i=string
1/62: i=dog
1/63: i = dog
1/64: i='dog'
1/65: i.Translate
1/66: i="dog"
1/67: i.translate
1/68: i.capitalize
1/69: i.translate()
1/70: i.translate(table,/)
1/71: i.imag
1/72: a.imag
1/73: i=10
1/74: i
 3/1: runfile('C:/Users/Admin/.spyder-py3/temp.py', wdir='C:/Users/Admin/.spyder-py3')
1/75: int i = 2
1/76:
id(k
  )
1/77: id(i)
1/78: id i
1/79: True+False
1/80: True+ True
1/81: ta = """"fag'""""
1/82: ta = """"fag'a"""
1/83: ta = """"fag'a""""
1/84: '1'+asr
1/85:
"1"+'a
'
1/86: "1"+"q"
1/87: mahesh * 10
1/88: "mahesh" * 10
1/89: 3**2
1/90: id(q)
1/91: id(i)
1/92: str(asdss)+str(fff)
1/93: str('asdss')+str('fff')
1/94: str(2)+str(22)
1/95: 's'+str(fff)
1/96: 's'+str(2)
1/97: print("""" "home""day" """)
1/98: print(""" "home""day" ")
1/99: print(""" "home""day" "")
1/100: print("" "home""day" "")
1/101: True - True
1/102: id(True)
1/103: id(True)
1/104: id(140735687944016)
1/105: type(140735687944016)
1/106: type(id(True))
1/107: type(True)
1/108: True / False
1/109:
i = 10
if i==9:
    pass
1/110:
if i ==9:
    print{"haha"}
1/111:
if i ==9:
    print("haha")
1/112:
if i ==10:
    print("haha")
1/113:
if i<5:
    print('this is false')
elif i<9:
    print('this is also false')
else:
    print('this is default')
1/114: True or True
1/115: True or False
1/116:
i = 7
id(i)
1/117:
i = 8
id(i)
1/118:
s = "mahesh"
if True:
    if s == "mahesh"
    print("both are same")
else:
    print("not same")
1/119:
s = "mahesh"
if True:
    if s == "mahesh":
    print("both are same")
else:
    print("not same")
1/120:
s = "mahesh"
if True:
    if s == "mahesh":
    print("both are same")
else :
    print("not same")
1/121:
s = "mahesh"
if True:
    if s == "mahesh":
    print("both are same")
else :
    print("not same")
1/122:
s = "mahesh"
if True:
    if s == "mahesh":
    print("both are same")
    else :
    print("not same")
1/123:
s = "mahesh"
if True:
    if s == "mahesh":
    print("both are same")
    else :
    print("not same")
1/124:
s = "mahesh"
if True:
    if s == "mahesh":
        print("both are same")
    else :
    print("not same")
1/125:
s = "mahesh"
if True:
    if s == "mahesh":
        print("both are same")
    else :
        print("not same")
1/126:
s = "mahesh"
if False:
    if s == "mahesh":
        print("both are same")
    else:
        print("not same")
1/127:
s = "mahesh"

    if s == "mahesh":
        print("both are same")
    else:
        print("not same")
1/128:
s = "mahesh"
    if s == "mahesh":
        print("both are same")
    else:
        print("not same")
1/129:
s = "mahesh"
if s == "mahesh":
        print("both are same")
else:
        print("not same")
1/130:
s = "mahesh"
if s == "mahesh":
    pass
        print("both are same")
else:
        print("not same")
1/131:
s = "mahesh"
if s == "mahesh":
       pass
        print("both are same")
else:
        print("not same")
1/132: input()
1/133: type( )
1/134: id( )
1/135: k = [2,"s","d",34]
1/136:
k = [2,"s","d",34]
k
1/137: k[2]
1/138: type(k[2])
1/139:
k = [2,4,6,8,10,12,14,16,18]
k[2:4]
1/140:
k = [2,4,6,8,10,12,14,16,18]
k[2:4:2]
1/141:
k = [2,4,6,8,10,12,14,16,18]
k[2:8:2]
1/142:
k = [2,4,6,8,10,12,14,16,18]
k[2:8:3]
1/143:
k = [2,4,6,8,10,12,14,16,18]
k[:8:3]
1/144:
k = [2,4,6,8,10,12,14,16,18]
k[8:4:3]
1/145:
k = [2,4,6,8,10,12,14,16,18]
k[8:4:1]
1/146:
k = [2,4,6,8,10,12,14,16,18]
k[8:4:-1]
1/147: k[::-1]
1/148: k
1/149: k[-1]
1/150:
m = [mahesh]
m[::-1]
1/151:
m = ['mahesh]
m[::-1]
1/152:
m = ['mahesh']
m[::-1]
1/153:
m = ['m''a'h'e's''h']
m[::-1]
1/154:
m = ['m','a','h','e','s'.'h']
m[::-1]
1/155:
m = ['m','a','h','e','s','h']
m[::-1]
1/156:
o="mahesh"
o[::-1]
1/157:
o="mahesh'dance"
o[::-1]
1/158: len(m)
1/159: k = 2,3,4
1/160: type(k)
1/161: k = 2,3,
1/162: type(k)
1/163: type(k)
1/164: k = 2,
1/165: type(k)
1/166: k = 2
1/167: type(k)
1/168: k = 2,
1/169: type(k)
1/170:
for i in k:
    if i == type(k)
    print(i)
1/171:
for i in k:
    if i == type(k):
    print(i)
1/172:
for i in k:
    if str == type(k):
    print(i)
1/173:
for i in k:
    if str == type(k):
        print(i)
1/174:
for i in m:
    if str == type(m):
        print(i)
1/175:
for i in m:
    if str == type(m):
        print(i)
1/176:
for i in m:
    if str == type(m):
        print(i)
1/177:
m = ['m','a','h','e','s','h']
for i in m:
    if str == type(m):
        print(i)
1/178:
m = ['mah','a','h','e','s','h']
for i in m:
    if str == type(m):
        print(i)
1/179:
m = ['mah','a','h','e','s','h']
for i in m:
    b =[]
    if str == type(m):
        print(i)
        append[b]
1/180:
m = ['mah','a','h','e','s','h']
for i in m:
    b =[]
    if str == type(m):
        print(i)
        append[b]
1/181:
m = ['mah','a','h','e','s','h']
for i in m:
    b =[]
    if str == type(m):
        print('i')
        append[b]
1/182:
m = ['mah','a','ha','e','s','h']
for i in m:
    b =[]
    if str == type(m[1]):
        print('i')
        append[b]
1/183: k = 'jdhch',
1/184:
for i in m:
    if str == type(k):
        print(true)
1/185:
k="huhuh"
for i in m:
    if str == type(k):
        print(true)
1/186:
k="huhuh"
for i in m:
    if str == type(k):
        print('true')
1/187:
k="huhuh"
for i in m:
if str == type(k):
        print('true')
1/188:
k="huhuh"
for i in m:
  if str == type(k):
        print('true')
1/189:
k="huhuh"
for i in m:
    if str == type(k):
        print('true')
1/190:
k="h"
for i in m:
    if str == type(k):
        print('true')
1/191:
k="h"
for i in m:
    if str == type(k):
        print('true')
1/192:
k="ha"
for i in m:
    if str == type(k):
        print('true')
 4/1:
m = list(mahesh)
m
 4/2:
m = list("mahesh)
m
 4/3:
m = list("mahesh")
m
 4/4: m[0:3]
 4/5: m[0:10]
 4/6:
m = list(["mahesh"])
m
 4/7: m[0:3]
 4/8: m[0:10]
 4/9: m[0:3]
4/10: m[0:3]
4/11:
m = list("mahesh")
m
4/12: m[0:10]
4/13: input[]
4/14: input i[]
4/15: input()
4/16: input([])
4/17: input([])
4/18: input()
4/19: m = "mahesh"
4/20: m[-1]
4/21: m[0:8:-1]
4/22: m[0:5:-1]
4/23: m[0:3:-1]
4/24:
m[0:3
 ]
4/25:
m[3:0
 ]
4/26: m[3:0:-1 ]
4/27: m[0:3:1 ]
4/28: m[10:3:1 ]
4/29: m[10:3:-1 ]
4/30: m[10:0:-1 ]
4/31: m[10:-1:-1 ]
4/32: m[8:-1:-1 ]
4/33: m[5:-1:-1 ]
4/34: m[5:0:-1 ]
4/35: m[0:5:-1 ]
4/36: m[0:5:1 ]
4/37:
k = []
k.append('are')
k.append('bad')
k.append('cat')
k.insert[1,'you']
4/38:
k = []
k.append('are')
k.append('bad')
k.append('cat')
k.insert(1,'you')
4/39: k
4/40:
k = []
k.append('are')
k.append('bad')
k.append('cat')
k.insert(0,'you')
4/41: k
4/42:
k = []
k.append('are')
#k.append('bad')
k.append('cat')
k.insert(0,'you')
4/43: k
4/44:
k = []
k.append('are')
k.append('bad')
k.append('cat')
k.insert(0,'you')
4/45:
k = []
k.append('are')
k.append('bad')
k.append('cat')
k.insert(0,'you')
k
4/46:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
0[1]
4/47:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
0 [1]
4/48:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
0
4/49:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
o
4/50:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
o[4
 ]
4/51:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
o[1][1]
4/52:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
o[6][0]
4/53:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
o[5][0]
4/54:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
list(o[5][0])
4/55:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
list(o[5])
4/56:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
list(o)
4/57:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
list(o[2])
4/58:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
list(o[1])
4/59:
o = ['asd', [12,2,2],'sjsjs',1,2,[1.2,'ajsj']]
list(o[5])
4/60: m = 'mahesh'
4/61: m[::-1]
4/62: m[::1]
4/63: m
4/64: m
4/65: m[::-1]
4/66: m[6::-1]
4/67: list(2)
4/68: list(2,2,4)
4/69:
k = []
k.append('are')
k.append('bad')
#k.append('cat')
k.insert(0,'you')
k
4/70:
k = []
#k.append('are')
k.append('bad')
#k.append('cat')
k.insert(0,'you')
k
4/71:
k = []
#k.append('are')
#k.append('bad')
#k.append('cat')
k.insert(0,'you')
k
4/72:
k = []
#k.append('are')
#k.append('bad')
#k.append('cat')
k.insert(1,'you')
k
4/73:
k = []
k.append('are')
#k.append('bad')
#k.append('cat')
k.insert(1,'you')
k
4/74:
k = []
k.append('are')
k.append('bad')
#k.append('cat')
k.insert(1,'you')
k
4/75:
k = []
k.append('are')
k.append('bad')
#k.append('cat')
.insert(1,'you')
k
4/76:
k = []
k.append('are')
k.append('bad')
k.append('cat')
.insert(1,'you')
k
4/77:
k = []
k.append('are')
k.append('bad')
k.append('cat')
K.insert(1,'you')
k
4/78:
k = []
k.append('are')
k.append('bad')
k.append('cat')
K.insert(1,'you')
k
4/79:
k = []
k.append('are')
k.append('bad')
k.append('cat')
K.insert(0,'you')
k
 5/1:
k = []
k.append('are')
k.append('bad')
k.append('cat')
K.insert(0,'you')
k
 5/2:
k = []
k.append('are')
k.append('bad')
k.append('cat')
K.insert(0,'you')
print(k)
 5/3:
k = []
k.append('are')
k.append('bad')
k.append('cat')
K.insert(0,'you')
print('
      k)
 5/4: k
 5/5: k
 5/6:
for i in m:
    print(i)
 5/7:
m = 'mahesh'
for i in m:
    print(i)
 5/8:
m = 'mahesh'
for i in 'sjd2367':
    print(i)
 5/9:
m = 'mahesh'
for i in [1,2,'d''djdj]:
    print(i)
5/10:
m = 'mahesh'
for i in [1,2,'d''djdj;]:
    print(i)
5/11:
m = 'mahesh'
for i in [1,2,'d''djdj;]:
    print(i)
5/12:
m = 'mahesh'
for i in [1,2,'d''djdj']:
    print(i)
5/13:
m = 'mahesh'
for i in [1,2,'d','djdj']:
    print(i)
5/14:
m = 'mahesh'
for i in [1,2,'d','djdj']:
    print(type(i)
5/15:
m = 'mahesh'
for i in [1,2,'d','djdj']:
    print(type(i))
5/16:
m = 'mahesh'
for i in [1,2,'d','djdj']:
    if type of i == str:
        print(i)
5/17:
m = 'mahesh'
for i in [1,2,'d','djdj']:
    if type(i) == str:
        print(i)
5/18:
m = 'mahesh'
for i in [1,2,'d','djdj']:
    if type(i) == str:
        print(i[::-1])
5/19:
m = 'mahesh'
for i in [1,2,'d','batmanj']:
    if type(i) == str:
        print(i[::-1])
5/20:
m = 'mahesh'
for i in (1,2,'d','batmanj'):
    if type(i) == str:
        print(i[::-1])
5/21:
m = 'mahesh'
for i in (1,2,'d','batmanj'):
    if type(i) == str:
        print(i[::-1])
5/22:
m = 'mahesh'
for i in [1,2,'d','[batmanj']]:
    if type(i) == str:
        print(i[::-1])
5/23:
m = 'mahesh'
for i in [1,2,'d',['batmanj']]:
    if type(i) == str:
        print(i[::-1])
5/24:
m = 'mahesh'
for i in [1,2,'d',['batmanj']]:
    if type(i) == list:
        print(i[::-1])
5/25:
m = 'mahesh'
for i in [1,2,'d',['batmanj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            print(::-1)
5/26:
m = 'mahesh'
for i in [1,2,'d',['batmanj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            print(j[::-1])
5/27:
m = 'mahesh'
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            print(j[::-1])
5/28:
m = 'mahesh'
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            j.append[::-1])
5/29:
m = 'mahesh'
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            j.append(::-1)
5/30:
m = 'mahesh'
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            j.append(j.[::-1])
5/31:
m = 'mahesh'
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            j.append(j[::-1])
5/32:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in (i[::-1]):
            m.append(m[::-1])
5/33:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in i[::-1]:
            m.append(m[::-1])
5/34:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in i[::-1]:
            m.append(m[::-1])
            m
5/35:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in i[::-1]:
            m.append(m[::-1])
            m
5/36:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in i[::-1]:
            m.append(m[::-1])
m
5/37:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for j in i[::-1]:
            m.append(m[::-1])
m 
m
5/38:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for m in i[::-1]:
            m.append(m[::-1])
m 
m
5/39:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for m in i[::-1]:
            m.append(i[::-1])
m 
m
5/40:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for m in i[::-1]:
            m.append(i[::-1])
m 
m
5/41:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for m in i[::-1]:
            m.append(i[::-1])
m
5/42:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for k in i[::-1]:
            m.append(k[::-1])
m
5/43:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        print(i[::-1])
        for k in i[::-1]:
            print(k)
            m.append(k[::-1])
m
5/44: m = 'mahesh'
5/45: m.Reverse
5/46: m.capitalize
5/47: m.capitalize()
5/48: print('mahesh',end='\n')
5/49:
print('mahesh',end='\t
')
5/50: print('mahesh',end='\t')
5/51:
print('mahesh', 
      end='\t')
5/52:
print('mahesh', end='\ 
t')
5/53: print('mahesh', end='\ t')
5/54: print('mahesh', end='\t')
5/55: print('mahesh', end='\tteight')
5/56: print('mahesh', end='')
5/57:
for i in 0 to 100:
    print(i)
5/58:
for i in 100:
    print(i)
5/59:
for i in '100':
    print(i)
5/60:
for i in [100]:
    print(i)
5/61:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        
        for k in i[::-1]:
         
            m.append(k[::-1])
m
5/62:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        
        for k in i[::1]:
         
            m.append(k[::-1])
m
5/63:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        
        for k in i[::-1]:
         
            m.append(k[::-1])
m
5/64:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        
        for k in i[::1]:
         
            m.append(k[::-1])
m
5/65:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        
        for k in i[::-1]:
         
            m.append(k[::-1])
m
5/66:
sum = 0
for i in [1,2,3,4,5]:
    sum = sum + i
    print(i)
5/67:
sum = 0
for i in [1,2,3,4,5]:
    sum = sum + i
print(i)
5/68:
sum = 0
for i in [1,2,3,4,5]:
    sum = sum+i
print(i)
5/69:
sum = 0
for i in [1,2,3,4,5]:
    sum = sum+i
print(sum)
5/70:
sum = 0
for i in 1,2,3,4,5:
    sum = sum+i
print(sum)
5/71:
sum = 0
for i in 1,2,3,4,5:
    sum = sum+i
print(sum)
5/72:
sum = 0
for i in 5:
    sum = sum+i
print(sum)
5/73:
sum = 0
for i in 3,5:
    sum = sum+i
print(sum)
5/74:
sum = 0
for i in 3-5:
    sum = sum+i
print(sum)
5/75:
sum = 0
for i in 3to5:
    sum = sum+i
print(sum)
5/76:
sum = 0
for i in 3<5:
    sum = sum+i
print(sum)
5/77:
sum = 0
for i in 3<>5:
    sum = sum+i
print(sum)
5/78:
sum = 0
for i in 3,5:
    sum = sum+i
print(sum)
5/79:
sum = 0
numbers= input[]
for i in numbers:
    sum = sum+i
print(sum)
5/80:
sum = 0
numbers= input[()
for i in numbers:
    sum = sum+i
print(sum)
5/81:
sum = 0
numbers= input[()
for i in numbers:
    sum = sum+i
print(sum)
5/82:
sum = 0
numbers = input[()
for i in numbers:
    sum = sum+i
print(sum)
5/83:
sum = 0
numbers = input()
for i in numbers:
    sum = sum+i
print(sum)
5/84:
sum = 0
numbers = input()
for i in numbers:
    sum = sum+i
print(sum)
5/85:
sum = 0
numbers = input()
for i in numbers:
    sum = sum+i
print(sum)
5/86:
sum = 0
numbers = [1,2,3,4,5]
for i in numbers:
    sum = sum+i
print(sum)
5/87:
v = 10
while v < 10 :
    v++
    print(v)
5/88:
v = 10
while v < 10 :
    v+
    print(v)
5/89:
v = 10
while v < 10 :
    v = v++
    print(v)
5/90:
v = 10
while v < 10 :
    v = v++
    print(v)
5/91:
v = 10
while v < 10 :
    v = v+1
    print(v)
5/92:
v = 10
while v < 10 :
    v = v+1
    print(v)
5/93:
v = 1
while v < 10 :
    v = v+1
    print(v)
5/94:
v = -1
while v < 10 :
    v = v+1
    print(v)
5/95:
v = -2
while v < 10 :
    v = v+1
    print(v)
5/96:
v = 0
while v < 10 :
    v = v+1
    print(v)
5/97:
v = 0
while v < 10 :
    print(v)
    v = v+1
    print(v)
5/98:
v = 0
while v < 10 :
    print(v,end='\n')
    v = v+1
    print(v)
5/99:
v = 0
while v < 10 :
    print(v,end='\t')
    v = v+1
    print(v)
5/100:
v = 0
while v < 10 :
    print(v,end='\t')
    v = v+1
print(v)
5/101:
v = 0
while v < 10 :
print(v,end='\t')
    v = v+1
print(v)
5/102:
v = 0
while v < 10 :
    print(v,end='\t')
    v = v+1
print(v)
5/103:
v = 0
while v < 10 :
   
    v = v+1
print(v)
5/104:
v = 0
while v < 10 :
   
    v = v+1
    print(v)
5/105:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(8,0,-1):
    print(i)
5/106:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(l(8,0,-1)):
    print(i)
5/107:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(l[8,0,-1]):
    print(i)
5/108:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l),-1,-1):
    print(i)
5/109:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l),-1,-1):
    print(m[i])
5/110:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l),-1,-1):
    m.append(l[i])
5/111:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l),-1,-1):
    m.append(l[i])
print(m)
5/112:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
5/113:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m[::-1])
5/114:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m[::-1])
5/115:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m[::-1])
5/116:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i]-1)
print(m[::-1])
5/117:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
5/118:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m[::-1]:
k
5/119:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m[::-1]:
5/120:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m[::-1]:
    pass
5/121:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m[::-1]:
    m.append(l[i])
5/122:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m[::-1]:
    m.append(l[i])
5/123:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj']]:
    if type(i) == list:
        
        for k in i[::1]:
         
            m.append(k[::-1])
m
5/124:
m = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m[::-1]:
    m.append(k[::-1])
5/125:
m = []
s = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m[::-1]:
    s.append(k[::-1])
5/126:
m = []
s = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m
    s.append(k[::-1])
5/127:
m = []
s = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m
    s.append(m[::-1])
5/128:
m = []
s = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m:
    s.append(m[::-1])
5/129:
m = []
s = []
l = ['mdadeg',2,3,5,7,'ddre','rre']
for i in range(len(l)-1,-1,-1):
    m.append(l[i])
print(m)
for k in m:
    s.append(m[::-1]) 
s
5/130:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj',2]]:
    if type(i) == list:
        
        for k in i[::1]:
         
            m.append(k[::-1])
m
5/131:
m = []
for i in [1,2,'d',['batmanj','sjsj','sjsjsj''ksjsjsj',2,3]]:
    if type(i) == list:
        
        for k in i[::1]:
         
            m.append(k[::-1])
m
5/132:
t =['mahesh', 'suresh']
s.append[t]
s
5/133:
t =['mahesh', 'suresh']
s = []
s.append[t]
s
5/134:
t =['mahesh', 'suresh']
s = []
s.append[t[0]]
s
5/135:
t =['mahesh', 'suresh']
s = []
s.append[t[0]]
s
5/136:
t = ['mahesh', 'suresh']
s = []
s.append[t[0]]
s
5/137:
l = []
for i in 'mahesh':
    l.append[i]
5/138:
l = []
for i in 'mahesh':
    l.append(i)
5/139:
l = []
for i in 'mahesh':
    l.append(i)
    i
5/140:
l = []
for i in 'mahesh':
    l.append(i)
    i
5/141:
l = []
for i in 'mahesh':
    l.append(i)
i
5/142:
l = []
for i in 'mahesh':
    l.append(i)
l
5/143:
l = []
for i in 'mahesh':
    l.append(i[::-1])
l
5/144:
l = []
for i in 'mahesh':
    l.append(i[::-1])
l
5/145:
l = []
for i in ['sudh','mahesh',3,4,'dog']:
    for j in i[::-1]:
        l.append(j[::-1])
l
5/146:
l = []
for i in ['sudh','mahesh',3,4,'dog']:
    type(i)
    for j in i[::-1]:
        l.append(j[::-1])
l
5/147:
l = []
for i in ['sudh','mahesh','dog']:
    type(i)
    for j in i[::-1]:
        l.append(j[::-1])
l
5/148:
l = []
for i in ['sudh','mahesh','dog']:
    type(i)
    for j in i[::]:
        l.append(j[::-1])
l
5/149:
l = []
for i in ['sudh','mahesh','dog']:
    type(i)
    for j in i[::-1]:
        l.append(j[::-1])
l
5/150:
l = []
for i in ['sudh','mahesh','dog']:
    type(i)
    for j in i[::-1]:
        l.append(j[::])
l
5/151:
l = []
for i in ['sudh','mahesh','dog']:
    type(i)
    for j in i[::-1]:
        l.append(j[::])
l
5/152:
l = []
for i in ['sudh','mahesh','dog']:
    type(i)
    for j in i[::-1]:
        l.append(j[::-1])
l
5/153:
l = []
for i in ["sudh","mahesh",'dog']:
    type(i)
    for j in i[::-1]:
        l.append(j[::-1])
l
5/154:
l = []
for i in ["sudh","mahesh","dog"]:
    type(i)
    for j in i[::-1]:
        l.append(j[::-1])
l
5/155:
l = []
for i in ["sudh","mahesh","dog"]:
    type(i)
    for j in i[::-1]:
        print(j)
        l.append(j[::-1])
l
5/156:
l = []
for i in [["sudh","mahesh","dog"]:
    type(i)
    for j in i[::-1]:
        print(j)
        l.append(j[::-1])
l
5/157:
l = []
for i in [["sudh","mahesh","dog"]]:
    type(i)
    for j in i[::-1]:
        print(j)
        l.append(j[::-1])
l
5/158: range(10)
5/159: list(range(10))
5/160: list(range(5,10))
5/161: list(range(10,5))
5/162: list(range(10,5,-1))
5/163:
l = []
for i in [["sudh","mahesh","dog",5]]:
    type(i)
    for j in i[::-1]:
        print(j)
        l.append(j[::-1])
l
5/164:
l = []
for i in [["sudh","mahesh","dog"]]:
    type(i)
    for j in i[::-1]:
        print(j)
        l.append(j[::-1])
l
5/165: list(range(10,5,-2))
5/166: list(range(10,5,-3))
5/167: list(range(10,0,-3))
5/168:
list(range(10,0,-1
          
          ))
5/169:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2)
i
5/170:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2):
i
5/171:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2):
    pass
i
5/172:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2):
    pass i
i
5/173:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2):
       i
5/174:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2):
       i
5/175:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2):
       print(i)
5/176:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),2):
       print(l[i])
5/177:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),0,-1):
       print(l[i])
5/178:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),-1,-1):
       print(l[i])
5/179:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),-1,-1):
       print(l[i])
5/180:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),-1,-2):
       print(l[i])
5/181:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),-1,-1):
       print(l[i])
5/182:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),-1,-1):
       print(i)
5/183:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),0,-1):
       print(i)
5/184:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),0,-1):
       print(l[i])
5/185:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),5,-1):
       print(l[i])
5/186:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),5,1):
       print(l[i])
5/187:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),5,1):
       print(l[i])
5/188:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),5,-1):
       print(l[i])
5/189:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),5,-1):
       print(i)
5/190:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),4,-1):
       print(i)
5/191:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),4,1):
       print(i)
5/192:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),4,-1):
       print(i)
5/193:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),4,-1):
       print(l[0]i)
5/194:
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),0,-1):
    k.append(l[i])
5/195:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),0,-1):
    k.append(l[i])
5/196:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l)-1,0,-1):
    k.append(l[i])
5/197:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l)-1,0,-1):
    k.append(l[i])
5/198:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l)-1,0,-1):
    k.append(l[i])
5/199:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l)-1,0,-1):
    k.append(l[i])
5/200:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),-1):
    k.append(l[i])
5/201:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),-1):
    k.append(l[i])
5/202:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),1):
    k.append(l[i])
5/203:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),1):
    k.append(l[i])
 8/1:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),1):
    k.append(l[i])
 8/2:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),1):
k.append(l[i])
 8/3:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(0,len(l),1):
    pass
k.append(l[i])
 8/4:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),0,-1):
        k.append(l[i])
k
 8/5:
k = []
l = ["sudh", "kumar","abc","xyz",3,56,67]
for i in range(len(l),-1,-1):
        k.append(l[i])
k
 8/6:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l),-1,-1):
        k.append(l[i])
k
 8/7:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
print(k)
 8/8:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,0,-1):
        k.append(l[i])
print(k)
 8/9:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
print(k)
8/10:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1):
        k.append(l[i])
print(k)
8/11:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,,-1):
        k.append(l[i])
print(k)
8/12:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
print(k)
8/13:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i][-1])
print(k)
8/14:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i[::-1]])
print(k)
8/15:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i-1])
print(k)
8/16:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i,-1])
print(k)
8/17:
k = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
print(k)
8/18:
k = [] n=[]
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(k)
8/19:
k = [] 
n=[]
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(k)
8/20:
k = [] 
n=[]
p
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(k)
8/21:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(k)
8/22:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(p)
print(k)
8/23:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(p)
print(k)
8/24:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append([::-1])
print(n)
print(k)
8/25:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(n)
print(k)
8/26:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(i[::-1])
print(n)
print(k)
8/27:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(n)
print(k)
8/28:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(p)
print(k)
8/29:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(p[::-1])
print(l[i])
print(k)
8/30:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(k[::-1])
print(l[i])
print(k)
8/31:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        l[i] = p
        n.append(k[::-1])
print(n)
print(k)
8/32:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        n.append(k[::-1])
print(n)
print(k)
8/33:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        n.append(k[::-1])
print(n)
8/34:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        n.append(k[::-1])
        type(k)
print(n)
8/35:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        n.append(k[::-1])
        print(type(k))
print(n)
8/36:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        n.append(k[::-1])
        print(type(k))
print(k)
8/37:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
n.append(k[::-1])
print(type(k))
print(n)
8/38:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
         n.append(k[::-1])
print(type(k))
print(n)
8/39:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        n.append(k[::-1])
print(type(k))
print(n)
8/40:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        for u in k
        n.append(k[::-1])
print(type(k))
print(n)
8/41:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        for u in k:
        n.append(k[::-1])
print(type(k))
print(n)
8/42:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        for u in k:
        n.append(u[::-1])
print(type(k))
print(n)
8/43:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz",3,67]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        for u in k:
                 n.append(u[::-1])
print(type(k))
print(n)
8/44:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz"]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
        for u in k:
                 n.append(u[::-1])
print(type(k))
print(n)
8/45:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz"]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
       
                 n.append(u[::-1])
print(type(k))
print(n)
8/46:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz"]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
       
                 n.append(k[::-1])
print(type(k))
print(n)
8/47:
l = []
for i in [["sudh","mahesh","dog",5]]:
    type(i)
    for j in i[::-1]:
        print(j)
        l.append(j[::-1])
l
8/48: l[::-1]
8/49:
l = ["sudh", "kumar","abc","xyz"]
l[::-1]
8/50:
m= []
l = ["sudh", "kumar","abc","xyz"]
l[::-1]
m.append(l[::-1])
8/51:
m= []
l = ["sudh", "kumar","abc","xyz"]
l[::-1]
m.append(l[::-1])
m
8/52:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz"]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
for n in l[i]:    
         print(type(k))
print(n)
8/53:
k = [] 
n=[]
p = []
l = ["sudh", "kumar","abc","xyz"]
for i in range(len(l)-1,-1,-1):
        k.append(l[i])
for n in l[i]:    
         print(type(k))
          print(n)
8/54:
m = mahesh
m.upper()
m
8/55:
m = 'mahesh
m.upper()
m
8/56:
m = 'mahesh,
m.upper()
m
8/57:
m = 'mahesh'
m.upper()
m
8/58:
m = 'mahesh'
m.upper()
8/59:
m = 'mahesh'
m.capitalize()
8/60:
m = 'mahesh'
m.center()
8/61:
m = 'mahesh'
m.encode()
8/62:
m = 'mahesh'
m.split()
8/63:
m = 'mahesh'
m.split(e)
8/64:
m = 'mahesh'
m.split('e')
8/65:
m = 'mah,esh'
m.split(',')
8/66:
m = 'mah,es.h'
m.split(',')
8/67:
m = 'mah,es,h'
m.split(',')
8/68:
m = 'ma.h,es,h'
m.split(','&'.')
8/69:
m = 'ma.h,es,h'
m.split(',''.')
8/70:
m = 'ma.h,es,h'
m.split(',.')
8/71:
m = 'ma.h,es,h'
m.split(',.')
8/72: '{}'.format(m)
8/73: '{a}'.format(m)
8/74: 'mad{}'.format(m)
8/75: 'mad {}'.format(m)
8/76: 'god {}'.format(m)
8/77: 'god {}'.format(m[3])
8/78: 'god {}'+ m
8/79: 'god {}'.format(m)
8/80:
m = ' mahesh is god'
m.count('e')
8/81:
m= []
l = ["sudh", "kumar","abc","xyz"]
l[::-1] 
m.append(l[::-1])
8/82:
m= []
l = ["sudh", "kumar","abc","xyz"]
l[::-1] 
m.append(l[::-1])
8/83: m = 0
8/84:
m= []
l = ["sudh", "kumar","abc","xyz"]
l[::-1] 
m.append(l[::-1])
8/85:
m= []
l = ["sudh", "kumar","abc","xyz"]
l[::-1] 
m.append(l[::-1])
8/86:
m= []
l = ["sudh", "kumar","abc","xyz"]
l[::-1] 
m.append(l[::-1])
m
8/87:
m = ' mahesh is god'
m.center(100,'')
8/88:
m = ' mahesh is god'
m.center(100,' ')
8/89:
m = ' mahesh is god'
m.center(100,' ')
8/90:
m = ' mahesh is god'
m.center(100,' ',/n)
8/91:
m = ' mahesh is god'
m.center(100,' ', /n)
8/92:
m = ' mahesh is god'
m.center(100,' ', /t)
8/93:
m = ' mahesh is god'
m.center(100,' ')
8/94:
m = ' mahesh is god'
m.center(100,' ')
m
8/95:
m = ' mahesh is god'
m = m.center(100,' ')
8/96:
m = ' mahesh is god'
m = m.center(100,' ')
8/97:
m = ' mahesh is god'
m = m.center(100,' ')
m
8/98:
m = ' mahesh is god'
m = m.center(100,' ')
m.append('ok')
8/99:
m = ' mahesh is god'
m = m.center(100,' ')
m.append("ok")
8/100:
m = ' mahesh is god'

m.append("ok")
8/101:
m = ' mahesh is god'

m.append("ok")
8/102:
m = [' mahesh is god']

m.append("ok")
8/103:
m = [' mahesh is god']

m.append("ok")
8/104:
m = [' mahesh is god']

m.append("ok")
m
8/105:
m = [' mahesh is god']

m.insert("ok")
m
8/106:
m = [' mahesh is god']

m.insert(1,"ok")
m
8/107:
m = [' mahesh is god']

m.insert(0,"ok")
m
8/108:
o = [l,c,d,2,d,4]
o.push
8/109:
o = [l,'c','d',2,'d',4]
o.push
8/110:
o = [l,'c','d',2,'d',4]
o.push()
8/111:
o = [l,'c','d',2,'d',4]
o.push(q)
8/112:
o = [l,'c','d',2,'d',4]
o.push(2)
8/113:
o = [l,'c','d',2,'d',4]
o.push('2')
8/114:
o = [l,'c','d',2,'d',4]
                    o.push()
8/115:
o = [l,'c','d',2,'d',4]
                    o.pop()
8/116:
o = [l,'c','d',2,]
o.pop()
8/117:
o = [l,'c','d',2,]
o.push()
8/118:
o = [l,'c','d',2,]
o.push(1,'a')
8/119:
o = [l,'c','d',2]
o.push(1,'a')
8/120:
o = [l,'c','d',2]
o.push('a')
8/121:
o = [l,'c','d',2]
o.push('action: a')
8/122: d = [[1,2,3],[4,5,6][7,8,9]]
8/123: d = [[1,2,3],[4,5,6],[7,8,9]]
8/124:
d = [[1,2,3],[4,5,6],[7,8,9]]
[i[1] for i in d]
8/125:
d = [[1,2,3],[4,5,6],[7,8,9]]
[i[1]**2 for i in d]
8/126:
d = [[1,2,3],[4,5,6],[7,8,9]]
f = [i[1]**2 for i in d]
f
8/127:
d = [[1,2,3],[4,5,6],[7,8,9]]
f = [ for i in d]
f
8/128:
d = [[1,2,3],[4,5,6],[7,8,9]]
f = [i[0] for i in d]
f
8/129: 'god {}'.format(a)
8/130: 'god {}'.format(m)
8/131:
p = [1 ,2,3]
p.append(5)
type(p)
8/132:
p = [1 ,2,3]
p=p.append(5)
type(p)
8/133:
p = [1 ,2,3]
p=p.append(5)
type(p)
p
8/134:
p = [1 ,2,3]
p=p.append(5)
type(p)
p
8/135:
p = [1 ,2,3]
p=p.append(5)
type(p)
print(p)
8/136:
p = [1 ,2,3]
p=p.append('5')
type(p)
print(p)
8/137:
p = [1 ,2,3]
p=p.append('5')
type(p)
print(p)
8/138:
p = [1 ,2,3]
p.append('5')
type(p)
print(p)
8/139:
p = [1 ,2,3]
p.append(5)
type(p)
print(p)
8/140: t = ['True',True,2]
8/141: t = ['True',True,2,s]
8/142: t = ['True',True,2,0]
8/143: t = ['True',True,2,(2,5),0]
8/144: set(1,2,3,4)
8/145: set([1,2,3,4])
8/146:
s = {1,2,3}
s
8/147:
s = {1,2,3}
s[1]
8/148:
s = {1,2,3}
s(1)
8/149:
s = {1,2,3}
s[1]
8/150:
s = {1,2,3}
s(0)
8/151:
s = {1,2,3,[1,2]}
s(3)
8/152:
s = (1,2,3)
s(3)
8/153:
s = (1,2,3)
s(0)
8/154:
s = (1,2,3)
s[1]
8/155:
s = (1,2,3)
s[1] = 3
8/156:
s = {1,2,3}
s[1] = 3
8/157:
s = {1,2,3}
s[1]
8/158:
for i in range 2000 to 3200 :
    i
8/159:
for i in range (2000,3200) :
    i
8/160:
for i in range (2000,3200) :
    i
8/161:
for i in range (2000,3200) :
i
8/162:
for i in range (2000,3200) :
        print(i)
8/163:
for i in range (2000,3200) :
        if i / 7 & i is not i*5 :
            print(i)
8/164:
for i in range (2000,3200) :
        if i / 7 && i is not i*5 :
            print(i)
8/165:
for i in range (2000,3200) :
        if i / 7 and i is not i*5 :
            print(i)
8/166:
for i in range (2000,3201) :
        if i / 7 and i is not i*5 :
            print(i)
8/167:
for i in range (2000,3201) :
        if i / 7 and i is not i*5 :
            print(i/7)
8/168:
for i in range (2000,3201) :
        if i / 7 = 0 and i is not i*5 :
            print(i)
8/169:
for i in range (2000,3201) :
        if i / 7 == 0 and i is not i*5 :
            print(i)
8/170:
for i in range (2000,3201) :
        if i / 7 == 0 and i is not i*5 :
            print(i)
8/171:
for i in range (2000,3201) :
        if i / 7 == 0 :
            print(i)
8/172:
for i in range (2000,3201) :
        if i / 7 == 0 :
            print(i)
8/173:
for i in range (2000,3201) :
        if j = i / 7 == 0 :
            print(j)
8/174:
for i in range (2000,3201) :
        if j = i/7 == 0 :
            print(j)
8/175:
for i in range (2000,3201) :
         j = i/7 == 0 
            print(j)
8/176:
for i in range (2000,3201) :
         j = i/7 == 0 
        print(j)
8/178:
for i in range (2000,3201) :
         j = i/7 == 0
        
print(j)
8/179:
for i in range (2000,3201) :
         j = i/7 == 0
        print(j)
8/181:
for i in range (2000,3201) :
         j = i/7 == 0
             print(j)
8/182:
for i in range (2000,3201) :
    if:
         j = i/7 == 0
             print(j)
8/183:
for i in range (2000,3201) :
    if :
         j = i/7 == 0
             print(j)
8/184:
for i in range (2000,3201) :
    if : j = i/7 == 0
             print(j)
8/185:
for i in range (2000,3201) :
    if :j = i/7 == 0
        print(j)
8/186:
for i in range (2000,3201) :
    if :j = i/7 
        print(j)
8/187:
for i in range (2000,3201) :
    if i/7 == 0 : 
        print(j)
8/188:
for i in range (2000,3201) :
    if i/7 == 0 : 
        print(j)
8/189:
for i in range (2000,3201) :
    if j=i/7 == 0 :
    
        print(j)
8/190:
for i in range (2000,3201) :
    if j = i/7 == 0 :
    
        print(j)
8/191:
for i in range (2000,3201) :
    if i/7 == 0 :
    
        print(i)
8/192:
for i in range (2000,3201) :
    if i/7 == 0 :
        print(i)
8/193:
for i in range (2000,3201) :
    if i/7 == 0 :
        print(i)
        else:
            print('not divisible')
8/194:
for i in range (2000,3201) :
    if i/7 == 0 :
        print(i)
    else:
            print('not divisible')
8/195:
for i in range (2000,3201) and /7:
    print(i)
8/196:
for i in range (2000,3201) and i/7:
    print(i)
8/197:
for i in range (2000,3201) and i/7=0:
    print(i)
8/198:
for i in range (2000,3201):
    j = i/7 = 0
    j
8/199:
for i in range (2000,3201):
    j = i/7 == 0
    j
8/200:
for i in range (2000,3201):
    j = i%7 == 0
    j
8/201:
for i in range (2000,3201):
    j = i%7 == 0
    j
8/202:
for i in range (2000,3201):
    j = i%7 == 0
    print(j)
8/203:
for i in range (2000,3201):
    j = i%7 == 0
    if j == True:
        print(i)
8/204:
for i in range (2000,3201):
    j = i%7 == 0 and isMultiple(5)
    if j == True:
        print(i)
8/205:
for i in range (2000,3201):
    j = i%7 == 0 and is Multiple(5)
    if j == True:
        print(i)
8/206:
for i in range (2000,3201):
    j = i%7 == 0 and i%5==0
    if j == True:
        print(i)
8/207:
for i in range (2000,3201):
    j = i%7 == 0 m= i%5==0
    if j == True and m is False:
        print(i)
8/208:
for i in range (2000,3201):
    j = i%7 == 0 
    m= i%5==0
    if j == True and m is False:
        print(i)
8/209:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i,/,)
8/210:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i,/',')
8/211:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i,/',')
8/212:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i,/n)
8/213:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i,/n)
8/214:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i,/t)
8/215:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,"/t")
8/216:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,"/n")
8/217:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,'/n')
8/218:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,'\n')
8/219:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,'\t')
8/220:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,'\,')
8/221:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,end="")
8/222:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,end=" ")
8/223:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,end=" , ")
8/224:
for i in range (2000,3201):
    j = i%7 == 0 
    m = i%5 ==0
    if j == True and m is False:
        print(i ,end=",")
8/225:
print("enter first name")
a = input()
b = input()
8/226:
print("enter first name")
a = input()
print("enter last name")
b = input()
c = [a+b]
d = c[::-1]
print("the reverse order is :" d )
8/227:
print("enter first name")
a = input()
print("enter last name")
b = input()
c = [a+b]
d = c[::-1]
print("the reverse order is :"d )
8/228:
print("enter first name")
a = input()
print("enter last name")
b = input()
c = [a+b]
d = c[::-1]
print("the reverse order is :"'d' )
8/229:
print("enter first name")
a = input()
print("enter last name")
b = input()
c = [a+b]
d = c[::-1]
print("the reverse order is :",d )
8/230:
print("enter first name")
a = input()
print("enter last name")
b = input()
c = a+b
d = c[::-1]
print("the reverse order is :",d )
8/231:
print("enter first name")
a = input()
print("enter last name")
b = input()
c = a+b
d = c[::-1]
print("the reverse order is :",\d )
8/232:
print("enter first name")
a = input()
print("enter last name")
b = input()
c = a+b
d = c[::-1]
print("the reverse order is :",d,'\n' )
8/233:
print("enter first name")
a = input( )
print("enter last name")
b = input()
c = a+b
d = c[::-1]
print("the reverse order is :",d,'\n' )
8/234:
print("enter first name")
a = input(,)
print("enter last name")
b = input()
c = a+b
d = c[::-1]
print("the reverse order is :",d,'\n' )
8/235:
print("enter first name")
a = input()
print("enter last name")
space = ' '
b = input()
c = a+b
d = c[::-1]
print("the reverse order is :",d,'\t' )
8/236:
print("enter first name")
a = input()
print("enter last name")
space = ' '
b = input()
c = a+space+b
d = c[::-1]
print("the reverse order is :",d,'\t' )
8/237:
print("enter first name")
a = input()
print("enter last name")
space = ' '
b = input()
c = a+space+b
d = c[::-1]
print("the reverse order is :",d)
8/238:
print("enter first name")
a = input()
print("enter last name")
space = '   '
b = input()
c = a+space+b
d = c[::-1]
print("the reverse order is :",d)
8/239:
print("enter first name")
a = input()
print("enter last name")
space = ' '
b = input()
c = a+space+b
d = c[::-1]
print("the reverse order is :",d)
8/240:
pi = 3.14
d = 12
r = d/2
v = (4/3)*pi*r^3
print(v)
8/241:
pi = 3.14
d = 12
r = d/2
v = (4/3)*pi*r**3
print(v)
8/242:
d{}
d('mahesh') = 'god'
8/243:
d={}
d('mahesh') = 'god'
8/244:
d={}
d'('mahesh')'' = 'god'
8/245:
d={}
d'('mahesh')' = 'god'
8/246:
d={}
d[mahesh'] = 'god'
8/247:
d={}
d['mahesh'] = 'god'
8/248:
d={}
d['mahesh'] = 'god'
d
8/249:
d={}
d{'mahesh'} = 'god'
d
8/250:
d={}
d[]'mahesh'] = 'god'
d
8/251:
d={}
d['mahesh'] = 'god'
d
8/252: m{key:value}
8/253: m{key:'value,}
8/254: m{8:'value,}
8/255: {8:'value,}
8/256: l = {8:'value,}
8/257: l = {8:'value'}
8/258: {8:'value'}
8/259: {b:'value'}
8/260: {ba:'value'}
8/261: {'ba':'value'}
8/262: [i:**2 for in range(10)]
8/263: [i:i**2 for in range(10)]
8/264: [i:i**2 for in range(10)]:
8/265: [i**2 for in range(10)]:
8/266: [i**2 for in range(10)]:
8/267: [i**2 for i in range(10)]:
8/268: m = [i**2 for i in range(10)]:
8/269: m[] = [i**2 for i in range(10)]:
8/270:
h_letters = [ letter for letter in 'human' ]
print( h_letters)
8/271: m[] = [for i in range(10)]:
8/272: m = [for i in range(10)]:
8/273: m = [for i in range(10)]
8/274:
m = [for i in range(10)]
print(m)
8/275:
m = [i for i in range(10)]
print(m)
8/276:
m = [i**2 for i in range(10)]
print(m)
8/277:
m = [i**2:i for i in range(10)]
print(m)
8/278:
m = {i**2:i for i in range(10)}
print(m)
8/279:
m = [mahesh for i in range(10)]
print(m)
8/280:
m = [mahesh for i in range(10)]
print(m)
8/281:
m = [mahesh for i in range()]
print(m)
8/282:
m = [mahesh for i in range(1)]
print(m)
8/283:
m = [mahesh for mahesh in 'human']
print(m)
8/284:
m = [for mahesh in 'human']
print(m)
8/285:
m = [i for mahesh in 'human']
print(m)
8/286:
m = [suresh for mahesh in 'human']
print(m)
8/287:
m = [i for i in 'human']
print(m)
8/288:
m = [in for in in 'human']
print(m)
8/289:
m = [inn for inn in 'human']
print(m)
8/290:
m = [i**6 for i in range(10)]
print(m)
8/291:
m = [i**600000 for i in range(10)]
print(m)
8/292:
m = [i**600000 for i in range(10)]
print(m)
8/293:
m = [i**60000 for i in range(10)]
print(m)
8/294:
m = [i**6000 for i in range(10)]
print(m)
8/295:
m = [i**600 for i in range(10)]
print(m)
8/296:
m = [i**60 for i in range(10)]
print(m)
8/297:
m = [i**60 for i in range(10)]
print(m)
8/298:
m = [i**60 for i in range(10)]
print(m)
8/299:
m = [i**6 for i in range(10)]
print(m)
8/300:
m = [i**6 for i in range(10)]
print(m)
8/301:
m = [i**6 for i in range(10)]
print(m)
8/302:
m = [i**6 for i in range(10)]
print(m)
8/303:
m = [i**6 for i in range(10)]
print(m)
8/304:
m = [i**10 for i in range(10)]
print(m)
8/305:
m = [i**15 for i in range(10)]
print(m)
8/306:
m = [i**20 for i in range(10)]
print(m)
8/307:
m = [i**50 for i in range(10)]
print(m)
8/308:
m = [i**100 for i in range(10)]
print(m)
8/309:
m = [i**200 for i in range(10)]
print(m)
8/310:
m = [i**2000 for i in range(10)]
print(m)
8/311:
m = [i**2000 for i in range(10)]
print(m,'end= ')
8/312:
m = [i**2000 for i in range(10)]
print(m,end='\t ')
8/313:
m = [i**2000 for i in range(10)]
print(m,end='\n ')
8/314:
m = [i**2000 for i in range(10)]
print(m,end="  , ")
8/315:
m = [i**2000 for i in range(10)]
print(m,end="  ,        ")
8/316:
while i = 5 :
    print('true')
8/317:

while i == 5 :
    print('true')
8/318:
i=5
while i == 5 :
    print('true')
10/1:
i=5
   while i == 5 :
print('true')
10/2:
i=5
   while i == 5 :
print('true')
10/3:
i=5
   while i == 5 :
        break
print('true')
10/4:
i=5
   while i == 5 :
        continue
print('true')
10/5:
i=5
   while i == 5 :
        continue
print('true')
10/6:
i=5
   while i == 5 :
          print('true')
10/7:
i=5
   while i == 5 :
        print("true")
        else:
            print('true')
10/8:
i=5
while i == 5 :
        print("true")
        else:
            print('true')
10/9:
i=5
while i == 5 :
        print("true")
        else:
            print('true')
10/10:
i=5
while i == 5 :
        print("true")
        else:
            print('true')
10/11:
i=5
while i == 5 :
        print("true")
11/1: topper()
11/2:
def topper(name,marks):
    if marks > 60 :
        print("student {} is a topper with {} marks".format(name,marks))
        else:
             print("student {} is not a topper with {} marks".format(name,marks))
11/3:
def topper(name,marks):
    if marks > 60 :
        print("student {} is a topper with {} marks".format(name,marks))
    else:
             print("student {} is not a topper with {} marks".format(name,marks))
11/4: topper()
11/5: topper('mahesh',68)
11/6:

topper('suresh',68)
topper('mahesh',68)
topper('mahesh',68)
11/7:

topper('suresh',60)
topper('mahesh',68)
topper('mahesh',68)
11/8:

topper('suresh',60)
topper('mahesh',68)
topper('eldorado',68)
11/9:

topper('pavan',65)
topper('mahesh',68)
topper('eldorado',60)
11/10: topper
11/11: [topper() for i in range(0,5)]
11/12: [input.topper() for i in range(0,5)]
11/13: [input.topper() for topper in range(0,5)]
11/14: [input.topper() for topper() in range(0,5)]
11/15: [input.topper() for topper() in 5]
11/16:
for i in range(0,5):
    print(topper())
11/17:
for i in range(0,5):
    print(input.topper())
11/18:
for i in range(0,5):
    print(input(topper())
11/19:
while i>0 and i<5:
    input(a)
11/20:
i=0
while i>0 and i<5:
    input(a)
11/21:
i=0
while i>0 and i<5:
    input(a)
12/1:
i=0
while i>0 and i<5:
    input(a)
12/2:
i=1
while i>0 and i<5:
    input(a)
12/3:
i=1
while i>0 and i<5:
    input(a)
    a
12/4:
i=1
while i>0 and i<5:
    input(a)
    a = 0
12/5:
i=1 a=[]
while i>0 and i<5:
    input(a)
12/6:
i=1
a=[]
while i>0 and i<5:
    input(a)
13/1:
i=1
a=[]
for i>0 and i<5:
    input(a)
13/2:
i=1
a=[]
for i in range (0,5)
    input(a)
13/3:
i=1
a=[]
for i in range (0,5):
    input(a)
13/4: dir(lst)
13/5: dir(list)
13/6: dir(tuple)
13/7: dir(set)
13/8: i = iter(1,2,3,4)
13/9: i = iter(1,2)
13/10:
i = iter(1,2)
i
13/11: i = iter(call,2)
13/12:
i = iter(call,2)
call = 0
13/13: i = [1,2,3,'mah']
13/14:
i = [1,2,3,'mah']
m = iter(i)
13/15:
i = [1,2,3,'mah']
m = iter(i)
m
13/16:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
    print(j)
13/17:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
    print(-j)
13/18:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
    print(reverse(j))
13/19:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
       j =  j.capitalize
    print(j)
13/21:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
       j =  j.capitalize
        print(j)
13/22:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
       j =  j.capitalize
    print(j)
13/24:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
        j =  j.capitalize
        print(j)
13/25:
i = [1,2,3,'mah']
m = iter(i)
for j in m:
        yeild j
13/26: my_test(j)
13/27:
def my_test()
m = iter(i)
for j in m:
     yield j*3
13/28:
def my_test()
m = iter(i)
for j in m:
    yield j*3
13/29:
def my_test(i)
m = iter(i)
for j in i:
    yield j*3
13/30:
def my_test(i)
for j in i:
    yield j*3
13/31:
def test(i)
for j in i:
    yield j*3
13/32:
def test(n)
for j in i:
    yield j*3
13/33:
def test(n):
for j in i:
    yield j*3
13/34:
def test(n):
    |for j in i:
    yield j*3
13/35: my_test(10)
13/36:
def test(n):
    for j in i:
    yield j*3
13/37:
def test(n):
    for j in i:
        yield j*3
13/38: my_test(10)
13/39: test(10)
13/40: for i in test(10)
13/41:
for i in test(10):
    print(i)
13/42:
def test(n):
    for i in i:
        yield j*3
13/43:
def test(n):
    for i in i:
        yield i*3
13/44:
for i in test(10):
    print(i)
13/45:
def test(n):
    for m in i:
        yield i*3
13/46:
for i in test(10):
    print(i)
13/47:
def test(n):
    for i in n:
        yield i*3
13/48:
for i in test(10):
    print(i)
13/49:
for i in test(10):
    print(i)
13/50:
def test(n):
    for j in n:
        yield i*3
13/51:
for i in test(10):
    print(i)
13/52:
for i in test(10):
    print(i)
13/53:
def test(n):
    for j in n:
        yield i*3
13/54:
def test(n):
    for j in n:
        yield j*3
13/55:
for i in test(10):
    print(i)
13/56:
def test(n):
    for j in n:
        yield j*3
13/57:
for i in test(10):
    print(i)
13/58:
def test(n):
    for j in range (n):
        yield j*3
13/59:
for i in test(10):
    print(i)
13/60:
def ups(pow):
    lst = [1,2,'sds']
    l = iter(lst)
    return l
13/61: l
13/62: pow()
13/63: pow(l)
13/64: ups()
13/65: ups(1)
13/66:
def ups(pow):
    lst = [1,2,'sds']
    l = iter(lst)*pow
    return l
13/67: ups(1)
13/68:
def test(n):
    for j in range (n):
        o = yield j*3
13/69:
for i in test(10):
    print(i)
13/70:
def test(n):
    for j in range (n):
         o = yield j*3
13/71:
for i in test(10):
    print(i)
13/72:
def ups(pow):
    lst = [1,2,'sds']
    l = iter(lst)
    return l
13/73:
pit = [1,2,3,4]
def ott(pit):
    i =pit*2
    i
13/74:
pit = [1,2,3,4]
def ott(pit):
    i =pit*2
    i
13/75: OTT()
13/76: ott()
13/77: ott(pit)
13/78: ott(pit)
13/79: ott(pit)
13/80: ott(1)
13/81:

def ott(pit):
    i =pit*2
13/82: ott(1)
13/83:
ott(1)
print(i)
13/84:

def ott(pit):
    i =pit*2
13/85:
ott(1)
print(i)
13/86:
ott(1)
print(i)
13/87:

def ott(pit):
    i =pit*2
13/88:
ott(1)
print(i)
13/89:
pit = [1,2,3]
def ott(pit):
    i =pit*2
13/90:
ott(pit)
print(i)
13/91:
def cost(price):
    if price > 100:
        print("{} is costly".format(price))
13/92: list(map(cost,price))
13/93:
list(map(cost,price))
price = [1,45,68,90]
13/94:
list(map(cost,price))
t = [1,45,68,90]
13/95:
list(map(cost,t))
t = [1,45,68,90]
13/96:
t = [1,45,68,90]
list(map(cost,t))
13/97:
def cost(price):
    if price > 100:
        print("{} is costly".format(price))
    else:
         print("{} is not costly".format(price))
13/98:
t = [1,45,68,90]
list(map(cost,t))
13/99:
t = [1,45,68,90,200,900,292]
list(map(cost,t))
13/100:
t = [1,45,235,68,90,200,900,292]
list(map(cost,t))
13/101:
price = [1,45,235,68,90,200,900,292]
list(map(cost,price))
13/102:
price = [1,45,235,450,68,90,200,900,292]
list(map(cost,price))
13/103:
input(price) = [1,45,235,450,68,90,200,900,292]
list(map(cost,price))
13/104:
input([price]) 
list(map(cost,price))
14/1:
def cost(price):
    if price > 100:
        print("{} is costly".format(price))
    else:
         print("{} is not costly".format(price))
14/2:
input([price]) 
list(map(cost,price))
14/3:
price = []
input([price])
list(map(cost,price))
14/4:
price = []
input(price)
list(map(cost,price))
14/5: dir(tuple)
14/6:
price = [12,23,434,33]
oot = [12,34,343]
list(map(cost,price,oot))
14/7:
def cost(price, oot):
    if price > 100:
        print("{} is costly".format(price))
    else:
         print("{} is not costly".format(price))
14/8:
price = [12,23,434,33]
oot = [12,34,343]
list(map(cost,price,oot))
14/9:
def cost(price, oot):
    if price > 100:
        print("{} is costly {}".format(price, oot))
    else:
         print("{} is not costly {}".format(price, oot))
14/10:
price = [12,23,434,33]
oot = ['test']
list(map(cost,price,oot))
14/11:
price = [12,23,434,33]
oot = ['test']
list(map(cost,price,oot))
14/12:
price = [,234,12,23,434,33]
oot = ['test']
list(map(cost,price,oot))
14/13:
price = [234,12,23,434,33]
oot = ['test']
list(map(cost,price,oot))
15/1:
def cost(price, oot):
    if price > 100:
        print("{} is costly {}".format(price, oot))
    else:
         print("{} is not costly {}".format(price, oot))
15/2:
price = [234,12,23,434,33]
oot = ['test']
list(map(cost,price,oot))
15/3:
price = [234,12,23,434,33]
oot = ['test']
list(map(cost,(price,oot))
15/4:
price = [234,12,23,434,33]
oot = ['test']
list(map(cost,(price,oot)))
15/5:
price = [234,12,23,434,33]
oot = ['test']
list(map(cost,price,*oot))
15/6:
price = [234,12,23,434,33]
oot = ['test']
list(map(cost,price,**oot))
15/7:
price = [234,12,23,434,33]
oot = ['test']
list(map(cost,price,*oot))
15/8:
price = [234,12,23,434,33]
oot = ["test"]
list(map(cost,price,*oot))
15/9:
def cost(price, oot):
    if price > 100:
        print("{} is costly {}".format(price, oot))
    else:
         print("{} is not costly {}".format(price, oot))
15/10:
price = [234,12,23,434,33]
oot = ["test"]
list(map(cost,price,*oot))
15/11:
price = [234,12,23,434,33]
oot = [1,2]
list(map(cost,price,*oot))
15/12:
price = [234,12,23,434,33]
oot = [1,2]
list(map(cost,price,oot))
15/13:
price = [234,12,23,434,33]
oot = [1,2,0,7,5]
list(map(cost,price,oot))
15/14:
price = [234,12,23,434,33]
oot = [1,2,0,7,5]
list(map(cost,price,oot))
15/15:
price = [234,12,23,434,33]
oot = [1,2,'',7,5]
list(map(cost,price,oot))
15/16:
price = [234,12,23,434,33]
oot = [1,2,'',7,5]
list(map(cost,price,oot))
15/17:
price = [234,12,23,434,33]
oot = [1,2,'',7,5,'test']
list(map(cost,price,oot))
15/18:
price = [234,12,23,434,33,45]
oot = [1,2,'',7,5,'test']
list(map(cost,price,oot))
15/19:
price = [234,12,23,434,33,45,100]
oot = [1,2,'',7,5,'test','test']
list(map(cost,price,oot))
15/20:
smart(
)
15/21: smart()
15/22: smart()
15/23:
def smart (mahesh, o ="smart",j='idiot'):
    print("{} is {}".format(o,j))
15/24: smart()
15/25:
def smart(mahesh, o ="smart",j='idiot'):
    print("{} is {}".format(o,j))
15/26: smart()
15/27: smart(mahesh)
15/28: smart('mahesh')
15/29:
def smart(m, o ="smart",j='idiot'):
    if m='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,j))
        else:
            print("test")
15/30:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,j))
    else:
            print("test")
15/31:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,j))
    else:
            print("test")
15/32: smart('mahesh')
15/33:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,O))
    else:
            print("test")
15/34: smart('mahesh')
15/35:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,o))
    else:
            print("test")
15/36: smart('mahesh')
15/37:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,o))
    else:
            print("%o")
15/38: smart('mahes')
15/39: smart('kjk')
15/40:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,o))
        else:
            print("%o")
15/41:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,o))
           else:
            print("%o")
15/42:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,o))
else:
            print("%o")
15/43:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m =='mahesh' or "MAHESH" or "mahesh":
        print("{} is {}".format(m,o))
        else:
            print("%o")
15/44: smart('kjk')
15/45:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh"]:
        print("{} is {}".format(m,o))
        else:
            print("%o")
15/46:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh"]:
        print("{} is {}".format(m,o))
    else:
            print("%o")
15/47: smart('kjk')
15/48:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh"]:
        print("{} is {}".format(m,o))
    else:
            print("%o"%s)
15/49: smart('kjk')
15/50:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh"]:
        print("{} is {}".format(m,o))
    else:
            print("%o",%s)
15/51:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh"]:
        print("{} is {}".format(m,o))
    else:
            print("%m", %s)
15/52:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh"]:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
15/53: smart('kjk')
15/54:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
15/55: smart('maheshs')
15/56:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m ==['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
15/57: smart('maheshs')
15/58: smart('mahesh')
15/59: smart('Mahesh')
15/60:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
15/61: smart('Mahesh')
15/62:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
            m
15/63:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
m
15/64:
m =[]
def smart(m, o ="smart",j='idiot'):
    if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/65:

def smart(m, o ="smart",j='idiot'):
    if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/66:

def smart(m, o ="smart",j='idiot'):
    if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/67:

def smart(m, o ="smart",j='idiot'):
    for i in m:
        if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/68:  j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
15/69:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if m == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/70:
j =[] 
j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
15/71:
j =[] 
j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
j
15/72:
j =[] 
j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
ptint(j)
15/73:
j =[] 
j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
print(j)
15/74:
j =[] 
j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
j[0]
15/75:
j =[] 
j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
j[1]
15/76:
j =[] 
j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
j
15/77:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs'
j
15/78:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs'
j
15/79:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs'
print(j)
15/80:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs
15/81:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs
15/82:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs
j
15/83:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs
j
15/84:
j =[] 
j == 'mahesh' or "MAHESH" or "mahesh" or 'maheshs
15/85: j =['mahesh' or "MAHESH" or "mahesh" or 'maheshs]
15/86: j = ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
15/87: j == ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
15/88: j = ['mahesh' or "MAHESH" or "mahesh" or 'maheshs']
15/89:
j = ['mahesh' or "MAHESH" or "mahesh" or 'maheshs'] 
j
15/90:
j = ['mahesh' or "MAHESH" or "mahesh" or 'maheshs'] 
j[2]
15/91:
j = ['mahesh' or "MAHESH" or "mahesh" or 'maheshs'] 
j
15/92:
j = ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j
15/93:
j = ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j
15/94:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j
15/95:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j
15/96: j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs']
15/97:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
is j == "man"
15/98:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
is j == "man":
15/99:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j and 'm'
15/100:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j is'm'
15/101:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j is'mahesh'
15/102:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j is'mahesh'
15/103:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j ==?'mahesh'
15/104:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j ==? 'mahesh'
15/105:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j is 'mahesh'
15/106:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j "=="? 'mahesh'
15/107:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j ==? 'mahesh'
15/108:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j is'mahesh'
15/109:
j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs'] 
j
15/110: j == ['mahesh' or 'MAHESH' or "mahesh" or 'maheshs']
15/111: j == ['mahesh' or 'MAHESH' or "mahesh" and 'maheshs']
15/112: j == ['mahesh' and 'MAHESH' and "mahesh" and 'maheshs']
15/113: j == ['mahesh' and 'MAHESH' or "mahesh" and 'maheshs']
15/114: j == ['mahesh' or 'MAHESH' or "mahesh" and 'maheshs']
15/115: j == ['mahesh' and 'MAHESH' or "mahesh" and 'maheshs']
15/116: j == ['mahesh' or 'MAHESH' or "mahesh" and 'maheshs']
15/117: j == ['mahesh' or 'MAHESH' and"mahesh" and 'maheshs']
15/118:
j == ['mahesh' or 'MAHESH' and"mahesh" and 'maheshs'] 
j
15/119:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j
15/120:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j
15/121:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j
15/122: j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs']
15/123:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is "mahesh"
15/124:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is 'mahesh'
15/125:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is ['mahesh']
15/126:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is ['mahesh']
15/127:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs']
15/128:
j == ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs']
15/129:
j = ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs']
15/130:
j = ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs']
15/131:
j = ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs'] 
j is ['mahesh' and 'MAHESH' and"mahesh" and 'maheshs']
15/132:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m
        if m == ['mahesh',"MAHESH","mahesh",'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/133:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if m == ['mahesh',"MAHESH","mahesh",'maheshs']:
        print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/134:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if m == ['mahesh',"MAHESH","mahesh",'maheshs']:
            print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/135: smart('Mahesh')
15/136:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == ['mahesh',"MAHESH","mahesh",'maheshs']:
            print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/137: smart('mahesh',"MAHESH","mahesh",'maheshs')
15/138: smart(['mahesh',"MAHESH","mahesh",'maheshs'])
15/139:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
    else:
            print("{} is {}".format(m,j))
print(m)
15/140: smart(['mahesh',"MAHESH","mahesh",'maheshs'])
15/141: smart(['mahesh'])
15/142:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
            else:
            print("{} is {}".format(m,j))
print(m)
15/143:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
                else:
            print("{} is {}".format(m,j))
print(m)
15/144:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
            else:
            print("{} is {}".format(m,j))
print(m)
15/145:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(m)
15/146: smart(['mahesh'])
15/147: smart(['mahesh,suresh'])
15/148: smart(['maheshs])
15/149: smart(['maheshs'])
15/150: smart('maheshs')
15/151: smart('mahesh')
15/152: smart('mahesh')
15/153:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(i)
15/154:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(m)
15/155:

def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(m)
15/156: smart('mahesh')
15/157:
m = []
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(m)
15/158: smart('mahesh')
15/159: smart('mahesh',"suresh")
15/160: smart(['mahesh',"suresh"})
15/161: smart(['mahesh',"suresh"])
15/162:

def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(m)
15/163: smart(['mahesh',"suresh"])
15/164: smart(['mahesh'])
15/165: smart(['maheshs'])
15/166:

def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh' and "suresh":
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(m)
15/167: smart(['maheshs'])
15/168: smart(['mahesh'])
15/169: smart(['mahesh', "suresh"])
15/170: smart(["suresh"])
15/171:

mahesh and suresh
15/172:

"mahesh" and 'suresh'
15/173:

"mahesh" and "suresh"
15/174:

"mahesh" and "suresh"
15/175:

'mahesh' and "suresh"
15/176:

'mahesh' and "suresh"
15/177: smart(["mahesh"])
15/178:

def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        elif i == 'maheshs':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(m)
15/179: smart(["maheshs"])
15/180: smart(["mahess"])
15/181: smart(["mahesh"])
15/182: smart(['mahesh'])
15/183: smart(['mahsh'])
15/184: smart(['mahesh'])
15/185: smart(['Mahesh'])
15/186: smart(['mahesh'])
15/187: k = lambda i:i**3
15/188: k(5)
15/189: k(input())
15/190: k(input(i))
15/191:
i = 0
k = lambda i:i**3
15/192: k(input(i))
15/193: input(k())
15/194: input(k(i))
15/195: input(k(i))
15/196: input(k())
15/197:
m=["mahesh","suresh"]
list(map(smart,m))
15/198:
m=["mahesh","suresh",'maheshs']
list(map(smart,m))
15/199:

def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        elif i == 'maheshs':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(i)
15/200:
m=["mahesh","suresh",'maheshs']
list(map(smart,m))
15/201: smart('mahesh')
15/202:
m =[]
def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        elif i == 'maheshs':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(i)
15/203: smart('mahesh')
15/204: smart(['mahesh'])
15/205:

def smart(m, o ="smart",j='idiot'):
    for i in m:
        if i == 'mahesh':
            print("{} is {}".format(m,o))
        elif i == 'maheshs':
            print("{} is {}".format(m,o))
        else:
            print("{} is {}".format(m,j))
print(i)
15/206: smart(['mahesh'])
15/207:
i = 0
k = lambda i,j:i**3
15/208: input(k())
15/209: input(k(3,4))
15/210: k(3,4)
15/211: map(mahesh,a,b)
15/212: list(map(mahesh,a,b))
15/213: list(map(mahes,a,b))
15/214: def mahes(a,b):
15/215: def mahesh(a,b):
15/216: def mahesh(a,b):
15/217:
def mahesh(a,b):
    break
15/218:
def mahesh(a,b):
break
15/219:
def mahesh(a,b):
      break
15/220:
def mahesh(a,b):
      print(a,b)
15/221: list(map(mahes,a,b))
15/222: list(map(mahesh,a,b))
15/223:
list(map(mahesh,a,b))
a =5
b=5
15/224:
a =5
b=5
list(map(mahesh,a,b))
15/225:
a ='5'
b='5'
list(map(mahesh,a,b))
15/226:
lst = [47,56,78,98]
for i in lst:
15/227:
lst = [47,56,78,98]
for i in lst:
    i = i + i
    print(i)
15/228: from functools import reduce
15/229:
lst = [47,56,78,98]
for i in lst:
    i = i + i
    print(i)
15/230: reduce(lamdba )
15/231: reduce(lamdba x,y: x+y,lst )
15/232: reduce(lambda x,y: x+y,lst )
15/233:
lst = [47,56,78,98]
for i in lst:
    j = i + i
    print(j)
15/234:
lst = [47,56,78,98]
for i in lst:
    j = i + i++
    print(j)
15/235:
lst = [47,56,78,98]
for i in lst:
    j = i + i[1]
    print(j)
15/236:
lst = {47,56,78,98}
for i in lst:
    j = i + i
    print(j)
15/237: type({})
15/238: type({4})
15/239: type(())
15/240:
lst = (47,56,78,98)
for i in lst:
    j = i + i
    print(j)
15/241:
lst = (47,56,78,98)
for i in lst:
    j = i + i[2]
    print(j)
15/242:
lst = (47,56,78,98)
for i in lst:
    j = i + lst[2]
    print(j)
15/243:
lst = (47,56,78,98)
for i in lst:
    j = i + lst[2]
    k = j + lst[3]
    print(j)
15/244:
lst = (47,56,78,98)
for i in lst:
    j = i + lst[2]
    k = j + lst[3]
    print(k)
15/245:
lst = (47,56,78,98)
for i in lst:
    j = i + lst[2]
    k = j + lst[3]
    m = k + lst[4]
    print(k)
15/246:
lst = (47,56,78,98)
for i in lst:
    j = i + lst[2]
    k = j + lst[3]
    m = k + lst[4]
    print(m)
15/247:
lst = (47,56,78,98)
for i in lst:
    j = i + lst[1]
    k = j + lst[2]
    m = k + lst[3]
    print(m)
15/248: lambda x,y: x+y,lst
15/249:
lambda i :i**3
i=10
15/250:
lambda i :i**3
i=10
i
15/251: lambda i :i**3
15/252:
i =10
lambda i :i**3
15/253:
def mahesh(i)
i.lambda i :i**3
15/254:
def mahesh(i)
i.lambda i :i**3
15/255:
def mahesh(m)
m.lambda i :i**3
15/256:
def mahesh()
mahesh.lambda i :i**3
15/257:
def mahesh():
mahesh.lambda i :i**3
15/258:
def mahesh():
mahesh.lambda i:i**3
15/259:
def mahesh():
mahesh. lambda i:i**3
15/260:
def mahesh():
lambda i:i**3
15/261:

lambda i:i**3
15/262: p(7)
15/263:

p=lambda i:i**3
15/264: p(7)
15/265: reduce(lambda x,y: x+y,lst)
15/266: list(reduce(lambda x,y: x+y,lst))
15/267: list(filter(lambda x,y: x+y,lst))
15/268: list(reduce(lambda x,y: x+y,lst))
15/269: reduce(lambda x,y: x+y,lst)
15/270: list(reduce(lambda i: i%2==0,lst))
15/271: list(reduce(lambda x: x%2==0,lst))
15/272: list(reduce(lambda b: b%2==0,lst))
15/273: list(filter(lambda b: b%2==0,lst))
15/274: list(filter(lambda m: m%2==0,lst))
15/275:
p = ["mahesh","MAHESH",'maheshs']
j = [k for i in p if i == p]
15/276:
p = ["mahesh","MAHESH",'maheshs']
j = [k for i in p if i == p]
k
15/277:
num_list = [y for y in range(100) if y % 2 == 0 if y % 5 == 0]
print(num_list)
15/278:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in p if i == p]
15/279:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in p if i == p]
15/280:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in p if i == p]
print(j)
15/281:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in p if i == p]
print(i)
15/282:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i == 10]
print(i)
15/283:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i == 10]
print(j)
15/284:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i%10 ==0]
print(j)
15/285:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i%10 ==0,i%2==0 ]
print(j)
15/286:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i%10 ==0 if i%2==0 ]
print(j)
15/287:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i%10 ==0 if i%2==0 if i%5==0]
print(j)
15/288:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i%10 ==0 if i%2==0 if i%5==0 if i%3 ==3]
print(j)
15/289:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i%10 ==0 if i%2==0 if i%5==0 if i%3 ==0]
print(j)
15/290:
p = ["mahesh","MAHESH",'maheshs']
j = [i for i in range(100) if i%10 ==0 if i%7==0 if i%5==0 if i%3 ==0]
print(j)
15/291: for i in range 20:
15/292: for i in range (20):
15/293: reduce(for i in range (20):i)
15/294: reduce(for i in range (20),i)
15/295: reduce(for i in range (20),x+y,i)
15/296: y =[reduce(i for i in range (20),x+y,i)]
15/297: y =[reduce((i) for i in range (20),x+y,i)]
15/298: y =[reduce( for i in range (20),x+y,i)]
15/299: y =[reduce(i for i in range (20),x+y,i)]
15/300: y =[reduce(j for i in range (20),x+y,i)]
15/301: y =[reduce({i} for i in range (20),x+y,i)]
15/302: y =[reduce({i] for i in range (20),x+y,i)]
15/303: y =[reduce([i] for i in range (20),x+y,i)]
15/304: y =[reduce([i] for i in range (20):x+y,i)]
15/305: y =reduce([i] for i in range (20):x+y,i)
15/306: y =reduce( for i in range (20):x+y,i)
15/307:
def jkl ():
    for i in range(20):
        continue
y =reduce( jkl:x+y,i)
15/308:
def jkl ():
    for i in range(20):
        continue
y =reduce( jkl():x+y,i)
15/309:
def jkl ():
    for i in range(20):
        continue
y =reduce( lambda x,y:x+y,i)
15/310:
def jkl ():
    for i in range(20):
        continue
y =reduce( lambda x,y:x+y,jkl)
15/311:
def jkl ():
    for i in range(20):
        continue
y =reduce( lambda x,y:x+y,jkl())
15/312:
def jkl ():
    for i in range(20):
        k =[i]
y =reduce( lambda x,y:x+y,jkl())
15/313:
def jkl ():
    for i in range(20):
        k =[i]
y =reduce( lambda x,y:x+y,k
15/314:
def jkl ():
    for i in range(20):
        k =[i]
y =reduce( lambda x,y:x+y,k)
15/315:
def jkl ():
    for i in range(20):
        k =[i]
15/316:
def jkl ():
    for i in range(20):
        k =[i]
15/317:
def jkl ():
    for i in range(20):
        k =[i]
k
15/318:
def jkl ():
    for i in range(20):
        k =list.i
k
15/319:
def jkl ():
    for i in range(20):
        k =list(i)
k
15/320:
def jkl ():
    for i in range(20):
        
k
15/321:
def jkl ():
    for i in range(20):
        i
        
k
15/322:
def jkl ():
    for i in range(20):
        i
        
i
15/323:
def jkl ():
    for i in range(20):
        i
        
i
15/324:

    for i in range(20):
        i
        
i
15/325:
for i in range(20):
        k= list.i
        
i
15/326:
for i in range(20):
        k= list(i)
        
i
15/327:
for i in range(20):
        
        
i
15/328:
for i in range(20):
    i
        
        
i
15/329:
for i in range(20):
    i
15/330:
for i in range(20):
    i
15/331:
for i in range(20):
    ptint(i)
15/332:
for i in range(20):
    print(i)
15/333:
for i in range(20):
    k = {i}
15/334:
for i in range(20):
    k = [i]
15/335:
for i in range(20):
    k = [i]
        
        
k
15/336:
for i in range(20):
    k = k.append(i)
        
        
k
15/337:
for i in range(20):
    j = k.append(i)
        
        
j
15/338:
for i in range(20):
    j =[]
    j = k.append(i)
        
        
j
15/339:
for i in range(20):
    j =[]
    j = k.append(i)
        
        
j
15/340:
for i in range(20):
    j =[]
    k = j.append(i)
        
        
j
15/341:
for i in range(20):
    j =[]
    j = j.append(i)
        
        
j
15/342:
for i in range(20):
    j =[]
    j = j.append(i)
        
        
print(j)
15/343:
for i in range(20):
    j =[]
    j = j.append(i)
        
j        
print(j)
15/344:
for i in range(20):
    j =[]
    j = j.append(i)
15/345:
for i in range(20):
    j =[]
    j = j.append(i)
    j
15/346:
for i in range(20):
    j =[]
    j.append(i)
    j
15/347: y =reduce( lambda x,y:x+y,k)
15/348:
for i in range(20):
    j =[]
    j.append(i)
    j
15/349:
for i in range(20):
    j =[]
    j.append(i)
j
15/350:
for i in range(20):
    j =[]
    j.append(i)
print(j)
15/351:
for i in range(20):
    j.append(i)
print(j)
15/352:
for i in range(20):
    j.append(i)
print(j)
15/353:
for i in range(20):
    j.append(i)
print(j)
15/354:
for i in range(20):
    j.append(i)
print(j)
15/355:
for i in range(20):
    j.append(i)
print(j)
16/1:
for i in range(20):
    j.append(i)
print(j)
16/2:
for i in range(20):
    j.append(i)
print(j)
16/3:
for i in range(20):
    j.append(i)
print(j)
16/4:
for i in range(20):
    j.append(i)
print(j)
16/5:
j=[]
for i in range(20):
    j.append(i)
print(j)
16/6: y =reduce( lambda x,y:x+y,j)
16/7: y =reduce( lambda x,y: x+y,j)
16/8: y = reduce( lambda x,y: x+y,j)
16/9:
import reduce
y = reduce( lambda x,y: x+y,j)
16/10:
from functools import reduce
y = reduce( lambda x,y: x+y,j)
16/11:
from functools import reduce
y = reduce( lambda x,y: x+y,j)
16/12:
from functools import reduce
y = reduce( lambda x,y: x+y,j)
y
16/13:
from functools import reduce
y = reduce( lambda x,y: x*y,j)
y
16/14:
from functools import reduce
y = reduce( lambda x,y: x**y,j)
y
16/15:
from functools import reduce
y = reduce( lambda x,y: x-y,j)
y
16/16: sum(lst)
16/17: sum(j)
16/18: def mahesh ():
16/19:
def mahesh ():
    mahesn
16/20:
def mahesh ():
    mahesh
16/21:
class company():
    pass

bosch = company()
bosch.employeename= "mahesh"
bosch.age=12
bosh.salary=1200000
16/22:
class company():
    pass

bosch = company()
bosch.employeename= "mahesh"
bosch.age=12
bosch.salary=1200000
16/23:
def bosch ():
    mahesh 
bosch = company()
bosch.employeename= "mahesh"
bosch.age=12
bosch.salary=1200000
16/24:
def boschi ():
    mahesh 
boschi = company()
boschi.employeename= "mahesh"
boschi.age=12
boschi.salary=1200000
16/25:
def boschi ():
    mahesh 
boschi = company()
boschi.employeename= "mahesh"
boschi.age=12
boschi.salary=1200000
16/26:
def boschi ():
    mahesh 
boschi = company()
boschi.employeename= "mahesh"
boschi.age=12
boschi.salary=1200000
17/1:
def boschi ():
    mahesh 
boschi = company()
boschi.employeename= "mahesh"
boschi.age=12
boschi.salary=1200000
17/2:
def dompany ():
    mahesh 
boschi = dompany()
boschi.employeename= "mahesh"
boschi.age=12
boschi.salary=1200000
17/3:
def dompany ():
    mahesh 
boschi = dompany()
boschi.employeename= "mahesh"
boschi.age=12
boschi.salary=1200000    
print(
boschi.employeename,
boschi.age,
boschi.salary)
17/4:
class company():
    pass

bosch = company()
bosch.employeename= "mahesh"
bosch.age=12
bosch.salary=1200000

print(bosch.employeename
bosch.age
bosch.salary)
17/5:
class company():
    pass

bosch = company()
bosch.employeename= "mahesh"
bosch.age=12
bosch.salary=1200000

print(bosch.employeename,
bosch.age,
bosch.salary)
17/6:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing)
    ptr.coal = coal1
    ptr.petroleum = petroleum
    ptr.manufacturing = manufacturing
print(industry)
17/7:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal1
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
           
print(industry)
17/8:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal1
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
           
print([industry])
17/9:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal1
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
bot = industry()
bot('namw','sbsj',90)
print(bot)
17/10:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal1
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
bot = industry()
bot.('namw','sbsj',90)
print(bot)
17/11:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal1
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
bot = industry()
bot('namw','sbsj',90)
print(bot)
17/12:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal1
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
bot = industry('namw','sbsj',90)

print(bot)
17/13:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
bot = industry('namw','sbsj',90)

print(bot)
17/14:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
bot = industry('namw','sbsj',90)

print(bot.coal
     )
17/15:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def add(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)
17/16:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def add(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(add)
17/17:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def add(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(add())
17/18:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def add(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(add("robots"))
17/19:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def add(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(add("robots"))
17/20:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def adc(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(adc("robots"))
17/21:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def adc(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(bot.adc("robots"))
17/22:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def adc(ptr,robots):
        return robots+ptr.manufaturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(bot.adc("robots"))
17/23:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def adc(ptr,robots):
        return robots+ptr.manufacturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(bot.adc())
17/24:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def adc(ptr,robots):
        return robots+ptr.manufacturing
        
        
bot = industry('namw','sbsj',90)

print(bot.coal)    
print(bot.adc("asd"))
17/25:
class industry():
    def __init__(ptr,coal,petroleum,manufacturing):
        ptr.coal = coal
        ptr.petroleum = petroleum
        ptr.manufacturing = manufacturing
        
    def adc(ptr,robots):
        return robots+ptr.manufacturing
        
        
bot = industry('namw','sbsj','manufacturing')

print(bot.coal)    
print(bot.adc("asd"))
17/26:
import numpy
dic numpy
17/27:
import numpy
dic
17/28:
import numpy
dir
17/29:
import numpy
dir numpy
17/30:
import numpy
dir ()
17/31:
import numpy
dir (numpy)
17/32: __version__
17/33: python __version__
18/1:
class Person:
    def __init__ (ptr,name,age,gender):
18/2:
class Person:
    def __init__ (ptr,name,age,gender):
        ptr.name = name
        ptr.age = age
        ptr.gender = gender
    def age:
18/3:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return ""%s was born on %d and of gender %s" %(ptr.name,ptr.year,ptr.gender)

mahesh = person(mahesh,1999,male)    
print(person)
18/4:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

mahesh = person(mahesh,1999,male)    
print(person)
18/5:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

mahesh = person(mahesh,1999,male)    
print(mahesh)
18/6:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

maheshs = person(mahesh,1999,male)    
print(mahesh)
18/7:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

maheshs = person(mahesh,1999,male)    
print(maheshs)
19/1:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

maheshs = person(mahesh,1999,male)    
print(maheshs)
19/2:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

maheshs = person("mahesh",1999,"male")    
print(maheshs)
19/3:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
19/4:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
19/5:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
19/6:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s." % (ptr.name,ptr.year,ptr.gender,maheshs.age)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
19/7:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s%S" % (ptr.name,ptr.year,ptr.gender,maheshs.age)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
19/8:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s%s" % (ptr.name,ptr.year,ptr.gender,maheshs.age)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
19/9:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s" % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(mahehs.age(2019))
19/10:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s" % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2019))
19/11:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s" % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2020))
19/12:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s" % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(Person.age(2020))
19/13:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s" % (ptr.name,ptr.year,ptr.gender)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2020))
19/14:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age)

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2020))
19/15:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2020))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2020))
19/16:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2020))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
19/17:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
19/18:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
19/19:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
19/20:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
21/1:
import tensorflow as tf

# Create TensorFlow object called tensor
hello_constant = tf.constant('Hello World!')

with tf.Session() as sess:
        # Run the tf.constant operation in the session
        output = sess.run(hello_constant)
        print(output)
21/2:
import tensorflow as tf

# Create TensorFlow object called tensor
hello_constant = tf.constant('Hello World!')

with tf.Session() as sess:
        # Run the tf.constant operation in the session
        output = sess.run(hello_constant)
        print(output)
21/3: import tensorflow as tf
22/1: import tensorflow as tf
22/2: import tensorflow as tf
22/3: import tensorflow as tf
23/1: import tensorflow as tf
24/1: import tensorflow as tf
24/2:
hello_constant = tf.constant('Hello World!')
hello_constant
24/3:
import tensorflow as tf

# Create TensorFlow object called tensor
hello_constant = tf.constant('Hello World!')

with tf.Session() as sess:
        # Run the tf.constant operation in the session
        output = sess.run(hello_constant)
        print(output)
24/4:
# A is a 0-dimensional int32 tensor
A = tf.constant(1234)

# B is a 1-dimensional int32 tensor
B = tf.constant([123,456,789])

# C is a 2-dimensional int32 tensor
C = tf.constant([ [123,456,789], [222,333,444] ])
24/5:
x = tf.add(5, 2)  # 7
with tf.Session() as sess:
    output = sess.run(x)
    print(output)
24/6:
x = tf.subtract(10, 4) # 6
y = tf.multiply(2, 5)  # 10
24/7: tf.subtract(tf.constant(2.0),tf.constant(1))
24/8:
c = tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1
with tf.Session() as sess:
    output = sess.run(c)
    print(output)
24/9:
import tensorflow as tf


x = tf.constant(10)
y = tf.constant(2)
z = tf.subtract(tf.divide(x, y), 1)


with tf.Session() as sess:
    output = sess.run(z)
    print(output)
25/1:
class student(person):
    def __init__(ptr,studentname,person*kwargs)
    super(student,ptr).__init__(*kwargs) 
    ptr.studentname=studentname
25/2:
class student(person):
    def __init__(ptr,studentname,*kwargs)
    super(student,ptr).__init__(*kwargs) 
    ptr.studentname=studentname
25/3:
class student(person):
    def __init__(ptr,studentname,*args)
    super(student,ptr).__init__(*kwargs) 
    ptr.studentname=studentname
25/4:
class student(person):
    def __init__(ptr,studentname,*args):
    super(student,ptr).__init__(*kwargs) 
    ptr.studentname=studentname
25/5:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*kwargs) 
         ptr.studentname=studentname
25/6:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*kwargs) 
        ptr.studentname=studentname
25/7:
class Person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/8:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*kwargs) 
        ptr.studentname=studentname
25/9:
class person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/10:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*kwargs) 
        ptr.studentname=studentname
25/11:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*kwargs) 
        ptr.studentname=studentname
charlie = student('mahesh')
25/12:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
charlie = student('mahesh')
25/13:
class person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/14:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
charlie = student('mahesh')
25/15:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
charlie = student(1,'mahesh','2019','male')
25/16:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
charlie = student(1,'mahesh','2019','male') 
print(charlie)
25/17:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
charlie = student(1,'mahesh',2019,'male') 
print(charlie)
25/18:
class person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2022))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/19:
class person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2020))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/20:
class person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2020))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/21:
class person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2019))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/22:
class person:
    def __init__ (ptr,name,year,gender):
        ptr.name = name
        ptr.year = year
        ptr.gender = gender
    def age(ptr,currentyear):
        return currentyear - ptr.year
    def __str__(ptr):
        return "%s was born on %d and of gender %s %d" % (ptr.name,ptr.year,ptr.gender,maheshs.age(2018))

maheshs = Person("mahesh",1999,"male")    
print(maheshs)
print(maheshs.age(2021))
25/23:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
    def __str__(ptr):
        return charlie
charlie = student(1,'mahesh',2019,'male') 
print(charlie)
25/24:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
    def __str__(ptr):
        return "%d %s was born on %d and of gender %s %d" % (ptr.studentid,ptr.name,ptr.year,ptr.gender,maheshs.age(2018))
charlie = student(1,'mahesh',2019,'male') 
print(charlie)
25/25:
class student(person):
    def __init__(ptr,studentname,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentname=studentname
    def __str__(ptr):
        return "%d %s was born on %d and of gender %s %d" % (ptr.studentname,ptr.name,ptr.year,ptr.gender,maheshs.age(2018))
charlie = student(1,'mahesh',2019,'male') 
print(charlie)
25/26:
class student(person):
    def __init__(ptr,studentid,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentid=studentid
    def __str__(ptr):
        return "%d %s was born on %d and of gender %s %d" % (ptr.studentid,ptr.name,ptr.year,ptr.gender,maheshs.age(2018))
charlie = student(1,'mahesh',2019,'male') 
print(charlie)
25/27:
class student(person):
    def __init__(ptr,studentid,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentid=studentid
    def __str__(ptr):
        return "%d %s was born on %d and of gender %s %d" % (ptr.studentid,ptr.name,ptr.year,ptr.gender,maheshs.age(2020))
charlie = student(1,'mahesh',2019,'male') 
print(charlie)
25/28:
class student(person):
    def __init__(ptr,studentid,*args):
        super(student,ptr).__init__(*args) 
        ptr.studentid=studentid
    def __str__(ptr):
        return "%d %s was born on %d and of gender %s %d" % (ptr.studentid,ptr.name,ptr.year,ptr.gender,maheshs.age(2020))
charlie = student(1,'mahesh',1999,'male') 
print(charlie)
26/1:
class Tyres:
    def __init__(self, branch, belted_bias, opt_pressure):
        self.branch = branch
        self.belted_bias = belted_bias
        self.opt_pressure = opt_pressure
        
    def __str__(self):
        return ("Tyres: \n \tBranch: " + self.branch +
               "\n \tBelted-bias: " + str(self.belted_bias) + 
               "\n \tOptimal pressure: " + str(self.opt_pressure))
        
class Engine:
    def __init__(self, fuel_type, noise_level):
        self.fuel_type = fuel_type
        self.noise_level = noise_level
        
    def __str__(self):
        return ("Engine: \n \tFuel type: " + self.fuel_type +
                "\n \tNoise level:" + str(self.noise_level))
        
class Body:
    def __init__(self, size):
        self.size = size
        
    def __str__(self):
        return "Body:\n \tSize: " + self.size
        
class Car:
    def __init__(self, tyres, engine, body):
        self.tyreso = tyreso
        self.engine = engine
        self.body = body
        
    def __str__(self):
        return str(self.tyres) + "\n" + str(self.engine) + "\n" + str(self.body)

        
t = Tyres('Pirelli', True, 2.0)
e = Engine('Diesel', 3)
b = Body('Medium')
c = Car(t, e, b)
print(c)
26/2:
class Tyres:
    def __init__(self, branch, belted_bias, opt_pressure):
        self.branch = branch
        self.belted_bias = belted_bias
        self.opt_pressure = opt_pressure
        
    def __str__(self):
        return ("Tyres: \n \tBranch: " + self.branch +
               "\n \tBelted-bias: " + str(self.belted_bias) + 
               "\n \tOptimal pressure: " + str(self.opt_pressure))
        
class Engine:
    def __init__(self, fuel_type, noise_level):
        self.fuel_type = fuel_type
        self.noise_level = noise_level
        
    def __str__(self):
        return ("Engine: \n \tFuel type: " + self.fuel_type +
                "\n \tNoise level:" + str(self.noise_level))
        
class Body:
    def __init__(self, size):
        self.size = size
        
    def __str__(self):
        return "Body:\n \tSize: " + self.size
        
class Car:
    def __init__(self, tyres, engine, body):
        self.tyres = tyres
        self.engine = engine
        self.body = body
        
    def __str__(self):
        return str(self.tyres) + "\n" + str(self.engine) + "\n" + str(self.body)

        
t = Tyres('Pirelli', True, 2.0)
e = Engine('Diesel', 3)
b = Body('Medium')
c = Car(t, e, b)
print(c)
29/1: f= open("mahesh','r')
29/2:
f= open("mahesh','w')
f.write("test")
29/3:
f= open("mahesh",'w')
f.write("test")
29/4:
f= open("mahesh",'w')
f.write("test") 
f.read('mahesh')
29/5:
f= open("mahesh",'w')
f.write("test") 
f= open("mahesh",'r')
f.read('mahesh')
29/6:
f= open("mahesh",'w')
f.write("test") 
f= open("mahesh",'r')
29/7:
f= open("mahesh",'w')
f.write("test") 
f= open("mahesh",'r')
29/8:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    printf("error 404")
29/9:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
       printf("error 404")
29/10:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    printf("error 404")
29/11:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
29/12:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
29/13:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
29/14:
try:
    f= open("mahesh",'w')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
29/15:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
29/16:
if:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
29/17:
if:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
29/18:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
29/19:
try:
    f= open("mahesh",'r')
    f.write("test") 
except:
    print("error 404")
else:
    print("hello")
finally:
    print('gotya')
29/20: def paslen():
29/21:
def paslen():
    while true:
        try:
            val = input("enter the password of 8 letters")
            val.length==8
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
        finally:
            print('thankyou')
29/22: paslen()
29/23:
def paslen():
    while True:
        try:
            val = input("enter the password of 8 letters")
            val.length==8
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
        finally:
            print('thankyou')
29/24: paslen()
31/1: paslen()
31/2: paslen()
31/3:
def paslen():
    while True:
        try:
            val = input("enter the password of 8 letters")
            val.length==8
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
31/4: paslen()
32/1: paslen()
32/2:
def paslen():
    while True:
        try:
             val.length==8
            val = input("enter the password of 8 letters")
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
32/4: paslen()
32/5:
def paslen():
    while True:
        try:
            val.length==8
            val = input("enter the password of 8 letters")
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
32/6: paslen()
33/1:
def paslen():
    while True:
        try:
            val = input("enter the password of 8 letters")
             val.index==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
33/2:
def paslen():
    while True:
        try:
            val = input("enter the password of 8 letters")
             val.index == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
33/3:
def paslen():
    while True:
        try:
            val = input("enter the password of 8 letters")
             val.len() == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
33/4:
def paslen():
    while True:
        try:
            val = input("enter the password of 8 letters")
            val.len() == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
33/5: paslen()
34/1:
m = input()
print(m.len())
m
34/2:
def paslen():
    while True:
        try:
            val = int(input("enter the password of 8 letters"))
            val.len() == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
34/3: paslen()
35/1:
m = int(input())
print(m.len())
m
36/1:
m = int(input())
print(m.len())
m
36/2:
m = 123456
m.len()
36/3:
m = "123dfdf"
m.len()
36/4:
m = "123dfdf"
len(m)
36/5:
def paslen():
    while True:
        try:
            val = int(input("enter the password of 8 letters"))
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
36/6: paslen()
37/1:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/2: paslen()
37/3: paslen()
37/4: paslen()
37/5:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
            is len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/6:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
            is: len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/7:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
            if len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/8:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
            if :
                len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/9:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
            if:
                len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/10:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
            if:
                len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/11:
def paslen():
    while True:
        try:
            val = (input("enter the password of 8 letters"))
        if:
                len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/12:
def paslen():
    while True:
        try:
            val = 
            input()
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/13:
def paslen():
    while True:
        try:
            val = input()
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/14: paslen()
37/15:
def paslen():
    while True:
        try:
            if: len(val) == 8
                val = input()
            
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/16:
def paslen():
    while True:
        try:
            len(val) == 8
                val = input()
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/17:
def paslen():
    while True:
        try:
            len(val) == 8
            val = input()
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/18: paslen()
37/19:
def paslen():
    while True:
        try:
            val = input()
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/20:
def paslen():
    while True:
        try:
            val = input()
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/21: paslen()
37/22:
def paslen():
    while True:
        try:
             len(val) = 0
            val = input()
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/24:
def paslen():
    while True:
        try:
            len(val) = 0
            val = input()
            len(val) == 8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/25:
def paslen():
    while True:
        try:
            len(val) = 0
            val = len(input())==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/26:
def paslen():
    while True:
        try:
            val = len(input())==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/27: paslen()
37/28:
def paslen():
    while True:
        try:
            val = len(input())==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/29:
def paslen():
    while True:
        try:
            val = len(input())==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/30: paslen()
37/31:
def paslen():
    while False:
        try:
            val = len(input())==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/32: paslen()
37/33: paslen()
37/34:
def paslen():
    while Talse:
        try:
            val = len(input())==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/35:
def paslen():
    while True:
        try:
            val = len(input())==8
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/36: paslen()
37/37:
def paslen():
    while True:
        try:
            val = len(input()==8)
           
        except:
            print("not 8 letters")
            continue
        else:
            print("thats correct")
            break
        finally:
            print('thankyou')
37/38: paslen()
38/1:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!") 
  
# Driver Code      
if __name__ == '__main__': 
    main()
38/2:
# Password validation in Python 
# using naive method 

# Function to validate the password 
def password_check(passwd): 
    
    SpecialSym =['$', '@', '#', '%'] 
    val = True
    
    if len(passwd) < 6: 
        print('length should be at least 6') 
        val = False
        
    if len(passwd) > 20: 
        print('length should be not be greater than 8') 
        val = False
        
    if not any(char.isdigit() for char in passwd): 
        print('Password should have at least one numeral') 
        val = False
        
    if not any(char.isupper() for char in passwd): 
        print('Password should have at least one uppercase letter') 
        val = False
        
    if not any(char.islower() for char in passwd): 
        print('Password should have at least one lowercase letter') 
        val = False
        
    if not any(char in SpecialSym for char in passwd): 
        print('Password should have at least one of the symbols $@#') 
        val = False
    if val: 
        return val 

# Main method 
def main(): 
    passwd = 'Geek12@'
    
    if (password_check(passwd)): 
        print("Password is valid") 
    else: 
        print("Invalid Password !!") 
        
# Driver Code        
if __name__ == '__main__': 
    main()
38/3:
# Python program to demonstrate the use of 
# len() method 

# Length of below string is 5 
string = "geeks"
print(len(string)) 

# Length of below string is 15 
string = "geeks for geeks"
print(len(string))
38/4:
# Password validation in Python 
# using naive method 

# Function to validate the password 
def password_check(passwd): 
    
    SpecialSym =['$', '@', '#', '%'] 
    val = True
    
    if len(passwd) < 6: 
        print('length should be at least 6') 
        val = False
        
    if len(passwd) > 20: 
        print('length should be not be greater than 8') 
        val = False
        
    if not any(char.isdigit() for char in passwd): 
        print('Password should have at least one numeral') 
        val = False
        
    if not any(char.isupper() for char in passwd): 
        print('Password should have at least one uppercase letter') 
        val = False
        
    if not any(char.islower() for char in passwd): 
        print('Password should have at least one lowercase letter') 
        val = False
        
    if not any(char in SpecialSym for char in passwd): 
        print('Password should have at least one of the symbols $@#') 
        val = False
    if val: 
        return val 

# Main method 
def main(): 
    passwd = 'Geek12@'
    
    if (password_check(passwd)): 
        print("Password is valid") 
    else: 
        print("Invalid Password !!")
38/5:
# Password validation in Python 
# using naive method 

# Function to validate the password 
def password_check(passwd): 
    
    SpecialSym =['$', '@', '#', '%'] 
    val = True
    
    if len(passwd) < 6: 
        print('length should be at least 6') 
        val = False
        
    if len(passwd) > 20: 
        print('length should be not be greater than 8') 
        val = False
        
    if not any(char.isdigit() for char in passwd): 
        print('Password should have at least one numeral') 
        val = False
        
    if not any(char.isupper() for char in passwd): 
        print('Password should have at least one uppercase letter') 
        val = False
        
    if not any(char.islower() for char in passwd): 
        print('Password should have at least one lowercase letter') 
        val = False
        
    if not any(char in SpecialSym for char in passwd): 
        print('Password should have at least one of the symbols $@#') 
        val = False
    if val: 
        return val 

# Main method 
def main(): 
    passwd = 'Geek12@'
    
    if (password_check(passwd)): 
        print("Password is valid") 
    else: 
        print("Invalid Password !!")
38/6:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!")
38/7:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!")
38/8:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!")
38/9:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!")
38/10:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!") 
  
# Driver Code      
if __name__ == '__main__': 
    main()
38/11:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!")
38/12:
# importing re library 
import re 
  
def main(): 
    passwd = 'Geek12@'
    reg = "^(?=.*[a-z])(?=.*[A-Z])(?=.*\d)(?=.*[@$!%*#?&])[A-Za-z\d@$!#%*?&]{6,20}$"
      
    # compiling regex 
    pat = re.compile(reg) 
      
    # searching regex                  
    mat = re.search(pat, passwd) 
      
    # validating conditions 
    if mat: 
        print("Password is valid.") 
    else: 
        print("Password invalid !!")
38/13:
# Password validation in Python 
# using naive method 

# Function to validate the password 
def password_check(passwd): 
    
    SpecialSym =['$', '@', '#', '%'] 
    val = True
    
    if len(passwd) < 6: 
        print('length should be at least 6') 
        val = False
        
    if len(passwd) > 20: 
        print('length should be not be greater than 8') 
        val = False
        
    if not any(char.isdigit() for char in passwd): 
        print('Password should have at least one numeral') 
        val = False
        
    if not any(char.isupper() for char in passwd): 
        print('Password should have at least one uppercase letter') 
        val = False
        
    if not any(char.islower() for char in passwd): 
        print('Password should have at least one lowercase letter') 
        val = False
        
    if not any(char in SpecialSym for char in passwd): 
        print('Password should have at least one of the symbols $@#') 
        val = False
    if val: 
        return val 

# Main method 
def main(): 
    passwd = 'Geek12@'
    
    if (password_check(passwd)): 
        print("Password is valid") 
    else: 
        print("Invalid Password !!") 
        
# Driver Code        
if __name__ == '__main__': 
    main()
38/14:
# Password validation in Python 
# using naive method 

# Function to validate the password 
def password_check(passwd): 
    
    SpecialSym =['$', '@', '#', '%'] 
    val = True
    
    if len(passwd) < 6: 
        print('length should be at least 6') 
        val = False
        
    if len(passwd) > 20: 
        print('length should be not be greater than 8') 
        val = False
        
    if not any(char.isdigit() for char in passwd): 
        print('Password should have at least one numeral') 
        val = False
        
    if not any(char.isupper() for char in passwd): 
        print('Password should have at least one uppercase letter') 
        val = False
        
    if not any(char.islower() for char in passwd): 
        print('Password should have at least one lowercase letter') 
        val = False
        
    if not any(char in SpecialSym for char in passwd): 
        print('Password should have at least one of the symbols $@#') 
        val = False
    if val: 
        return val 

# Main method 
def main(): 
    passwd = 'Geek12@'
    
    if (password_check(passwd)): 
        print("Password is valid") 
    else: 
        print("Invalid Password !!") 
        
# Driver Code        

    main()
38/15:
# Password validation in Python 
# using naive method 

# Function to validate the password 
def password_check(passwd): 
    
    SpecialSym =['$', '@', '#', '%'] 
    val = True
    
    if len(passwd) < 6: 
        print('length should be at least 6') 
        val = False
        
    if len(passwd) > 20: 
        print('length should be not be greater than 8') 
        val = False
        
    if not any(char.isdigit() for char in passwd): 
        print('Password should have at least one numeral') 
        val = False
        
    if not any(char.isupper() for char in passwd): 
        print('Password should have at least one uppercase letter') 
        val = False
        
    if not any(char.islower() for char in passwd): 
        print('Password should have at least one lowercase letter') 
        val = False
        
    if not any(char in SpecialSym for char in passwd): 
        print('Password should have at least one of the symbols $@#') 
        val = False
    if val: 
        return val 

# Main method 
def main(): 
    passwd = 'Geek12@'
    
    if (password_check(passwd)): 
        print("Password is valid") 
    else: 
        print("Invalid Password !!") 
        
# Driver Code        

    main()
38/16:
# Password validation in Python 
# using naive method 

# Function to validate the password 
def password_check(passwd): 
    
    SpecialSym =['$', '@', '#', '%'] 
    val = True
    
    if len(passwd) < 6: 
        print('length should be at least 6') 
        val = False
        
    if len(passwd) > 20: 
        print('length should be not be greater than 8') 
        val = False
        
    if not any(char.isdigit() for char in passwd): 
        print('Password should have at least one numeral') 
        val = False
        
    if not any(char.isupper() for char in passwd): 
        print('Password should have at least one uppercase letter') 
        val = False
        
    if not any(char.islower() for char in passwd): 
        print('Password should have at least one lowercase letter') 
        val = False
        
    if not any(char in SpecialSym for char in passwd): 
        print('Password should have at least one of the symbols $@#') 
        val = False
    if val: 
        return val 

# Main method 
def main(): 
    passwd = 'Geek12@'
    
    if (password_check(passwd)): 
        print("Password is valid") 
    else: 
        print("Invalid Password !!") 
        
# Driver Code        
main()
38/17: import sqlite3
38/18:
import sqlite3
db = sqlite3.connect("mydatabase")
38/19:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,text,score) values(1,mahesh,90)")
38/20:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,text,score) values(1,'mahesh',90)")
38/21:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
38/22:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(1,mahesh,90)")
db.execute("insert into grade1(id,text,score) values(1,mahesh,90)")
38/23:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(1,mahesh,90)")
db.execute("insert into grade1(id,text,score) values(1,mahesh,90)")
38/24:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(2,mahesh,90)")
db.execute("insert into grade1(id,text,score) values(1,mahesh,90)")
38/25:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(2,mahes,70)")
db.execute("insert into grade1(id,text,score) values(3,mahsh,80)")
38/26:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(2,mahes,70)")
db.execute("insert into grade1(id,text,score) values(3,mahsh,80)")
38/27:
import sqlite3
db = sqlite3.connect("mydatabase")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(2,mahes,70)")
db.execute("insert into grade1(id,text,score) values(3,mahsh,80)")
db.commit()
38/28:
import sqlite3
db = sqlite3.connect("mydatabase1")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(2,mahes,70)")
db.execute("insert into grade1(id,text,score) values(3,mahsh,80)")
db.commit()
38/29:
import sqlite3
db = sqlite3.connect("mydatabase1")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,text,score) values(2,'mahes',70)")
db.execute("insert into grade1(id,text,score) values(3,'mahsh',80)")
db.commit()
38/30:
import sqlite3
db = sqlite3.connect("mydatabase1")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,name,score) values(2,'mahes',70)")
db.execute("insert into grade1(id,name,score) values(3,'mahsh,80)")
db.commit()
38/31:
import sqlite3
db = sqlite3.connect("mydatabase1")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,name,score) values(2,'mahes',70)")
db.execute("insert into grade1(id,name,score) values(3,mahsh,80)")
db.commit()
38/32:
import sqlite3
db = sqlite3.connect("mydatabase1")
db.execute("drop table if exists grade1")
db.execute("create table grade1 (id int,name text,score int)")
db.execute("insert into grade1(id,name,score) values(1,'mahesh',90)")
db.execute("insert into grade1(id,name,score) values(2,'mahes',70)")
db.execute("insert into grade1(id,name,score) values(3,'mahsh',80)")
db.commit()
38/33:
result = db.execute("select * from grade1 order by id")
for i in result:
    print(i)
print('-',*60)
38/34:
result = db.execute("select * from grade1 order by id")
for i in result:
    print(i)
print('-',* 60)
38/35:
result = db.execute("select * from grade1 order by id")
for row in result:
    print(row)
print('-',* 60)
38/36:
result = db.execute("select * from grade1 order by id")
for row in result:
    print(row)
print('-',* 60)
38/37:
result = db.execute("select * from grade1 order by id")
for row in result:
    print(row)
print("-",* 60)
38/38:
result = db.execute("select * from grade1 order by id")
for row in result:
    print(row)
print("-" * 60)
38/39:
result = db.execute("select * from grade1 order by id")
for row in result:
    print(row)
38/40:
result = db.execute("select * from grade1 order by id")
for row in result:
    print(row)
38/41:
result = db.execute("select * from grade1 order by score")
for row in result:
    print(row)
38/42:
result = db.execute("select * from grade1 order by score")
    print(row)
38/43:
result = db.execute("select * from grade1 order by score")
print(result)
38/44:
result = db.execute("select * from grade1 order by score")
print([result])
38/45:
result = db.execute("select * from grade1 order by score")
print(list.result)
38/46:
result = db.execute("select * from grade1 order by score")
print(list(result)
38/47:
result = db.execute("select * from grade1 order by score")
print(list(result))
38/48:
result = db.execute("select * from grade1 order by score")
print(list(result))
print("-" * 60)
38/49:
result = db.execute("select * from grade1 order by score")
print(list(result))
print("-" * 160)
38/50:
result = db.execute("select * from grade1 where name = mahesh")
print(list(result))
print("-" * 160)
38/51:
result = db.execute("select * from grade1 where name = 'mahesh'")
print(list(result))
print("-" * 160)
38/52:
result = db.execute("select  from grade1 where name = 'mahesh'")
print(list(result))
print("-" * 160)
38/53:
result = db.execute("select  *from grade1 where name = 'mahesh'")
print(list(result))
print("-" * 160)
38/54:
%%writefile test.txt
hello,this is a test
38/55: pwd
38/56: file = open('test.txt')
38/57: pwd
38/58: file.read()
38/59: file.write(new)
38/60: file.write('new')
38/61: file.read()
38/62:
file.seek(0)
file.read()
38/63: file = open('test.txt','w')
38/64: file.write('new')
38/65: file.read()
38/66: file = open('test.txt')
38/67: file.read()
38/68:
file.seek(0)
file.read()
38/69:
file.seek(0)
file.read()
38/70:
file.seek(0)
file.read()
38/71: file = open('test.txt',w+)
38/72: file = open('test.txt','w+'')
38/73: file = open('test.txt','w+')
38/74: file.write('new')
38/75:
file.seek(0)
file.read()
38/76: file.write('newadad')
38/77:
file.seek(0)
file.read()
38/78: file.write('newadad sjsnh jbdjbkj')
38/79:
file.seek(0)
file.read()
38/80: file.write(' sjsnh jbdjbkj')
38/81: file.write(' sjsnh jbdjbkj nbbnz')
38/82:
file.seek(0)
file.read()
38/83: lambda x:y x*y
38/84: lambda x:y, x*y
38/85:
x=5 y=6
lambda x:y, x*y
38/86:
x=5 
y=6
lambda x:y, x*y
40/1:
import numpy as np
import pandas as pd
np.set_printoptions(precision==4)
40/2:
import numpy as np
import pandas as pd
np.set_printoptions(precision==4)
40/3:
import numpy as np
import pandas as pd
np.set_printoptions(precision==4)
40/4:
import numpy as np
import pandas as pd
np.set_printoptions(pr==4)
40/5:
import numpy as np
import pandas as pd
np.set_printoptions()
40/6:
import numpy as np
import pandas as pd
np.set_printoptions(4)
40/7:
import pandas as pd
f = pd.read(test.csv)
40/8:
import pandas as pd
f = pd.read('test.csv')
40/9:
import pandas as pd
f = pd.read('C:\Users\Admin\Desktop\test.csv')
40/10:
import pandas as pd
f = pd.read('C\Users\Admin\Desktop\test.csv')
40/11:
import pandas as pd
f = pd.read(r'C\Users\Admin\Desktop\test.csv')
40/12:
import pandas as pd
f = pd.read('Users\Admin\Desktop\test.csv')
40/13:
import pandas as pd
f = pd.read_csv('Users\Admin\Desktop\test.csv')
40/14:
import pandas as pd
f = pd.read_csv(r'Users\Admin\Desktop\test.csv')
40/15:
import pandas as pd
f = pd.read_csv('c:\Users\Admin\Desktop\test.csv')
40/16:
import pandas as pd
f = pd.read_csv('C:\Users\Admin\Desktop\test.csv')
40/17:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv')
40/18:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv')
f
40/19:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test2.tsv')
f
40/20:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv')
f
40/21:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv')
f
40/22:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv'header = none)
f
40/23:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv'header = None)
f
40/24:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv' header = None)
f
40/25:
import pandas as pd
seek(0)
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv' header = None)
f
40/26:
import pandas as pd
f.seek(0)
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv' header = None)
f
40/27:
import pandas as pd
f.seek(0)
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv' header = None)
f
40/28:
import pandas as pd
f.seek(0)
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv', header = None)
f
40/29:
import pandas as pd

f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv', header = None)
f
40/30:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv', header = None)
f
40/31:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv',header=None)
f
40/32:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv',header=None)
f
40/33:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv',header=None)
f
40/34:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv', header=None)
f
40/35:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv', header = None)
f
40/36:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv')
f
40/37:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv', header=None)
f
40/38:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv',name=['a','b','c'])
f
40/39:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv',names=['a','b','c'])
f
40/40:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv',header=None)
f
40/41:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv')
f
40/42:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv', names=['a','b','c'])
f
40/43:
import pandas as pd
f = pd.read_csv(r'C:\Users\Admin\Desktop\test.csv', names=['a','b','c'])
type(f)
40/44: f.dtypes
40/45: f[['m h k k n']]
40/46: f[['m h k k n']].head()
40/47: f[['mhkkn']].head()
40/48: cat:C:\Users\Admin\Desktop\test2.tsv
40/49: cat:\Users\Admin\Desktop\test2.tsv
40/50: cat:Users\Admin\Desktop\test2.tsv
40/51: cat Users\Admin\Desktop\test2.tsv
40/52: Cat Users\Admin\Desktop\test2.tsv
40/53: Cat:test2.tsv
40/54: Cat test2.tsv
41/1: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx')
41/2:
import pandas as pd
f = pd.read_table(r'C:\Users\Admin\Desktop\test2.tsv')
f
41/3: pd.isnull(f)
41/4: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx')
41/5: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx','sheetname = c6 559')
41/6: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheetname = 'c6 559')
41/7: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheetname = 'C6 559')
41/8: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx', sheetname = 'C6 559')
41/9: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx', sheetname = 'C6559')
41/10: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheetname = 'C6559')
41/11: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheetname = 'HX 87')
41/12: pd.read_excel(r 'C:\Users\Admin\Desktop\25987.xlsx',sheetname = 'HX 87')
41/13: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheetname = 'HX 87')
41/14: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'HX 87')
41/15: pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
41/16: rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
41/17: rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
41/18:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(9)
41/19:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(2)
41/20:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(100)
41/21:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(10)
41/22: l =pd.read_csv('https://roscon.ros.org/world/2020/')
41/23: l =pd.read_html('https://roscon.ros.org/world/2020/')
41/24: l =pd.read('https://roscon.ros.org/world/2020/')
41/25: l =pd.read_clipboard('https://roscon.ros.org/world/2020/')
41/26:
l =pd.read_clipboard('https://roscon.ros.org/world/2020/')
l
41/27:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[0].ix[,0:20].head(10)
41/28:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[0].ix[:,0:20].head(10)
41/29:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[0].ix[:, 0:20].head(10)
41/30:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[0].ix[:, 0:20].head(1)
41/31:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[o].ix[:, o:20=o].head(1)
41/32:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[0].ix[:, 0:20=0].head(1)
41/33:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajjas.head(1)
41/34:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(1)
41/35:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(18)
41/36:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(25)
41/37:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(30)
41/38:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(35)
41/39:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(50)
41/40:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas.head(80)
41/41:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[[REMARKS]].head(80)
41/42:
rajaas = pd.read_excel(r'C:\Users\Admin\Desktop\25987.xlsx',sheet_name = 'C6 559')
rajaas[['REMARKS']].head(80)
41/43: f.to_csv('https://roscon.ros.org/world/2020/')
41/44:
 import requests
    url = 'https://canvas.instructure.com/courses/2341846'
    resp = requests.get(url) 
    resp
41/45:
import requests
url = 'https://canvas.instructure.com/courses/2341846'
resp = requests.get(url) 
resp
41/46:
import requests
url = 'https://grabify.link/MX3YSM'
resp = requests.get(url) 
resp
41/47:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while :  i = 0
         resp = requests.get(url) 
resp
41/48:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while  i = 0
         resp = requests.get(url) 
resp
41/49:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i = 0
         resp = requests.get(url) 
resp
41/50:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i = 0
         resp = requests.get(url) 
resp
41/51:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i = 0
         resp = requests.get(url) 
resp
41/52:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
  while i = 0
         resp = requests.get(url) 
resp
41/53:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
  while i = 0:
         resp = requests.get(url) 
resp
41/54:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
  while i = 0 :
         resp = requests.get(url) 
resp
41/55:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
  while i = 0:
        
        resp = requests.get(url) 
resp
41/56:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i = 0:
        
        resp = requests.get(url) 
resp
41/57:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i = 0:
    resp = requests.get(url) 
resp
41/58:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i = 5:
    resp = requests.get(url) 
resp
41/59:
import requests
url = 'https://grabify.link/MX3YSM'
i = 5
while i = 5:
    resp = requests.get(url) 
resp
41/60:
import requests
url = 'https://grabify.link/MX3YSM'
i = 5
while i < 5:
    resp = requests.get(url) 
resp
41/61:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i < 5:
    resp = requests.get(url) 
resp
41/62:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
while i < 5:
    resp = requests.get(url) 
resp
41/63:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
    resp = requests.get(url) 
except:
    while i < 5:
    resp = requests.get(url) 
    
resp
41/64:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
    resp = requests.get(url) 
except:
    while i < 5:
    resp = requests.get(url) 
finally:    
resp
41/65:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
    resp = requests.get(url) 
except:
    while i < 5:
    resp = requests.get(url) 
finally:    
     resp
41/66:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
    resp = requests.get(url) 
except:
    while i < 5:
    resp = requests.get(url) 
finally:    
         resp
41/67:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
    resp = requests.get(url) 
except:
    while i < 5:
    resp = requests.get(url) 
finally:    
        resp
41/68:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
    resp = requests.get(url) 
except:
    while i < 5:
    resp = requests.get(url) 
finally:    
      resp
41/69:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
    resp = requests.get(url) 
except:
    while i < 5:
    resp = requests.get(url) 
finally:
    resp
41/70:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
41/71:
import requests
url = 'https://grabify.link/MX3YSM'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
41/72:
import requests
url = 'http://test1'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
41/73:
import requests
url = 'https://grabify.link/DTAPXL'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
41/74:
import requests
url = 'http://test1'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
41/75:
import requests
url = 'https://grabify.link/DTAPXL'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
41/76:
import requests
url = 'https://grabify.link/DTAPXL'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
41/77:
import requests
url = 'https://leancoding.co/join.php?id=DTAPXL'
i = 0
try:
    while i < 5:
                resp = requests.get(url) 
except:
    while i < 5:
                 resp = requests.get(url) 
finally:
    resp
44/1:
EPOCHS = 30
VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,
                    validation_data=VALIDATION_SET)
44/2:
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = "SGD" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,
              optimizer=OPTIMIZER,
              metrics=METRICS)
44/3:
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(300, activation="relu", name="hiddenLayer1"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer2"),
          tf.keras.layers.Dense(10, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
44/4:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
41/78:
import cv2
img = mpimg.imread('eight.jpg') 
im = cv2.resize(img,  (28, 28)) 
im.shape
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

def rgb2gray(rgb):
    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])
   
gray = rgb2gray(im)    
plt.imshow(gray, cmap=plt.get_cmap('gray'), vmin=0, vmax=1)
plt.show()
gray = gray.reshape(1,28,28)
48/1:
import pygame
from pygame.color import THECOLORS
import pdb
import pymunk
from pymunk.vec2d import Vec2d
from pymunk.pygame_util import from_pygame, to_pygame
from pymunk.pygame_util import DrawOptions as draw
import pymunk.util as u
import random
import math
import numpy as np
from MakeItLearn import *

width = 600 # Width Of The Game Window
height = 600  # Height Of The Game Window
pygame.init()
screen = pygame.display.set_mode((width, height))
clock = pygame.time.Clock() # Game Clock
SummarySensorData = []
StepSizeValue = 1/20.0 # Step Size For Simulation
ClockTickValue = 100 # Clock Tick
BotSpeed = 25 # Speed Of The Bot
model = None

def PointsFromAngle(angle):
    ### Returns The Unit Direction Vector With Given Angle ###
    return math.cos(angle),math.sin(angle)
    
class BotEnv:
    def __init__(self):
        ## Initialize Required Variables
        self.crashed = False
        self.DetectCrash = 0
        self.space = pymunk.Space()
        self.BuildBot(50.01, 450.01, 20)
        self.walls = []
        self.WallShapes = []
        self.WallRects = []
        ## Add Walls To The Map ###
        WallBody, WallShape, WallRect = self.BuildWall(200, 50, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(200, 125, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(200, 550, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(200, 450, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(400, 350, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(400, 250, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(500, 250, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(600, 250, 50)
        self.WallRects.append(WallRect)
        WallBody, WallShape, WallRect = self.BuildWall(115, 1050, 500)
        self.WallRects.append(WallRect)
        self.PreviousAction = 0
        
    def BuildWall(self, x, y, r):
        ### Build Wall On The Map ###
        size = r
        WallRect = pygame.Rect(x-r,600-y-r, 2*r, 2*r)
        return WallRect,WallRect,WallRect

    def BuildBot(self, x, y, r):
        ### Build The Bot Object ###
        size = r
        BoxPoints = list(map(Vec2d, [(-size, -size), (-size, size), (size,size), (size, -size)]))
        mass  = 0.5
        moment = pymunk.moment_for_poly(mass,BoxPoints, Vec2d(0,0))
        self.Bot = pymunk.Body(mass, moment)
        self.Bot.position = (x,y) # Declare Bot Position
        self.Bot.angle = 1.54     # Set the Bot Angle
        BotDirection = Vec2d(PointsFromAngle(self.Bot.angle)) # Get The Direction Vector From Angle
        self.space.add(self.Bot)
        self.BotRect = pygame.Rect(x-r,600-y-r, 2*r, 2*r)
        return self.Bot
    
    def DrawEverything(self,flag=0):
        ### Write Everything On The Game Window ###
        img = pygame.image.load("./assets/intel.jpg")
        x, y = 580,550
        AdjustedImagePosition = (x-50,y+50)
        screen.blit(img,to_pygame(AdjustedImagePosition,screen))
        if(flag==0 and self.DetectCrash == 0):
            (self.BotRect.x,self.BotRect.y) = self.Bot.position[0],600-self.Bot.position[1]
            self.CircleRect = pygame.draw.circle(screen, (169,169,169), (self.BotRect.x,self.BotRect.y), 20, 0)
        ## If Collision Detected Draw Green
        elif(flag==0 and self.DetectCrash >= 1):
            (self.BotRect.x,self.BotRect.y) = self.Bot.position[0],600-self.Bot.position[1]
            self.CircleRect = pygame.draw.circle(screen, (0,255,0), (self.BotRect.x,self.BotRect.y), 20, 0)
        ## If Collision Happens Draw Red
        else:
            (self.BotRect.x,self.BotRect.y) = self.Bot.position[0],600-self.Bot.position[1]
            self.CircleRect = pygame.draw.circle(screen, (255,0,0), (self.BotRect.x,self.BotRect.y), 20, 0)
        img = pygame.image.load("./assets/spherelight.png")
        offset = Vec2d(img.get_size()) / 2.0
        x, y =  self.Bot.position
        y = 600.0 -y
        AdjustedImagePosition = (x,y) - offset
        screen.blit(img,AdjustedImagePosition)
        for ob in self.WallRects:
            pygame.draw.rect(screen, (169,169, 169), ob)
    
    def PlanAngle(self,A,B):
        ### Find The Angle Between Two Vector ###
        angle = np.arctan2(B[1] - A[1], B[0] - A[0])
        return angle
        
    def _step(self, action, CrashStep=0):
        ### Take The Simulation One Step Further ###
        self.Bot.angle = self.Bot.angle % 6.2831853072
        ### If Action Is Left
        if action == 3:  
            self.Bot.angle -= 0.02
            self.PreviousBodyAngle =  self.Bot.angle
            self.BotDirection = Vec2d(PointsFromAngle(self.Bot.angle))
            BotDirection = self.BotDirection
            if(CrashStep > 0):
                self.Bot.velocity = BotSpeed/3 * BotDirection
            else:
                self.Bot.velocity = BotSpeed * BotDirection
            self.PreviousAction = 3
        ### If Action Is Right
        elif action == 4:
            self.Bot.angle += 0.02
            self.PreviousBodyAngle =  self.Bot.angle
            self.BotDirection = Vec2d(PointsFromAngle(self.Bot.angle))
            BotDirection = self.BotDirection
            self.Bot.velocity = BotSpeed * BotDirection
            if(CrashStep == 1):
                self.Bot.velocity = BotSpeed/3 * BotDirection
            else:
                self.Bot.velocity = BotSpeed * BotDirection
            self.PreviousAction = 4
        ### If Action Is Straight
        elif action == 5:
            self.Bot.angle += 0.
            self.PreviousBodyAngle =  self.Bot.angle
            self.BotDirection = Vec2d(PointsFromAngle(self.Bot.angle))
            BotDirection = self.BotDirection
            self.Bot.velocity = BotSpeed * BotDirection
            if(CrashStep == 1):
                self.Bot.velocity = BotSpeed/3 * BotDirection
            else:
                self.Bot.velocity = BotSpeed * BotDirection
                
        screen.fill(THECOLORS["white"]) ## Clear The Screen
        self.DrawEverything()          ## Write Everything To The Game
        self.space.step(StepSizeValue)## Take One Step In Simulation
        clock.tick(ClockTickValue)    ## Tick The Clock
        x, y = self.Bot.position        ## Get The Bot Position
        SensorsData = self.AllSensorSensorsData(x, y, self.Bot.angle) ## Get All The Sensor Data
        NormalizedSensorsData = [(x-100.0)/100.0 for x in SensorsData] ## Normalize The Sensor Values
        state = np.array([NormalizedSensorsData])
        SensorsData = np.append(SensorsData,math.degrees(self.Bot.angle))
        SensorsData = np.append(SensorsData,[0])
        print(SensorsData[:-2]) ## Print The Sensor Data
        DataTensor = torch.Tensor(SensorsData[:-1]).view(1,-1)
        for ob in self.WallRects:
            if ob.colliderect(self.CircleRect):
                    self.RecoverFromCrash(BotDirection)
        if (x >= 580 or x <= 20 or y <= 20 or y >=680):
                    self.RecoverFromCrash(BotDirection)
        SignalData = SensorsData[:-2]
        if 1 in SignalData:
            if(action == 5):
                action = self.PreviousAction
            self.crashed = True
            SensorsData[-1] = 1
            SummarySensorData.append(SensorsData)
            print(SensorsData[:-2])
            reward = -500
            self.RecoverFromCrash(BotDirection)
        else:
            self.DetectCrash = 0
            SummarySensorData.append(SensorsData)
        return

    def RecoverFromCrash(self, BotDirection):
        ### Execute Following When Bot Crashes ###
        while self.crashed:
            self.crashed = False 
            for i in range(1):
                self.Bot.angle += 3.14
                self.BotDirection = Vec2d(PointsFromAngle(self.Bot.angle))
                BotDirection = self.BotDirection
                self.Bot.velocity = BotSpeed * BotDirection
                screen.fill(THECOLORS["white"])
                self.DrawEverything(flag=1)
                self.space.step(StepSizeValue)
                pygame.display.flip()
                clock.tick(ClockTickValue)
                    
    def AllSensorSensorsData(self, x, y, angle):
        ### Return The All Sensor Values ###
        SensorsData = []
        MiddleSensorStartPoint = (25 + x, y)
        MiddleSensorEndPoint = (65 + x , y)
        NumberOfSensors = 5
        RelativeAngles = []
        AngleToBeginWith = 1.3
        OffsetIncrement =  (AngleToBeginWith*2)/(NumberOfSensors-1)
        RelativeAngles.append(-AngleToBeginWith)
        ## Generate Sensors
        for i in range(NumberOfSensors-1):
            RelativeAngles.append(RelativeAngles[i]+OffsetIncrement)
        SensorList = []
        ## Collect The Sensor Value From All Sensors
        for i in range(NumberOfSensors):
            SensorList.append([MiddleSensorStartPoint,MiddleSensorEndPoint, RelativeAngles[i]])
            SensorsData.append(self.SensorReading(SensorList[i], x, y, angle))
        pygame.display.update()
        return SensorsData

    def SensorReading(self, sensor, x, y, angle):
        ### Returns The Reading For A Single Sensor ###
        distance = 0
        (x1,y1) = sensor[0][0],sensor[0][1]
        (x2,y2) = sensor[1][0],sensor[1][1]
        SensorAngle = sensor[2]
        PixelsInPath = []
        NumberOfPoints = 100
        ## Generate Sensor Points
        for k in range(NumberOfPoints):
            x_new = x1 + (x2-x1) * (k/NumberOfPoints)
            y_new = y1 + (y2-y1) * (k/NumberOfPoints)
            PixelsInPath.append((x_new,y_new))
        for pixel in PixelsInPath:
            distance += 1
            PixelInGame = self.Rotate((x, y), (pixel[0], pixel[1]), angle + SensorAngle)
            SensorStartInGame = self.Rotate((x, y), (x1, PixelsInPath[-1][1]), angle + SensorAngle)
            SensorEndInGame = self.Rotate((x, y),  PixelsInPath[-1], angle + SensorAngle)
            if PixelInGame[0] <= 0 or PixelInGame[1] <= 0 or PixelInGame[0] >= width or PixelInGame[1] >= height:
                return distance
            else:
                for ob in self.WallRects:
                    if ob.collidepoint((PixelInGame[0],PixelInGame[1])): 
                        return distance
        ## Draw The Sensor
        pygame.draw.line(screen,(30,144,255),SensorStartInGame,SensorEndInGame)
        return distance
        
    def Rotate(self,origin, point, angle):
        ### Rotates A Point Along A Given Point ###
        x1, y1 = origin
        x2, y2 = point
        final_x = x1 + math.cos(angle) * (x2 - x1) - math.sin(angle) * (y2 - y1)
        final_y = y1 + math.sin(angle) * (x2 - x1) + math.cos(angle) * (y2 - y1)
        final_y = abs(width - final_y)
        return final_x,final_y
        
def TakeLeftOrRightTurn(env):
    ### Take Random Action - Left Or Right Turn ###
    x = random.randint(3,4)
    for i in range(40):
        env._step(x)

def GoStraight(env):
    ### Take Action To Go Straight ###
    x = random.randint(5,5)
    for i in range(40):
        env._step(x)
        

if __name__ == "__main__":
    env = BotEnv()
    random.seed(10)
    env._step(3)
    for i in range(200):
        if (random.random() > 0.5):
            TakeLeftOrRightTurn(env)
        else:
            GoStraight(env)
        np.savetxt("./SensorData/SensorData.txt",SummarySensorData)
49/1:
from __future__ import print_function
import torch
49/2:
from __future__ import print_function
import torch
49/3:
from __future__ import print_function
import torch
49/4:
a = torch.empty(6,3)
print(a)
49/5:
from __future__ import print_function
import torch
49/6:
from __future__ import print_function
import torch
49/7:
from __future__ import print_function
import torch
49/8:
from __future__ import print_function
import torch
50/1:
from __future__ import print_function
import torch
50/2:
a = torch.empty(6,3)
print(a)
50/3:
a = torch.empty(6,3)
print(a)
52/1: !pip install pandas
52/2: !pip install pandas
52/3:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
52/4:
!pip install pandas
!pip install matplotlib
52/5:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
52/6:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
52/7: dataset.head()
52/8: X.head()
52/9:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
52/10: geography
52/11:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
52/12: X.head()
52/13: X.head()
52/14: X.shape
52/15:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
52/16:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
52/17: dataset.head()
52/18:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
52/19:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
52/20: dataset.head()
52/21: dataset.isnull().sum()
52/22: X.head()
52/23: dataset.isnull().sum()
52/24: X.head()
52/25: y.head()
52/26:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
52/27: geography
52/28:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
52/29: X.shape
52/30: X.head()
52/31:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
52/32:
!pip install pandas
!pip install matplotlib
!pip install sklearn
52/33:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
52/34:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
52/35: X_train
52/36:
import tensorflow
print(tensorflow.__version__)
52/37:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
52/38:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
52/39: classifier.summary()
52/40: classifier.summary()
52/41:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
52/42: X_train.shape
52/43:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,callbacks = [histories])
52/44:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
52/45: !pip install keras-tuner
52/46: !pip install keras-tuner
52/47:
!pip install pandas
!pip install matplotlib
!pip install sklearn
52/48: tensorflow__version__()
52/49: print(tensorflow__version__())
52/50: print(tensorflow__version__
52/51: print(tensorflow__version__)
51/1:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,
                        validation_data=VALIDATION_SET)

def saveModel_path(model_dir="SAVED_MODELS"):
    os.makedirs(model_dir, exist_ok=True)
    fileName = time.strftime("Model_%Y_%m_%d_%H_%M_%S_.h5")    
    model_path = os.path.join(model_dir, fileName)
    print(f"your model will be saved at the following location\n{model_path}")
    return model_path
    UNIQUE_PATH = model_clf.save(saveModel_path())

model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
51/2: !pip install seaborn
51/3:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,
                        validation_data=VALIDATION_SET)

def saveModel_path(model_dir="SAVED_MODELS"):
    os.makedirs(model_dir, exist_ok=True)
    fileName = time.strftime("Model_%Y_%m_%d_%H_%M_%S_.h5")    
    model_path = os.path.join(model_dir, fileName)
    print(f"your model will be saved at the following location\n{model_path}")
    return model_path
    UNIQUE_PATH = model_clf.save(saveModel_path())

model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
51/4:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,
                        validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
53/1:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,
                        validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
53/2:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
54/1:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
52/52:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
52/53:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(330, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 60

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
54/2:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
54/3: X_train.shape
54/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
54/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
54/6:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
54/7:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
54/8: X_train.shape
54/9:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
54/10:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
54/11:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
54/12:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
54/13: X_train.shape
54/14:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
54/15:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
54/16:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/17:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/18:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/19:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/20:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/21: X_train.shape
54/22:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
54/23:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
54/24:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/25:
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/26: X_train.shape
54/27:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
54/28:
!pip install keras-tuner
import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/29:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/30: X_train.shape
54/31:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
54/32:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
54/33:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
54/34:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
54/35:
import tensorflow as tf
print(f"Tensorflow Version: {tf._version_}")
print(f"Keras Version: {tf.keras._version_}")

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
52/54:
import tensorflow as tf
print(f"Tensorflow Version: {tf._version_}")
print(f"Keras Version: {tf.keras._version_}")
52/55: print(f"Keras Version: {tf.keras._version_}")
52/56:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(330, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 60

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
54/36:
import tensorflow as tf
print(f"Tensorflow Version: {tf._version_}")
print(f"Keras Version: {tf.keras._version_}")

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
54/37:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
54/38: X_train.shape
54/39:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
57/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
57/2: X_train.shape
57/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
57/4:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
58/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
58/2: X_train.shape
58/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
58/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
58/5: tuner.search_space_summary()
59/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
59/2: X_train.shape
59/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
59/4:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
59/5: X_train.shape
59/6:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
59/7:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
59/8:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
59/9:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
59/10: tuner.search_space_summary()
59/11:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
59/12:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
59/13: tuner.get_best_hyperparameters
59/14: tuner.get_best_models()
59/15:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
59/16:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
59/17:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
59/18: tuner.search_space_summary()
59/19:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
59/20:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
59/21:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
59/22: X_train.shape
59/23:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
59/24:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
59/25: tuner.search_space_summary()
60/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
60/2: X_train.shape
60/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
60/4:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
60/5: tuner.search_space_summary()
62/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
62/2: X_train.shape
62/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
62/4:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
62/5: tuner.search_space_summary()
63/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
63/2: X_train.shape
63/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
63/4:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
63/5:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
63/6:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
63/7:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
63/8:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
63/9:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
63/10: X_train.shape
63/11:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
63/12:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
63/13: tuner.search_space_summary()
63/14:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
63/15: tuner.get_best_hyperparameters
63/16: tuner.get_best_models()
65/1:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(330, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 60

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
65/2:
!pip install pandas
!pip install matplotlib
!pip install sklearn
65/3:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(330, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 60

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
65/4:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(330, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 60

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
65/5: !pip install seaborn
65/6:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import os
import tensorflow as tf
import seaborn as sns
mnist = tf.keras.datasets.mnist
(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()
# Scale the data between 0 to 1 by dividing it by 255. as its an unsigned data between 0-255 range
X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.
y_valid, y_train = y_train_full[:5000], y_train_full[5000:]

# scale the test set as well
X_test = X_test / 255.
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(330, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 60

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)



model_clf.evaluate(X_test, y_test)
X_new = X_test[:3]
y_proba = model_clf.predict(X_new)
y_proba.round(2)
y_pred = np.argmax(model_clf.predict(X_new), axis=-1)
y_pred
y_test_new = y_test[:3]

for data, pred, actual in zip(X_new, y_pred, y_test_new):
    plt.imshow(data, cmap="binary")
    plt.title(f"Predicted: {pred}, Actual: {actual}")
    plt.axis('on')
    plt.show()
    print("---"*20)
65/7:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
65/8:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
65/9:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
65/10: dataset.head()
65/11: dataset.isnull().sum()
65/12: X.head()
65/13:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
65/14: geography
65/15:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
65/16: X.shape
65/17: X.head()
65/18:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
65/19:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
65/20: X_train
65/21:
import tensorflow
print(tensorflow.__version__)
65/22:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
65/23:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
65/24: classifier.summary()
65/25: X_train.shape
65/26:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
65/27: model_history.history.keys()
66/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
66/2: X_train.shape
66/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
66/4:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
66/5: !pip install keras-tuner
65/28:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
65/29:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
65/30:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
65/31: dataset.head()
65/32: dataset.isnull().sum()
65/33: X.head()
65/34: y.head()
65/35:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
65/36: geography
65/37:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
65/38: X.shape
65/39: X.head()
65/40:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
65/41:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
65/42: X_train
65/43:
import tensorflow
print(tensorflow.__version__)
65/44:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
65/45:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
65/46: classifier.summary()
65/47: X_train.shape
65/48:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
66/6: !pip install keras-tuner
66/7:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
66/8: X_train.shape
66/9:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
66/10:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
65/49:

! pip install tensorflow --upgrade
68/1:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
68/2:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
68/3:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
68/4: dataset.head()
68/5:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
68/6:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
68/7:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
68/8:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
68/9: dataset.head()
68/10: dataset.isnull().sum()
68/11: X.head()
68/12: y.head()
68/13:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
68/14: geography
68/15:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
68/16: X.shape
68/17: X.head()
68/18:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
68/19:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
68/20: X_train
68/21:
import tensorflow
print(tensorflow.__version__)
68/22:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
68/23:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
68/24: classifier.summary()
68/25: X_train.shape
68/26:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
68/27: model_history.history.keys()
68/28:
# list all data in history

print(model_history.history.keys())
# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
68/29:
# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
68/30:
# Part 3 - Making the predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)
68/31: y_pred
68/32:
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
68/33: cm
68/34:
# Calculate the Accuracy
from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,y_test)
68/35: score
69/1: "!pip install keras-tuner"
69/2:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
69/3: X_train.shape
69/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
69/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
69/6: tuner.search_space_summary()
69/7:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
69/8: !pip install keras-tuner
69/9:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
70/1:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
70/2: "!pip install keras-tuner"
70/3:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
70/4: X_train.shape
70/5:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
70/6:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
70/7: tuner.search_space_summary()
70/8:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
70/9: "!pip install keras-tuner"
70/10:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
70/11: X_train.shape
70/12:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
70/13:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
71/1: "!pip install keras-tuner"
71/2:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
71/3: X_train.shape
71/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
71/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
71/6: tuner.search_space_summary()
71/7:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
71/8: "!pip install keras-tuner"
71/9:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
71/10: X_train.shape
71/11:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
71/12:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
68/36:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
68/37:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
68/38:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
72/1: "!pip install keras-tuner"
72/2:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
72/3: X_train.shape
72/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
72/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
72/6: tuner.search_space_summary()
72/7:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
72/8: "!pip install keras-tuner"
72/9:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
72/10: X_train.shape
72/11:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
72/12:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
73/1: "!pip install keras-tuner"
73/2:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
73/3: X_train.shape
73/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
73/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
73/6: tuner.search_space_summary()
73/7:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
73/8:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
73/9: X_train.shape
73/10:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
73/11:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
74/1: "!pip install keras-tuner"
74/2:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
74/3: X_train.shape
74/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
74/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
74/6: tuner.search_space_summary()
74/7:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
78/1: "!pip install keras-tuner"
78/2:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
78/3: X_train.shape
78/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
78/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
78/6: tuner.search_space_summary()
78/7:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
77/1:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
77/2: X.head()
77/3:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
77/4:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
77/5:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
77/6: dataset.head()
78/8: tuner.get_best_hyperparameters
78/9: tuner.get_best_models()
77/7:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
77/8: X.head()
77/9:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
77/10:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
77/11: dataset.head()
77/12: dataset.isnull().sum()
77/13: X.head()
77/14: y.head()
77/15:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
77/16: geography
77/17:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
77/18: X.shape
77/19: X.head()
77/20:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
77/21:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
77/22: X_train
77/23:
import tensorflow
print(tensorflow.__version__)
77/24:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
77/25:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
77/26: classifier.summary()
77/27: X_train.shape
77/28:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
77/29:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
77/30:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
77/31: X_train.shape
77/32:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
77/33:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
77/34:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
77/35:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
77/36:

dataset.head()
77/37: dataset.isnull().sum()
77/38: X.head()
77/39: y.head()
77/40:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
77/41:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
77/42: X.shape
77/43: X.head()
77/44:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
77/45:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
77/46: X_train
77/47:
import tensorflow
print(tensorflow.__version__)
77/48:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
77/49:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
79/1:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
79/2:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
79/3: X.head()
79/4:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
79/5:

dataset.head()
79/6:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
79/7:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
79/8:

dataset.head()
79/9: dataset.isnull().sum()
79/10: X.head()
79/11: y.head()
79/12:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
79/13: geography
79/14:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
79/15: X.shape
79/16:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
79/17:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
79/18: X_train
79/19:
import tensorflow
print(tensorflow.__version__)
79/20:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
79/21:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
79/22: classifier.summary()
79/23: X_train.shape
79/24:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
79/25:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
78/10: "!pip install keras-tuner"
78/11:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
78/12: X_train.shape
78/13:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
78/14:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
78/15: tuner.search_space_summary()
78/16:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
78/17: tuner.get_best_hyperparameters
78/18: tuner.get_best_models()
79/26:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
80/1:
# list all data in history

print(model_history.history.keys())
# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
80/2:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
80/3:

X.head()
80/4:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
80/5:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
80/6:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
80/7:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
83/1:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
83/2: dataset.isnull().sum()
83/3:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
83/4: dataset.isnull().sum()
83/5:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
83/6:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
83/7:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
83/8:

dataset.head()
83/9: dataset.isnull().sum()
83/10: X.head()
83/11: y.head()
83/12:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
83/13: geography
83/14:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
83/15: X.shape
83/16:

X.head()
83/17:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
83/18:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
83/19: X_train
83/20:
import tensorflow
print(tensorflow.__version__)
83/21:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
83/22:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
83/23: classifier.summary()
83/24: X_train.shape
83/25:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
83/26: model_history.history.keys()
83/27:
# list all data in history

print(model_history.history.keys())
# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
83/28:
# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
83/29:
# Part 3 - Making the predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)
83/30: y_pred
83/31:
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
83/32: cm
83/33:
# Calculate the Accuracy
from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,y_test)
83/34: score
84/1:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
84/2: X_train.shape
84/3:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
84/4:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
84/5: tuner.search_space_summary()
84/6:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
84/7: tuner.get_best_hyperparameters
84/8: tuner.get_best_models()
92/1:
import pandas as pd

read_file = pd.read_csv (r'C:\Users\Admin\Desktop\conda tfgpu install.TXT')
read_file.to_csv (r'C:\Users\Admin\Desktop\condatf.csv', index=None)
92/2: a = pd.read_csv (r'C:\Users\Admin\Desktop\condatf.csv')
92/3:
a = pd.read_csv (r'C:\Users\Admin\Desktop\condatf.csv')
a
92/4:
a = pd.read_csv (r'C:\Users\Admin\Desktop\condatf.csv')
a[1]
92/5:
a = pd.read_csv (r'C:\Users\Admin\Desktop\condatf.csv')
a[:1]
92/6:
a = pd.read_csv (r'C:\Users\Admin\Desktop\condatf.csv')
a[:10]
92/7:
a = pd.read_csv (r'C:\Users\Admin\Desktop\condatf.csv')
a[:20]
92/8:
a = pd.read_csv (r'C:\Users\Admin\Desktop\condatf.csv')
a[:600]
93/1: "!pip install keras-tuner"
93/2:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
93/3: X_train.shape
93/4:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
93/5:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=5,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
93/6: tuner.search_space_summary()
93/7:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
93/8: tuner.get_best_hyperparameters
93/9: tuner.get_best_models()
93/10: tuner.result_summary()
93/11: tuner.results_summary()
93/12: "!pip install keras-tuner"
93/13:

import pandas as pd
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
93/14: X_train.shape
93/15:
def build_model(hp):
    model = keras.Sequential()
    for i in range(hp.Int('num_layers', 2, 20)):
        model.add(layers.Dense(units=hp.Int('units_' + str(i),
                                            min_value=32,
                                            max_value=512,
                                            step=32),
                               activation='relu'))
    model.add(layers.Dense(1, activation='sigmoid'))
    model.compile(
        optimizer=keras.optimizers.Adam(
            hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])),
        loss='binary_crossentropy',
        metrics=['accuracy'])
    return model
93/16:
from tensorflow import keras
from tensorflow.keras import layers
from kerastuner.tuners import RandomSearch
tuner = RandomSearch(
    build_model,
    objective='val_accuracy',
    max_trials=25,
    executions_per_trial=3,
    directory='project1',
    project_name='Churn')
93/17: tuner.search_space_summary()
93/18:
tuner.search(X_train, y_train,
             epochs=5,
             validation_data=(X_test, y_test))
95/1:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.5
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
95/2:
# Convolutional Neural Network

# Importing the libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
95/3: tf.__version__
95/4:
# Part 1 - Data Preprocessing

# Preprocessing the Training set
train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)
95/5:
training_set = train_datagen.flow_from_directory('Datasets/train',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('Datasets/test',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/6:
training_set = train_datagen.flow_from_directory('training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/7:
training_set = train_datagen.flow_from_directory('Desktop/tf2/cnn/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/8:
training_set = train_datagen.flow_from_directory('/Desktop/tf2/cnn/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/9:
training_set = train_datagen.flow_from_directory(r'/Desktop/tf2/cnn/training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/10:
training_set = train_datagen.flow_from_directory('C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/11:
training_set = train_datagen.flow_from_directory('\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/12:
training_set = train_datagen.flow_from_directory('Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/13:
training_set = train_datagen.flow_from_directory(r'Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/14:
# Convolutional Neural Network

# Importing the libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
95/15:
training_set = train_datagen.flow_from_directory(r'Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/16:
training_set = train_datagen.flow_from_directory(r 'Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/17:
training_set = train_datagen.flow_from_directory(r'Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/18:
training_set = train_datagen.flow_from_directory(r'Users\Admin\Desktop\tf2\cnn\training_set\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/19:
training_set = train_datagen.flow_from_directory('C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/20:
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/21:
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory('C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/22:
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (64, 64),
                                                 batch_size = 32,
                                                 class_mode = 'binary')

# Preprocessing the Test set
test_datagen = ImageDataGenerator(rescale = 1./255)
test_set = test_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (64, 64),
                                            batch_size = 32,
                                            class_mode = 'binary')
95/23: from tensorflow.keras.layers import Conv2D
95/24: Conv2D(padding="same")
95/25:
# Part 2 - Building the CNN

# Initialising the CNN
cnn = tf.keras.models.Sequential()

# Step 1 - Convolution
cnn.add(tf.keras.layers.Conv2D(filters=32,padding="same",kernel_size=3, activation='relu', input_shape=[64, 64, 3]))

# Step 2 - Pooling
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# Adding a second convolutional layer
cnn.add(tf.keras.layers.Conv2D(filters=32,padding='same',kernel_size=3, activation='relu'))
cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))

# Step 3 - Flattening
cnn.add(tf.keras.layers.Flatten())

# Step 4 - Full Connection
cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))

# Step 5 - Output Layer
cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))
95/26:
# Part 3 - Training the CNN

# Compiling the CNN
cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

# Training the CNN on the Training set and evaluating it on the Test set
cnn.fit(x = training_set, validation_data = test_set, epochs = 10)
95/27:
# save it as a h5 file


from tensorflow.keras.models import load_model

cnn.save('model_rcat_dog.h5')
95/28:
from tensorflow.keras.models import load_model
 
# load model
model = load_model('model_rcat_dog.h5')
95/29: model.summary()
95/30:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4016.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/31:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4016.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/32: result
95/33:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\cats\cat.4018.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/34: result
95/35:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/36:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4016.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/37: result
95/38:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/39:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/40:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4016.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/41: result
95/42:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/43:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\cats\cat.4018.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/44: result
95/45:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/46:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4018.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/47: result
95/48:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/49:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\cats\cat.4019.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/50: result
95/51:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/52:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4010.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/53: result
95/54:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/55: result
95/56:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4010.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/57: result
95/58:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4034.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/59: result
95/60:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/61: result
95/62:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4034.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/63: result
95/64:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/65: result
95/66:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4034.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/67:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4044.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/68: result
95/69:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4044.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/70:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/71:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/72: result
95/73:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/74:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4044.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/75: result
95/76:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/77:
if result[0]<=0.5:
    print("The image classified is cat")
else:
    print("The image classified is dog")
95/78:
# Part 4 - Making a single prediction

import numpy as np
from tensorflow.keras.preprocessing import image
test_image = image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\test_set\dogs\dog.4045.jpg', target_size = (64,64))
test_image = image.img_to_array(test_image)
test_image=test_image/255
test_image = np.expand_dims(test_image, axis = 0)
result = cnn.predict(test_image)
95/79: result
99/1:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
    train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
99/2:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
99/3:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
102/1:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
102/2: print(tf.__version__)
102/3:
import tensorflow as tf
print(tf.__version__)
102/4:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
104/1:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.5
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
104/2:
# Convolutional Neural Network

# Importing the libraries
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os
105/1:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/2:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/3:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/4:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/5:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D

from tf.keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/6:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D

from tensorflow.keras.layers.normalization import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/7:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D


df = pd.read_csv(r'C:\Users\Admin\Desktop\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/8:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D


df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/9:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import opencv as cv2


df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/10:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import opencv2 as cv2


df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/11:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2


df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/12:
import tensorflow as tf
print(tf.__version__)
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
105/13:
import tensorflow as tf
print(tf.__version__)
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
105/14:
import tensorflow as tf
print(tf.__version__)
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
105/15:
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics


import keras
from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization
105/16:
import tensorflow as tf
print(tf.__version__)
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
105/17:
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization
105/18:
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.models import Model, load_model
from keras import applications
from keras.callbacks import ReduceLROnPlateau

from keras.layers.normalization import BatchNormalization
105/19:
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers.normalization import BatchNormalization
105/20:
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
105/21:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/22:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 64
IMAGE_HEIGHT = 64
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/23:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 95
IMAGE_HEIGHT = 95
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/24:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 105
IMAGE_HEIGHT = 105
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/25:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 105
IMAGE_HEIGHT = 105
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
105/26:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/1:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 3
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("../input/inception.h5")

model.load_weights("../input/best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/2:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 7
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("C:\Users\Admin\Desktop\inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/3:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 7
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())

add_model.add(Dense(1024, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/4:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 7
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())
add_model.add(Dense(900, activation='relu',input_shape=base_model.output_shape)
add_model.add(Dense(204, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(124, activation='relu',input_shape=base_model.output_shape)              
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\cassava\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/5:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.7
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 7
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())
add_model.add(Dense(900, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(204, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(124, activation='relu',input_shape=base_model.output_shape))             
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\cassava\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/6:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 40
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())
add_model.add(Dense(948, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(248, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(128, activation='relu',input_shape=base_model.output_shape))             
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\cassava\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/7:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 40
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())
add_model.add(Dense(1208, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(248, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(128, activation='relu',input_shape=base_model.output_shape))             
add_model.add(Dropout(0.20))
add_model.add(Dense(15, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\cassava\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
108/8:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 40
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())
add_model.add(Dense(1248, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(248, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(128, activation='relu',input_shape=base_model.output_shape))             
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\cassava\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
112/1:
import numpy as np

a = np.arange(10) #using arange function
print(a)

b = np.array([1,2,3,4,5]) #creating array from list
print(b)
112/2:
#common arrays

d = np.ones((3,3,2))

d
112/3:
#common arrays

d = np.ones((3,3,3))

d
112/4:
#common arrays

d = np.ones((3,3))
d.reshape(2,3)

d
112/5:
#common arrays

d = np.ones((3,3))
d.reshape(3,3)

d
112/6:
#common arrays

d = np.ones((3,3))
d.reshape(3,3,2)

d
112/7:
#common arrays

d = np.ones((3,3))
d.reshape(3,3,3)

d
112/8:
#common arrays

d = np.ones((3,3))
d.reshape(3,3,1)

d
112/9:
#common arrays

d = np.ones((3,3)

d
112/10:
#common arrays

d = np.ones((3,3))


d
112/11:
#common arrays

d = np.ones((3,3))
d.reshape(4,3,2)

d
112/12:
#common arrays

d = np.ones((3,3))
d.reshape(4,3,2)

d
112/13:
#common arrays

d = np.ones((3,3))
d.reshape(3,4,2)

d
112/14:
#common arrays

d = np.ones((13,3))
d.reshape(3,4,2)

d
112/15:
#common arrays

d = np.ones((13,13))
d.reshape(3,4,2)

d
112/16:
#common arrays

d = np.ones((13,13))
d.reshape(13,13,2)

d
112/17:
#common arrays

d = np.ones((3,3))
d.reshape(3,3,2)

d
112/18:
#common arrays

d = np.ones((3,3))

d
112/19:
z = np.eye(3,2,2)

z
112/20:
z = np.eye(3,2,)

z
112/21:
z = np.eye(3,2)

z
112/22:
z = np.eye(3,2)

z
112/23:
a = np.array([1, 2, 3, 4, 5, 6]) 
a.resize(2,3)
112/24:
a = np.array([1, 2, 3, 4, 5, 6]) 
a.resize(2,3)
a
112/25: np.array([12.23,13.32,100,36.32])
112/26: a = np.array([12.23,13.32,100,36.32])
112/27:
a = np.array([12.23,13.32,100,36.32])
a
112/28:
a = np.arange([12.23,13.32,100,36.32])
a
112/29:
a = np.arrange([12.23,13.32,100,36.32])
a
112/30:
a = np.arange([12.23,13.32,100,36.32])
a
112/31:
a = np.array([12.23,13.32,100,36.32])
a
112/32: b = np.arrange(2,10).reshape(3,3)
112/33: b = np.arange(2,10).reshape(3,3)
112/34: b = np.arange(2,10)
112/35:
b = np.arange(2,10)
b
112/36:
b = np.arange(2,10).reshape(3,3)
b
112/37:
b = np.arange(2,10).reshize(3,3)
b
112/38:
b = np.arange(2,10).resize(3,3)
b
112/39:
b = np.arange(2,10).resize(3,3)
b
112/40:
b = np.arange(2,10).resize(3,3)
print(b)
112/41:
b = np.arange(2,10,3).resize(3,3)
print(b)
112/42:
b = np.arange(2,10,3)
print(b)
112/43:
b = np.arange(2,10)
print(b)
112/44:
b = np.arange(2,11)
print(b)
112/45:
b = np.arange(2,11,2)
print(b)
112/46:
b = np.arange(2,11)
print(b)
112/47:
b = np.arange(2,11).reshape()
print(b)
112/48:
b = np.arange(2,11).reshape(1)
print(b)
112/49:
b = np.arange(2,11).reshape(1,1)
print(b)
112/50:
b = np.arange(2,11).reshape(3,3)
print(b)
112/51:
b = np.arange(2,11).reshape(3,3,1)
print(b)
112/52:
b = np.arange(2,11).reshape(3,3)
print(b)
112/53:
b = np.arange(2,11).resize(3,3)
print(b)
112/54:
b = np.arange(2,11).reshape(3,3)
print(b)
112/55:
b = np.arange(2,11)
print(b)
112/56:
b = np.arange(2,11).reshape(3,3)
print(b)
112/57: c = np.sort(b)
112/58: c = np.sort(b)
112/59:
c = np.sort(b)
c
112/60:
c = np.sort(b , axis = 0)
c
112/61:
c = np.sort(b , axis = 1)
c
112/62:
c = np.sort(b , axis = -1)
c
112/63:
c = np.sort(b , axis =1)
c
112/64:
c = np.sort(b , axis =1)
c
112/65:
d = np.array([4,6] [2,1])
c = np.sort(d , axis =1)
c
112/66:
d = np.array([4,6],[2,1])
c = np.sort(d , axis =1)
c
112/67:
d = np.array(4,6,2,1).reshape(2,2)
c = np.sort(d , axis =1)
c
112/68:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
c
112/69:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
c1 = np.sort(d , axis =-1)
c2 = np.sort(d , axis =0)
print(c,c1,c2)
112/70:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
c1 = np.sort(d , axis =-1)
c2 = np.sort(d , axis =0)
print(c,c1,c2,end=/n)
112/71:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
c1 = np.sort(d , axis =-1)
c2 = np.sort(d , axis =0)
print(c,c1,c2,end=\n)
112/72:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
c1 = np.sort(d , axis =-1)
c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/73:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
c1 = np.sort(d , axis =-1)
#c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/74:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
#c1 = np.sort(d , axis =-1)
#c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/75:
d = np.array([4,6,2,1]).reshape(2,2)
#c = np.sort(d , axis =1)
#c1 = np.sort(d , axis =-1)
#c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/76:
d = np.array([4,6,2,1]).reshape(2,2)
#c = np.sort(d , axis =1)
#c1 = np.sort(d , axis =-1)
#c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/77:
d = np.array([4,6,2,1]).reshape(2,2)
#c = np.sort(d , axis =1)
#c1 = np.sort(d , axis =-1)
#c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/78:
d = np.array([4,6,2,1]).reshape(2,2)
#c = np.sort(d , axis =1)
#c1 = np.sort(d , axis =-1)
c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/79:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)
c1 = np.sort(d , axis =-1)
c2 = np.sort(d , axis =0)
print(c,c1,c2,end='\n')
112/80:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)

print(c,c1,c2,end='\n')
112/81:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)

print(c,end='\n')
112/82:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)

print(c1,end='\n')
112/83: c1 = np.sort(d , axis =-1)
112/84: c2 = np.sort(d , axis =0)
112/85:
c1 = np.sort(d , axis =-1)
c1
112/86: c2 = np.sort(d , axis =0)
112/87:
c2 = np.sort(d , axis =0)
c2
112/88:
d = np.array([4,6,2,1]).reshape(2,2)
c = np.sort(d , axis =1)

print(d,c1,end='\n')
112/89: c = np.sort(d , axis =1)
112/90:
c = np.sort(d , axis =1)
c
112/91:
d = np.array([4,6,2,1]).reshape(2,2)

print(d,end='\n')
112/92:
c = np.sort(d , axis =1)
c
112/93:
c1 = np.sort(d , axis =-1)
c1
112/94:
c1 = np.sort(d)
c1
112/95:
d = np.array([4,6,2,1]).reshape(1,2)

print(d,end='\n')
112/96:
d = np.array([4,6,2,1]).reshape(1,2)

print(d,end='\n')
112/97:
d = np.array([4,6,2,1]).reshape(2,2)

print(d,end='\n')
112/98:
c = np.sort(d , axis =1)
c
112/99:
c2 = np.sort(d ,axis =0)
c2
112/100: e = np.array([10,20,30,20,40,50])
112/101:
e = np.array([10,20,30,20,40,50])
e
112/102:
e = np.array([10,20,30,20,40,50]).reshape(1,2)
e
112/103:
e = np.array([10,20,30,20,40,50]).resize(1,2)
e
112/104:
e = np.array([10,20,30,20,40,50]).resize(1,2)
e
112/105:
e = np.array([10,20,30,20,40,50]).reshape(1,2)
e
112/106:
e = np.array([[10,20,30],[20,40,50]])
e.ravel()
112/107:
e = np.array([[10,20,30],[20,40,50]])
n = e.ravel()
112/108:
e = np.array([[10,20,30],[20,40,50]])
n = e.ravel()
n
112/109: i = np.arange(10,31)
112/110:
i = np.arange(10,31)
i
112/111:
i = np.random(10,31)
i
112/112:
i = np.random.random_integers(10,31,6)
i
112/113:
i = np.random.randint(10,31,6)
i
112/114:
r = np.array([[ 0.96336355 0.12339131 0.20295196 0.37243578 0.88105252] [ 0.93228246 0.67470158 0.38103235 0.32242645 0.40610231] [ 0.3113495 0.31688 0.79189089 0.08676434 0.60829874] [ 0.30360149 0.94316317 0.98142491 0.77222542 0.51532195] [ 0.97392305 0.16669609 0.81377917 0.2165645 0.00121611]])

r
112/115:
r = np.array([[ 0.96336355 0.12339131 0.20295196 0.37243578 0.88105252], [ 0.93228246 0.67470158 0.38103235 0.32242645 0.40610231], [ 0.3113495 0.31688 0.79189089 0.08676434 0.60829874], [ 0.30360149 0.94316317 0.98142491 0.77222542 0.51532195], [ 0.97392305 0.16669609 0.81377917 0.2165645 0.00121611]])

r
112/116:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

r
112/117:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.min()
112/118:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.minimum()
112/119:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.minimum()
112/120:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = r.minimum()
112/121:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = r.minimum()
o
112/122:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = r.mean()
o
112/123:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.maximum(r)
o
112/124: r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])
112/125:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])




r
112/126:
r = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.maximum(r)
o
112/127:
p = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.maximum(p)
o
112/128:
p = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.maximum([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])
)
o
112/129:
p = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.maximum([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]]))
o
112/130:
p = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.maximum([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])
o
112/131:
p = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.minimum([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])
o
112/132: t = np.array(5)
112/133:
t = np.array(5)
t
112/134:
t = np.array(5,2)
t
112/135:
t = np.array([0 1 2 3 4 5 6])
t
112/136:
t = np.array([0 ,1, 2, 3 ,4 ,5 ,6])
t
112/137:
t = np.array([0 ,1, 2, 3 ,4 ,5 ,6])
t = t**2
112/138:
t = np.array([0 ,1, 2, 3 ,4 ,5 ,6])
t = t**2
t
112/139:
t = np.array([0 ,1, 2, 3 ,4 ,5 ,6])
t = t**2
t.reduce()
112/140:
b = np.ones(10) + 1

print(a)
print(b)

print("b = ", b)

print("a - b = ",a-b)
112/141:
b = np.ones(10) + 1

print(a)
print(b)

print("b = ", b)

print("a - b = ",a-b)
112/142:
print(a)
print(a ** 2)
112/143:
a = np.arange(10)

print(a+1)
112/144:
b = np.ones(10) + 1

print(a)
print(b)

print("b = ", b)

print("a - b = ",a-b)
112/145:
b = np.ones(10) + 11

print(a)
print(b)

print("b = ", b)

print("a - b = ",a-b)
112/146:
b = np.ones(10) + 15

print(a)
print(b)

print("b = ", b)

print("a - b = ",a-b)
112/147:
a = np.arange(5)+10
print(a)
print(np.sin(a))
112/148:
a = np.arange(5)+1
print(a)
print(np.sin(a))
112/149:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u)
112/150:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u)
j
112/151:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,order=2)
j
112/152:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=none,order=2)
j
112/153:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=0,order=2)
j
112/154:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[0]
112/155:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[0:1]
112/156:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[0:1:1]
112/157:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[1:1:1]
112/158:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[1:1]
112/159:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[1]
112/160:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[1][1]
112/161:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
u[0][1]
112/162:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=0,order=2)
j
112/163:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,order=2)
j
112/164:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,order=2)
return j
112/165:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u)
return j
112/166:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u)
 j
112/167:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u)
j
112/168:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=0)
j
112/169:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=-1)
j
112/170:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=none)
j
112/171:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=-1)
j
112/172:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=1)
j
112/173:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=0)
j
112/174:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
j = np.sort(u,axis=0,order=2)
j
112/175: np.datetime_data()
112/176: r=np.datetime_data()
112/177:
r=np.datetime_data()
r
112/178:
r=np.datetime_data(all)
r
112/179: >>> np.datetime64('2010', np.datetime_data(dt_25s))
112/180: np.datetime64('2010', np.datetime_data(dt_25s))
112/181: np.datetime64('2017', np.datetime_data(dt_25s))
112/182:
np.datetime64('2017', np.datetime_data(dt_25s))
numpy.datetime64('2010-01-01T00:00:00','25s')
112/183:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)
('s', 25)
>>> np.array(10, dt_25s).astype('timedelta64[s]')
array(250, dtype='timedelta64[s]')


np.datetime64('2017', np.datetime_data(dt_25s))
numpy.datetime64('2010-01-01T00:00:00','25s')
112/184:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
numpy.datetime64('2010-01-01T00:00:00','25s')
112/185:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2010-01-01T00:00:00','25s')
112/186:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')
112/187:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01 T00:00:00','25s')
112/188:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')
112/189:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04',dtype=dt_25s)
112/190:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04',dtype= 'datetime64[d]')
112/191:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04',dtype= 'datetime64[D]')
112/192:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04',dtype= 'datetime64[T]')
112/193:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04',dtype= 'datetime64[D]')
112/194:
a = np.array([1, 2, 3, 4, 5, 6]) 
a.resize(3,3)
a
112/195:
a = np.array([1, 2, 3, 4, 5, 6]) 
a.resize(3,13)
a
112/196:
a = np.array([1, 2, 3, 4, 5, 6]) 
a.resize(1,13)
a
112/197:
a = np.array([1, 2, 3, 4, 5, 6]) 
a.resize(2,13)
a
112/198:
d = np.array([[2,5],[4,4]])

print(d,end='\n')
112/199:
d = np.array([[4 ,6],[2 ,1]])

print(d,end='\n')
112/200:
c = np.sort(d , axis =1)
c
112/201:
c = np.sort(d , axis =1)
c
112/202:
c1 = np.sort(d)
c1
112/203:
c2 = np.sort(d ,axis =0)
c2
112/204:
c2 = np.sort(d ,axis =0)
c2
112/205:
c1 = np.sort(d)
c1
112/206:
c = np.sort(d , axis =1)
c
112/207:
c = np.sort(d , axis =-1)
c
112/208:
c = np.sort(d ,axis =1)
c
112/209:
d = np.array([[4 ,6],[2 ,1]])

print(d,end='\n')
112/210:
c = np.sort(d ,axis =1)
c
112/211:
c1 = np.sort(d)
c1
112/212:
c2 = np.sort(d ,axis =0)
c2
112/213:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04', 'datetime64[D]')
np.datetime64
112/214:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04', dtype='datetime64[D]')
np.datetime64
112/215:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04', dtype= 'datetime64[D]')
np.datetime64
112/216:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04', dtype= 'datetime64[D]')
112/217:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04', )
112/218:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04' )
112/219:
>>> dt_25s = np.dtype('timedelta64[25s]')
>>> np.datetime_data(dt_25s)

np.datetime64('2017', np.datetime_data(dt_25s))
np.datetime64('2017-01-01T00:00:00','25s')

np.arange('2017-03','2017-04')
112/220:


np.arange('2017-03','2017-04')
112/221:


np.arange('2017-03','2017-04,dtype= 'datetime64[D]')
112/222:


np.arange('2017-03','2017-04', dtype= 'datetime64[D]')
112/223:


np.arange('2017-03','2017-04', dtype= 'datetime64[ns]')
112/224:


np.arange('2017-03','2017-04', dtype= 'datetime64[s]')
112/225:


np.arange('2017-03','2017-04', dtype= 'datetime64[D]')
112/226:


np.arange('2017-03','2017-04', dtype= 'datetime64[D]')
import numpy as np
print("Number of days, February, 2016: ")
print(np.datetime64('2016-03-01') - np.datetime64('2016-02-01'))
print("Number of days, February, 2017: ")
print(np.datetime64('2017-03-01') - np.datetime64('2017-02-01'))
print("Number of days, February, 2018: ")
print(np.datetime64('2018-03-01') - np.datetime64('2018-02-01'))
112/227:


np.arange('2017-03','2017-04', dtype= 'datetime64[D]')
112/228:
p = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.max([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])
o
112/229:
p = np.array([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

o = np.max([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

k = np.min([[ 0.96336355, 0.12339131, 0.20295196 ,0.37243578, 0.88105252], [ 0.93228246, 0.67470158 ,0.38103235 ,0.32242645, 0.40610231], [ 0.3113495 ,0.31688, 0.79189089, 0.08676434, 0.60829874], [ 0.30360149 ,0.94316317, 0.98142491, 0.77222542, 0.51532195], [ 0.97392305, 0.16669609, 0.81377917, 0.2165645, 0.00121611]])

print(o,k)
112/230:
t = np.array([0 ,1, 2, 3 ,4 ,5 ,6])
t = t**3
112/231:
t = np.array([0 ,1, 2, 3 ,4 ,5 ,6])
t = t**3
t
112/232:
import numpy as np
data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, dtype=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
112/233:
u = np.array([(b'James', 5, 48.5 ) ,(b'Nail', 6, 52.5 ), (b'Paul', 5, 42.1 ) ,(b'Pit', 5, 40.11)])
print(np.sort(u, order=['class', 'height'])))
112/234:

data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, dtype=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
112/235:
import numpy as np
data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, dtype=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
112/236:
import numpy as np
data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, dtype=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
112/237:
import numpy as np
student_id = np.array([1023, 5202, 6230, 1671, 1682, 5241, 4532])
student_height = np.array([40., 42., 45., 41., 38., 40., 42.0])
#Sort by studen_id then by student_height
indices = np.lexsort((student_id, student_height))
print("Sorted indices:")
print(indices)
print("Sorted data:")
for n in indices:
  print(student_id[n], student_height[n])
113/1:
# Module 3 - Lab 5
#Word clouds using Text Visualization

# Install wordcloud
!pip install wordcloud # Comment this line after installation
113/2:
# Module 3 - Lab 5
#Word clouds using Text Visualization
# Comment this line after installation
# Install wordcloud
!pip install wordcloud
113/3:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
113/4:
#Data 
book = open('Hob.txt', 'r',encoding='utf-8').read()

#Remove Stop Words
book_stopwords = set(STOPWORDS)
113/5:
# instantiate a word cloud object
book_wc = WordCloud(background_color='white', max_words=5000, stopwords=book_stopwords)

# generate the word cloud
book_wc.generate(book)
113/6:
# display the word cloud

plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/7:
# Update Stopwords
stp_words=['man','said','one']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
113/8:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/9:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/10:
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))

#Display mask

fig = plt.figure()
fig.set_figwidth(6) # set width
fig.set_figheight(8) # set height
plt.imshow(book_mask, cmap=plt.cm.gray, interpolation='bilinear')
plt.axis('off')
plt.show()
113/11:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"IMDB_Dataset.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/12:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"IMDB_Dataset.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/13:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"IMDB_Dataset.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/14:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/15:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.title[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/16:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.id[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/17:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.title[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/18:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/19:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_tsv(r"amazon_alexa.tsv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/20:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/21:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv", sep='\t',encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/22:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv", sep='\t',encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/23:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/24:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/25:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.feedback[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/26:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/27:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"IMDB_Dataset.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 
print(i)
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/28:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"IMDB_Dataset.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 
        print(i)
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/29:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/30:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/31:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df. reviews.text[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/32:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.text[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/33:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.username [0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/34:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.categories [0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/35:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.( reviews.text[0:1000]): 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/36:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.(reviews.text[0:1000]): 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/37:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.[reviews.text[0:1000]]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/38:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.text[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/39:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.text[0:100]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/40:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())

set low_memory=False
#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.text[0:100]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/41:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())

set low_memory = False
#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.text[0:100]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/42:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews.text[0:100]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/43:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df. keys manufacturer [0:100]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/44:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.keys manufacturer[0:100]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/45:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.text[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/46:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/47:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 

print(df.shape,df.head())


#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 



wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/48:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
    print(tokens)
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/49:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 


wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/50:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/51:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words = " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/52:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words+ = " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/53:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words+= " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/54:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words= " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/55:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words= " ".join(tokens)" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/56:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words= .join(tokens)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/57:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words+= .join(tokens)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/58:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words+="" .join(tokens)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/59:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words+="" .join(tokens)+

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/60:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
  
    comment_words+="" .join(tokens)+""

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/61:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+=" " .join(tokens)+""

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/62:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 1800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/63:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 3800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/64:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 3800, height = 1800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/65:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 3800, height = 2800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/66:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 5800, height = 4800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/67:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 200, height = 200, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/68:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 100, height = 100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/69:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 100, height = 100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (18, 18), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/70:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (18, 18), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/71:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+""

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/72:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words+="  " .join(tokens)+ " "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/73:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words +="  " .join(tokens)+ " "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/74:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words +=" ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/75:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/76:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/77:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/78:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/79:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/80:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/81:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/82:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/83:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/84:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='black', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/85:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='red', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/86:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='green', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/87:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/88:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/89:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 
book = open(r"amazon_alexa.tsv", sep='\t','r',encoding='utf-8').read()
print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
      for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/90:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 
book = open(r"amazon_alexa.tsv", sep='\t','r',encoding='utf-8').read()
print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/91:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 
book = open(r"amazon_alexa.tsv", sep='\t',encoding='utf-8').read()
print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/92:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 
book = open(r"amazon_alexa.tsv",encoding='utf-8').read()
print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/93: book = open(r"amazon_alexa.tsv",encoding='utf-8').read()
113/94:
book = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book_wc = WordCloud(background_color='white', max_words=5000, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book_wc.generate(book)
113/95:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book_wc = WordCloud(background_color='white', max_words=5000, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book_wc.generate(book1)
113/96:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book_wc.generate(book1)
113/97:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book_wc.generate(book1)
book1
113/98:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
113/99:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/100:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/101:
#Data 
book = open(r"amazon_alexa.tsv",encoding='utf-8').read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(wc) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/102:
#Data 
book2 = open(r"amazon_alexa.tsv",encoding='utf-8').read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(wc) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/103:
#Data 
book2 = open(r"amazon_alexa.tsv",encoding='utf-8').read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/104:
#Data 
book2 = open(r"amazon_alexa.tsv",encoding='utf-8').read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2_stopwords) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/105:
#Data 
book2 = open(r"amazon_alexa.tsv",encoding='utf-8').read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/106:
#Data 
book2 = open(r"Amazon_Customer_Reviews.csv",encoding='utf-8').read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/107:
#Data 
book2 = open(r"Amazon_Customer_Reviews.csv",encoding='utf-8').read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/108:
#Data 
book2 = open(r"Amazon_Customer_Reviews.csv",encoding ="latin-1").read()

#Remove Stop Words
book2_stopwords = set(STOPWORDS)

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/109:
#Data 
book2 = open(r"Amazon_Customer_Reviews.csv",encoding ="latin-1").read()


wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/110:
#Data 
book2 = open(r"Amazon_Customer_Reviews.csv",encoding ="latin-1").read()


wc = WordCloud(width = 1800, height = 1800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(book2) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
113/111:
book1 = open(r"Amazon_Customer_Reviews.csv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/112:
book1 = open(r"Amazon_Customer_Reviews.csv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/113:
book1 = open(r"Amazon_Customer_Reviews.csv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 1800, height = 1800, background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/114:
book1 = open(r"Amazon_Customer_Reviews.csv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 4800, height = 4800, background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/115:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/116:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
plt.figure(figsize = (8, 8), facecolor = None)
book1_wc.generate(book1)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/117:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book_stopwords)

# generate the word cloud
book1_wc.generate(book1)
plt.figure(figsize = (8, 8), facecolor = None)

plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/118:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/119:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=1500, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/120:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=100, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/121:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=10, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/122:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=5, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/123:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/124:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/125:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/126:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/127:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/128:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/129:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/130:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/131:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=50, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/132:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=5, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/133:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/134:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (18, 18), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/135:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(background_color='white', max_words=500, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/136:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 4800, height = 4800,background_color='white', max_words=500, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
113/137:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 4800, height = 4800,background_color='white', max_words=500, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
114/1:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
114/2:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 2800, height = 2800,background_color='white', max_words=500, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
114/3:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
114/4:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 2800, height = 2800,background_color='white', max_words=500,mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
114/5:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 2800, height = 2800,background_color='white', max_words=500,mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
114/6:
book1 = open(r"amazon_alexa.tsv",encoding='utf-8').read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 2800, height = 2800,background_color='white', max_words=500,mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
114/7:
book1 = open(r"amazon_alexa.tsv",encoding ="latin-1").read()
book1_stopwords = set(STOPWORDS)
book1_wc = WordCloud(width = 2800, height = 2800,background_color='white', max_words=500,mask=book_mask, stopwords=book1_stopwords)

# generate the word cloud
book1_wc.generate(book1)

plt.figure(figsize = (12, 12), facecolor = None)
plt.imshow(book1_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
116/1:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
116/2:
# 2. Perform text Visualization on Amazon amazon_alexa.tsv Dataset and draw 3 main insights.   

# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"amazon_alexa.tsv",sep='\t', encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.verified_reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
116/3:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"IMDB_Dataset.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 
        print(i)
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
116/4:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.name[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
116/5:
# 1. Perform text Visualization on Amazon_Customer_Reviews.csv Dataset and draw 3 main insights.   
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"Amazon_Customer_Reviews.csv", encoding ="latin-1") 
df.shape,df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.reviews[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 
 
    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 

    
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 4100, height = 4100, background_color ='white', stopwords = stopwords,mask=book_mask, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
117/1:
print('PyDev console: using IPython 7.19.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\Admin\\PycharmProjects\\pythonProject', 'C:/Users/Admin/PycharmProjects/pythonProject'])
117/2: runfile('C:/Users/Admin/PycharmProjects/pythonProject/main.py', wdir='C:/Users/Admin/PycharmProjects/pythonProject')
118/1:
print('PyDev console: using IPython 7.19.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\Admin\\PycharmProjects\\pythonProject', 'C:/Users/Admin/PycharmProjects/pythonProject'])
118/2: runfile('C:/Users/Admin/PycharmProjects/pythonProject/main.py', wdir='C:/Users/Admin/PycharmProjects/pythonProject')
119/1:
print('PyDev console: using IPython 7.19.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\Admin\\PycharmProjects\\pythonProject', 'C:/Users/Admin/PycharmProjects/pythonProject'])
119/2: runfile('C:/Users/Admin/PycharmProjects/pythonProject/main.py', wdir='C:/Users/Admin/PycharmProjects/pythonProject')
120/1:

data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, type=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
120/2:

data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, dtype=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
120/3:
import numpy as np

a = np.arange(10) #using arange function
print(a)

b = np.array([1,2,3,4,5]) #creating array from list
print(b)
120/4:

data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, dtype=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
120/5:

data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, type=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
120/6:

data_type = [('name', 'S15'), ('class', int), ('height', float)]
students_details = [('James', 5, 48.5), ('Nail', 6, 52.5),('Paul', 5, 42.10), ('Pit', 5, 40.11)]
# create a structured array
students = np.array(students_details, dtype=data_type)   
print("Original array:")
print(students)
print("Sort by class, then height if class are equal:")
print(np.sort(students, order=['class', 'height']))
122/1:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv("C:\\Users\\Admin\\Desktop\\...\\nyc_weather.csv")
122/2: import pandas as pd
122/3:
from google.colab import drive
drive.mount('/content/drive')
122/4:
#from google.colab import drive
#drive.mount('/content/drive')
122/5:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv("C:\\Users\\Admin\\Desktop\\...\\nyc_weather.csv")
122/6:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\\Users\\Admin\\Desktop\\...\\nyc_weather.csv")
122/7:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv")
122/8:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv")
122/9:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv("C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv")
122/10:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv")
122/11:
import pandas as pd
import os
122/12:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv")
122/13:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv" encoding ="latin-1")
122/14:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/15:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/16:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
#C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv
122/17:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
# C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv
122/18:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/19:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/20:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/21:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")

df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/22:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")

df = pd.read_csv("C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/23: df.head()
122/24:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")

df = pd.read_csv("C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/25:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")

df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/26:
#df = pd.read_csv("/content/drive/My Drive/data/nyc_weather.csv")
df = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\nyc_weather.csv", encoding ="latin-1")
122/27:
#get the maximum temparature 
df['Temperature'].max()
122/28:
#to know which day it rains
df['EST'][df['Events'] == 'Rain']
122/29:
#3. average wind speed
df['WindSpeedMPH'].mean()
121/1:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
121/2:
#Word Cloud from Dataset

# Data - IMDB Review Dataset  
df = pd.read_csv(r"IMDB_Dataset.csv", encoding ="latin-1") 

print(df.shape)
df.head()

#Stop Words
comment_words = '' 
stopwords = set(STOPWORDS) 

# iterate through the csv file only the first 1000 words 
for val in df.review[0:1000]: 
    # typecaste each val to string 
    val = str(val) 

    # split the value 
    tokens = val.split() 

    # Converts each token into lowercase 
    for i in range(len(tokens)): 
        tokens[i] = tokens[i].lower() 
        print(i)
    comment_words += " ".join(tokens)+" "

wc = WordCloud(width = 800, height = 800, background_color ='white', stopwords = stopwords, min_font_size = 10).generate(comment_words) 

# plot the WordCloud image 
plt.figure(figsize = (8, 8), facecolor = None) 
plt.imshow(wc) 
plt.axis("off") 
plt.tight_layout(pad = 0) 

plt.show()
121/3: print(df.to_string())
120/7: print('*',*5 \t,\n)
120/8: print('*'*5, \t,\n)
120/9: print('*'*5 \t\n)
120/10: print('*'*5 ,sep='\t',end='\n')
120/11:
rows = 5
for i in range(rows + 1, 0, -1):
    for j in range(0, i - 1):
        print("*", end=' ')
    print(" ")
120/12:
print('*'*5 ,sep='\t',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/13:
print('*'*5 ,sep=' ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/14:
print('*'*5 ,sep=' ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/15:
print('*'*5 ,sep='   ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/16:
print('*'*5 ,sep=' \t  ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/17:
for i in range(0,100):
    if i / 2 == 0:
        print("odd")
    else
    print('even')
120/18:
for i in range(0,100):
    if i / 2 == 0:
        print("odd")
    else:
        print('even')
120/19:
for i in range(0,100):
    if i % 2 == 0:
        print("odd")
    else:
        print('even')
120/20:
print('*'*5 ,sep=' --- ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/21:
print('*'*5 ,sep=' --- ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/22:
print('\t*'*5 ,sep=' --- ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/23:
print('\t\n*'*5 ,sep=' --- ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/24:
print('\t*'*5 ,sep=' --- ',end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/25:
print('\t*'*5 ,end='\n')
print('*'*4 ,sep='\t',end='\n')
print('*'*3 ,sep='\t',end='\n')
print('*'*2 ,sep='\t',end='\n')
print('*'*1 ,sep='\t',end='\n')
120/26:
for  i in range(0,5):
    print('\t*'*i ,end='\n')
120/27:
for  i in range(5,0):
    print('\t*'*i ,end='\n')
120/28:
for  i in range(5,0):
    print('\t*'*i ,end='\n')
120/29:
for  i in range(5,0,-1):
    print('\t*'*i ,end='\n')
122/30:

def mat (n1,n2,n3):
    return n1+n2+n3
n =mat(1,2,3)
122/31:

def mat (n1,n2,n3):
    return n1+n2+n3
n =mat(1,2,3)
n
122/32:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(1,2,3)
print(n)
122/33:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
122/34:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
122/35:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
122/36:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
122/37:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input(n1,n2,n3))
print(n)
122/38:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input(n1,n2,n3))
print(n)
122/39:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
122/40:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
122/41:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input(1 2 4))
print(n)
122/42:
def avg(n1,n2,n3):
    return(n1+n2+n3)/3.0
n1=float(input("Please enter n1:"))
n2=float(input("Please enter n2:"))
n3=float(input("Please enter n3:"))
print(avg(n1,n2,n3))
123/1:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input([1],[2],[3]))
print(n)
123/2:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input([1],[2]))
print(n)
123/3:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input([1]))
print(n)
123/4:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input([]))
print(n)
123/5:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input([],[]))
print(n)
123/6:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input([]))
print(n)
123/7:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input([]))
print(n)
123/8:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
123/9:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
123/10:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
123/11:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input(,,))
print(n)
123/12:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input(1,2,3))
print(n)
123/13:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
123/14:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input())
print(n)
123/15:

def mat (n1,n2,n3):
    return n1+n2+n3
n = mat(input(),input(),input())
print(n)
123/16:

def mat (n1,n2,n3):
    return n1+n2+n3/3
n = mat(input("enter n1"),input("enter n2"),input('enter n3'))
print(n)
123/17:

def mat (n1,n2,n3):
    return (n1+n2+n3)/2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print(n)
123/18:

def mat (n1,n2,n3):
    return (n1+n2+n3)/2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print(n)
123/19:

def mat (n1,n2,n3):
    return (n1+n2+n3)\2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print(n)
123/20:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print(n)
123/21:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print(/n n)
123/22:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print('/n' n)
123/23:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print('/n', n)
123/24:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print('/n,n', n)
123/25:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print( n)
123/26:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print( n)
i=5
i=6 
i
123/27:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print( n)
i=5
i=6 
print(i)
124/1:

def mat (n1,n2,n3):
    return (n1+n2+n3)*2
n = mat(input("enter n1: "),input("enter n2: "),input('enter n3: '))
print( n)
125/1:
#import the pandas library and aliasing as pd
import pandas as pd
import numpy as np
data = np.array(['a','b','c','d'])
s = pd.Series(data)
print(s)
125/2:
import pandas as pd
import numpy as np
data = np.array(['a','b','c','d'])
s = pd.Series(data,index=[100,101,102,103])
print (s)
125/3:
import pandas as pd
import numpy as np
data = {'a' : 0., 'b' : 1., 'c' : 2.}
s = pd.Series(data)
print (s)
125/4:
import pandas as pd
import numpy as np
data = {'a' : 0., 'b' : 1., 'c' : 2.}
s = pd.Series(data,index=['b','c','d','a'])
print (s)
125/5:
import pandas as pd
import numpy as np
s = pd.Series(5, index=[0, 1, 2, 3])
print (s)
125/6:
import pandas as pd
s = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])

#retrieve the first element
print (s[0])
print(s['a'])
125/7:
import pandas as pd
s = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])

#retrieve the first three element
print (s[:3])
125/8:
import pandas as pd
s = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])

#retrieve a single element
print (s['a'])
125/9:
import pandas as pd
s = pd.Series([1,2,3,4,5],index = ['a','b','c','d','e'])

#retrieve multiple elements
print (s[['a','c','d']])
125/10:
import pandas as pd
df = pd.DataFrame()
print (df)
125/11:
import pandas as pd
data = [1,2,3,4,5]
df = pd.DataFrame(data)
print (df)
125/12:
import pandas as pd
data = [['Alex',10],['Bob',12],['Clarke',13]]
df = pd.DataFrame(data,columns=['Name','Age'])
print (df)
125/13:
import pandas as pd
data = [['Alex',10],['Bob',12],['Clarke',13]]
df = pd.DataFrame(data,columns=['Name','Age'],dtype=float)
print (df)
125/14:
import pandas as pd
data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}
df = pd.DataFrame(data)
print (df)
125/15:
import pandas as pd
data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}
df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])
print (df)
125/16:
import pandas as pd
data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],
        'Age':[28,34,29,42]}
df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])
print (df)
125/17:
import pandas as pd
data = {'Name':['Tom', 'Jack', 'Steve', 'Ricky'],'Age':[28,34,29,42]}
df = pd.DataFrame(data, index=['rank1','rank2','rank3','rank4'])
print (df)
125/18:
import pandas as pd
data = [{'a': 1},{'a': 5, 'b': 10, 'c': 20}]
df = pd.DataFrame(data)
print (df)
125/19:
import pandas as pd
data = [{'a': 1, 'b': 2},{'a': 5, 'b': 10, 'c': 20}]
df = pd.DataFrame(data, index=['first', 'second'])
print (df)
125/20:
import pandas as pd
data = [{'a': 1, 'b': 2},{'a': 5, 'b': 10, 'c': 20}]

#With two column indices, values same as dictionary keys
df1 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b'])

#With two column indices with one index with other name
df2 = pd.DataFrame(data, index=['first', 'second'], columns=['a', 'b1'])
print (df1)
print (df2)
125/21:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}

df = pd.DataFrame(d)
print (df)
125/22:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}
print(d)
df = pd.DataFrame(d)
print (df)
125/23:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}
print(d)
df = pd.DataFrame(d)
print (/ndf)
125/24:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}
print(d,end=/n)
df = pd.DataFrame(d)
print (df)
125/25:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}
print(d,end='/n')
df = pd.DataFrame(d)
print (df)
125/26:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}

df = pd.DataFrame(d)
print (df)
print(d,end='/n')
125/27:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}

df = pd.DataFrame(d)
print (df,end='/n')
print(d,end='/n')
125/28:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}

df = pd.DataFrame(d)
print (df,end='\n')
print(d,end='\n')
125/29:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3,4], index=['a', 'b', 'c','d'])}

df = pd.DataFrame(d)
print (df,end='\n')
125/30:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)
print (df ['one'])
125/31:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)

# Adding a new column to an existing DataFrame object with column label by passing new series

print ("Adding a new column by passing as Series:")
df['three']=pd.Series([10,20,30],index=['a','b','c'])
print (df)

print ("Adding a new column using the existing columns in DataFrame:")
df['four']=df['one']+df['three']

print (df)
125/32:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)

# Adding a new column to an existing DataFrame object with column label by passing new series

print ("Adding a new column by passing as Series:")
df['three']=pd.DataFrame([10,20,30],index=['a','b','c'])
print (df)

print ("Adding a new column using the existing columns in DataFrame:")
df['four']=df['one']+df['three']

print (df)
125/33:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)

# Adding a new column to an existing DataFrame object with column label by passing new series

print ("Adding a new column by passing as Series:")
df['three']=pd.dataFrame([10,20,30],index=['a','b','c'])
print (df)

print ("Adding a new column using the existing columns in DataFrame:")
df['four']=df['one']+df['three']

print (df)
125/34:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)

# Adding a new column to an existing DataFrame object with column label by passing new series

print ("Adding a new column by passing as Series:")
df['three']=pd.dataframe([10,20,30],index=['a','b','c'])
print (df)

print ("Adding a new column using the existing columns in DataFrame:")
df['four']=df['one']+df['three']

print (df)
125/35:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)

# Adding a new column to an existing DataFrame object with column label by passing new series

print ("Adding a new column by passing as Series:")
df['three']=pd.DataFrame([10,20,30],index=['a','b','c'])
print (df)

print ("Adding a new column using the existing columns in DataFrame:")
df['four']=df['one']+df['three']

print (df)
125/36:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)

# Adding a new column to an existing DataFrame object with column label by passing new series

print ("Adding a new column by passing as Series:")
df['three']=pd.DataFrame([10,20,30],index=['a','b','c'])
print (df)

print ("Adding a new column using the existing columns in DataFrame:")
df['four']=df['one']+df['three']

print (df)
125/37:
# Using the previous DataFrame, we will delete a column
# using del function
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd']), 
   'three' : pd.Series([10,20,30], index=['a','b','c'])}

df = pd.DataFrame(d)
print ("Our dataframe is:")
print (df)

# using del function
print ("Deleting the first column using DEL function:")
del (df['one'])
print (df)

# using pop function
print ("Deleting another column using POP function:")
df.pop('two')
print (df)
125/38:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)
print (df.loc['b'])
125/39:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}
print (df.loc['b'])
df = pd.DataFrame(d)
print (df.loc['b'])
125/40:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}
print (df.loc['c'])
df = pd.DataFrame(d)
print (df.loc['b'])
125/41:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}
print (df.loc['d'])
df = pd.DataFrame(d)
print (df.loc['b'])
125/42:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}
print (df.loc['d'])
df = pd.DataFrame(d)
print (df.one)
125/43:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 30, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)
print (df.iloc[2])
125/44:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']), 
   'two' : pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)
print (df[2:4])
125/45:
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]], columns = ['a','b'])
df2 = pd.DataFrame([[5, 6], [7, 8]], columns = ['a','b'])

df = df.append(df2)
print (df)
125/46:
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]], columns = ['a','b'])
df2 = pd.DataFrame([[5, 6], [7, 8]], columns = ['a','b'])

df = df.append(df2)

# Drop rows with label 0
df = df.drop(0)

print (df)
125/47:
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]], columns = ['a','b'])
df2 = pd.DataFrame([[5, 6], [7, 8]], columns = ['a','b'])

df = df.append(df2)

# Drop rows with label 0
#df = df.drop(0)

print (df)
125/48:
import pandas as pd

df = pd.DataFrame([[1, 2], [3, 4]], columns = ['a','b'])
df2 = pd.DataFrame([[5, 6], [7, 8]], columns = ['a','b'])

df = df.append(df2)

# Drop rows with label 0
df = df.drop(0)

print (df)
125/49:
import pandas as pd

data = pd.read_csv('C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
125/50:
import pandas as pd

data = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
125/51: data.head()
125/52: data.tail()
125/53: data.shape
125/54: data.info()
125/55: data.describe()
125/56: data.columns
125/57: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
125/58: schema
125/59: pd.set_option('display.max_rows',85)
125/60: schema
125/61: pd.set_option('display.max_rows',100)
125/62: schema
125/63:
#pd.set_option('display.max_rows',100)
pd.set_option('di',100)
125/64: schema
125/65: pd.set_option(display.max_rows,10)
125/66: pd.set_option('display.max_rows',10)
125/67: schema
125/68: schema
125/69:
import pandas as pd
k = {"custom_account":  ['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI'] }
125/70:
import pandas as pd
k = {"custom_account":  ['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}}
test = pd.DataFrame(k)
125/71:
import pandas as pd
k = {"custom_account":  ['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}}test = pd.DataFrame(k)
125/72:
import pandas as pd
k = {"custom_account":  ['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k)
125/73:
import pandas as pd
k = {"custom_account":  ['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
k
125/74:
import pandas as pd
k = {"custom_account":  ['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
125/75:
import pandas as pd
k = {"custom_account":['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
125/76:
import pandas as pd
k = {"custom_account":['B100','J101','X102','P103',R104],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
125/77:
import pandas as pd
k = {"custom_account":['B100','J101','X102','P103',104],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
125/78:
import pandas as pd
k = {"custom_account":['B100','J101','X102','P103',r104],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
125/79:
import pandas as pd
k = {"custom_account":['B100','J101','X102','P103','R104'],"sales": [100,130,119,92,35], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
125/80: test.head()
125/81: test.coloums()
125/82: test.coloum()
125/83: test.coloumn()
125/84: test.coloumns()
125/85: test[1]
125/86: print(test[1])
125/87: print(test[:1])
125/88: print(test[1:])
125/89: print(test[0:])
125/90: print(test[:])
125/91: print(test[:1])
125/92: print(test[:2])
125/93: print(test.head(1))
125/94: print(test.head(0))
125/95:
print(test.head(0))
print(test['custom_account'])
125/96:
print(test.head(0))
print(test['1'])
125/97:
print(test.head(0))
print(test['custom_account'])
125/98:
print(test.head(0))
print(test[1])
125/99:
print(test.head(0))
print(test[:1])
125/100:
print(test.head(0))
print(test[1:])
125/101:
print(test.head(0))
print(test[0:])
125/102:
print(test.head(0))
print(test[0:0])
125/103:
print(test.head(0))
print(test[0:1])
125/104:
import pandas as pd

d = {'one' : pd.Series([1, 2, 3], index=['a', 'b', 'c']),
   'two' : pd.Series([1, 2, 30, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(d)
print(df)
print (df.iloc[2])
125/105:
print(test.head(0))
print(test[0:2])
125/106:
print(test.head(0))
print(test[1:0])
125/107:
print(test.head(0))
print(test[2:0])
125/108:
print(test.head(0))
print(test[2:1])
125/109:
print(test.head(0))
print(test[2:2])
125/110:
print(test.head(0))
print(test[:2])
125/111:
print(test.head(0))
print(test[1:2])
125/112:
print(test.head(0))
#print(test[1:2])
print(test['custom_account'])
125/113: test.sort_values['city']
125/114: srt = test.sort_values['city']
125/115: srt = test.sort['city']
125/116: srt = test.sort[by=['city']
125/117: srt = test.sort_values(by=['city'])
125/118:
srt = test.sort_values(by=['city'])
srt
125/119:
srt = test.sort_values(by=['city'],ascending=False, na_position='first')
srt
125/120:
srt = test.sort_values(by=['city'], ascending=False)
srt
125/121:
srt = test.sort_values(by=['city'], ascending=False )
srt
125/122: test.tail()
125/123:
import pandas as pd
k = {"sales": [100,130,119,92,35],"custom_account":['B100','J101','X102','P103','R104'], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
125/124:
print(test.head(0))
#print(test[1:2])
print(test['custom_account'])
125/125:
print(test.head(0))
#print(test[1:2])
print(test['sales'])
125/126:
srt = test.sort_values(by=['city'], ascending=False )
srt
125/127: test.tail()
125/128:
reorder = test.reindex(columns= ['custom_account', 'sales', 'city'])

print(reorder)
125/129:
test.tail()
test.iloc(4)
125/130:
test.tail()
test.iloc(3)
125/131:
test.tail()
u = test.iloc(3)
125/132:
test.tail()
u = test.iloc(:3)
125/133:
test.tail()
u = test.iloc([2])
125/134:
test.tail()
u = test.iloc[2]
print (u)
125/135:
test.tail()
u = test.iloc[0:2]
print (u)
125/136:
test.tail()
u = test.iloc[0:1]
print (u)
125/137:
test.tail()
u = test.iloc[:0]
print (u)
125/138:
test.tail()
u = test.iloc[:,0]
print (u)
125/139:
test.tail()
u = test.iloc[1:,0]
print (u)
125/140:
test.tail()
u = test.iloc[:,0]
print (u)
125/141:
test.tail()
u = test.iloc[3:,0]
print (u)
125/142:
test.tail()
u = test.iloc[-1:,0]
print (u)
125/143:
test.tail()
u = test.iloc[-1]
print (u)
125/144:
test.tail()
u = test.iloc[-1:]
print (u)
125/145: pd.set_option('display.max_rows',100)
125/146: schema
125/147:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;

schema
125/148:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
125/149: schema
125/150:
# this will resize output area
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
125/151:
# this will resize output area

%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
125/152:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
125/153:
print(test)
u = test.iloc[-1:]
print (u)
125/154:
print(test)
print(" "*100)
u = test.iloc[-1:]
print (u)
125/155:
print(test)
print(" "*100)
u = test.iloc[-2:]
print (u)
125/156:
print(test)
print(" "*100)
u = test.iloc[1:]
print (u)
125/157:
print(test)
print(" "*100)
u = test.iloc[2:]
print (u)
125/158:
print(test)
print(" "*100)
u = test.iloc[2:3]
print (u)
125/159:
print(test)
print(" "*100)
u = test.iloc[2:4]
print (u)
125/160:
print(test)
print(" "*100)
u = test.iloc[2:4,1]
print (u)
125/161:
print(test)
print(" "*100)
u = test.iloc[2:4,2]
print (u)
125/162:
print(test)
print(" "*100)
u = test.iloc[4,2]
print (u)
125/163:
print(test)
print(" "*100)
u = test.iloc[3:4,2]
print (u)
125/164:
print(test)
print(" "*100)
u = test.iloc[3:4,1]
print (u)
125/165:
print(test)
print(" "*100)
u = test.iloc[3:5,1]
print (u)
125/166:
print(test)
print(" "*100)
u = test.iloc[4:5,1]
print (u)
125/167:
print(test)
print(" "*100)
u = test.iloc[-1,1]
print (u)
125/168: schema
125/169: schema
125/170: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
125/171: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
125/172: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
126/1: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
126/2:
import pandas as pd
import os
126/3: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv')
126/4: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv',encode = latin)
126/5: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv',encode = 'latin')
126/6: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv', encoding = 'latin')
126/7: schema
126/8: pd.set_option('display.max_rows',100)
126/9:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
126/10: schema
126/11: schema
126/12: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv', encoding = 'latin')
126/13: schema
126/14: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv', encoding = 'latin')
126/15: schema
126/16: schema = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\ML_CLASS_5_PANDAS\survey_results_schema_2019.csv', encoding = 'latin')
126/17: schema
126/18:
print(test)
print(" "*100)
u = test.iloc[0,1]
print (u)
126/19:
import pandas as pd
k = {"sales": [100,130,119,92,35],"custom_account":['B100','J101','X102','P103','R104'], "city":['BOS','LA','NYC','SF','CHI']}
test = pd.DataFrame(k) 
test
126/20:
print(test)
print(" "*100)
u = test.iloc[0,1]
print (u)
131/1:
#Module 3 Lab 1
#Introduction to Seaborn

#Installation
pip install seaborn
131/2:
#Module 3 Lab 1
#Introduction to Seaborn

#Installation
!pip install seaborn
131/3:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
131/4:
#IV. Matrix Plots

fig, ax=plt.subplots(nrows=2,ncols=2, figsize=(15,10))

#Data
df1 = sns.load_dataset('flights') 
df2=sns.load_dataset('iris')

df11 = pd.pivot_table(values ='passengers', index ='month', columns ='year', data = df1) 

# calculates correlations between columns in  the dataframe.
dfc1 = df1.corr() 
dfc2 = df2.corr()


#Heatmaps - matrix plot. 
# plot a heatmap of the correlated data 
#annot - annotate the actual value
#cmap is used for the colour mapping - coolwarm/plasma/magma etc.
#linewidth -  width of the lines separating the cells.
#linecolor -  colour of the lines separating the cells.



sns.heatmap(df11,cmap='YlGnBu' , linecolor ='r', linewidths = 0.5, annot=True,fmt='d',square=True,ax=ax[0,0]).set_title('Heat Map Flights')

sns.heatmap(dfc2,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')

#lower Triangle Display
mask1 = np.triu(dfc2)
sns.heatmap(dfc2, annot=True, mask=mask1,ax=ax[1,0],cmap='coolwarm').set_title('Heat Map Lower Triangle')

#Upper Triangle Display

mask2 = np.tril(dfc2)
sns.heatmap(dfc2, annot=True, cmap='YlGnBu',mask=mask2,ax=ax[1,1]).set_title('Heat Upper Triangle')

#Cluster maps - used hierarchical clustering, it performs the clustering based on the similarity of the rows and columns.
#months and years clustered together on the basis of number of passengers that traveled in a specific month.
sns.clustermap(df11, cmap ='RdYlGn')

#standard_scale = 1 normalises the data from 0 to 1 range. 
sns.clustermap(df11, cmap ='plasma', standard_scale = 1)
131/5:
sns.set_style('whitegrid') 


#Data - 'iris'
df = sns.load_dataset('iris') 
print(df.head())

#Displot- used for univariant set of observations and visualizes it through a histogram 
#i.e. only one observation and hence we choose one particular column of the dataset.
#KDE is a way to estimate the probability density function (PDF) of the random variable that underlies the sample.
#KDE is a means of data smoothing.
#bins is used to set the number of bins you want in your plot and it actually depends on your dataset.
#color is used to specify the color of the plot
sns.distplot(df['petal_length'], kde = True, color ='red', bins = 30).set_title('Dist Plot') 

#Joinplot/jointgrid- draw a plot of two variables with bivariate and univariate graphs. It basically combines two different plots.
#Plot a bi-variate distribution along with marginal distributions in the same plot
#Joint Distribution of two variables can be visualised using scatter plot/regplot or kdeplot.
#Marginal Distribution of variables can be visualised by histograms and/or kde plot
#KDE shows the density where the points match up the most 
#The Axes-level function to use for joint distribution must be passed to JointGrid.plot_joint(). 
#The Axes-level function to use for marginal distribution must be passed to JointGrid.plot_marginals()

jointgrid = sns.JointGrid(x='petal_length', y='petal_width', data=df)
jointgrid.plot_joint(sns.scatterplot)
jointgrid.plot_marginals(sns.distplot)

#jointplot() to plot bi-variate distribution along with marginal distributions. 
#It uses JointGrid() and JointGrid.plot_joint() in the background.
g=sns.jointplot(x = 'petal_length',y = 'petal_width',data = df,kind = 'hex') 
g.fig.suptitle('Joint Plot')

#Pairplot-  pairwise relation across the entire dataframe
#hue sets up the categorical separation between the entries in the dataset.
#palette is used for designing the plots.
g=sns.pairplot(df, hue ="species", palette ='coolwarm') 
g.fig.suptitle("Pair Plot 1")
g.add_legend()


#PairGrid() - creates Axes for each pair of variables 
#PairGrid.map() - draws the plot on each Axes using data corresponding to that pair of variables
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_offdiag(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)

#Different kind of plots on Upper Triangular Axes, Diagonal Axes and Lower Triangular Axes.
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_upper(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)
pairgrid = pairgrid.map_lower(sns.kdeplot)

#Avoid Redundancy
g = sns.PairGrid(df, diag_sharey=False, corner=True)
g.map_lower(sns.scatterplot)
g.map_diag(sns.kdeplot)
131/6:
#Module 3 Lab 1
#Introduction to Seaborn

#Installation
#pip install seaborn
131/7:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
131/8:
#Simple Plotting with Seaborn

#Data
dates = ['1981-01-01', '1981-01-02', '1981-01-03', '1981-01-04', '1981-01-05',
        '1981-01-06', '1981-01-07', '1981-01-08', '1981-01-09', '1981-01-10' ]

min_temperature = [20.7, 17.9, 18.8, 14.6, 15.8, 15.8, 15.8, 17.4, 21.8, 20.0]
max_temperature = [34.7, 28.9, 31.8, 25.6, 28.8, 21.8, 22.8, 28.4, 30.8, 32.0]

#Plotting
fig,axes = plt.subplots(nrows=1, ncols=1, figsize=(15,10))
axes.plot(dates, min_temperature, label='Min Temperature')
axes.plot(dates, max_temperature, label = 'Max Temperature')
axes.legend()
131/9:
#seaborn style as the default matplotlib style
sns.set()
131/10:
#Simple sine plot 
x = np.linspace(0, 10, 1000)
plt.plot(x, np.sin(x), x, np.cos(x));
131/11:
# I. Relational Plots

# Line plot : The line plot is one of the most basic plot in seaborn library.  
#This plot is mainly used to visualize the data in form of some time series, i.e. in continuous manner.
sns.set(style="dark") 
fig, ax = plt.subplots(ncols=2, nrows=1, figsize=(15,10))

#Loading Data with Seaborn
df = sns.load_dataset("tips")

print(df.head())

#lineplot
sns.lineplot(x="total_bill", y="tip", hue="size", style="time", data=df,ax=ax[0]).set_title("Line Plot")


#scatterplot
Sct_plt=sns.scatterplot(x="total_bill", y="tip", hue="size", style="time", data=df,ax=ax[1]).set_title("Scatter Plot")

#Saving Plot
Sct_plt.figure.savefig('Scatter_plot1.png')
print('Plot Saved')
131/12:
#II. Categorical Plots
#Plots are basically used for visualizing the relationship between variables. 
#Variables can be either be completely numerical or a category like a group, class or division.

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=5,ncols=2)
fig.set_size_inches(18.5, 10.5)

#Data
# 'tips dataset contains information about people who probably had food at a restaurant 
#  whether or not they left a tip for the waiters, their gender, whether they smoke and so on.
df = sns.load_dataset('tips') 

#barplot -  basically used to aggregate the categorical data according to some methods and by default its the mean
sns.barplot(x ='sex', y ='total_bill', data = df,palette ='plasma', estimator = np.std,ax=ax[0,0]).set_title('Bar Plot')


#countplot -Counts the categories and returns a count of their occurrences
sns.countplot(x ='sex', data = df,ax=ax[0,1]).set_title('Count Plot')
    
#boxplot - known as the box and whisker plot.
#It shows the distribution of the quantitative data that represents the comparisons between variables
sns.boxplot(x ='day', y ='total_bill', data = df, hue ='smoker',ax=ax[1,0]).set_title('Box Plot') 

# Similar to the boxplot except that it provides a higher, more advanced visualization 
# Uses the KDE - kernel density estimation to give a better description about the data distribution.
sns.violinplot(x ='day', y ='total_bill', data = df, hue ='sex', split = True,ax=ax[1,1]).set_title('Violin Plot')

#Stripplot -  scatter plot based on the category
sns.stripplot(x ='day', y ='total_bill', data = df, jitter = True, hue ='smoker', dodge = True,ax=ax[2,0]).set_title('Strip Plot')

#Swarmplot-similar to stripplot except the fact that the points are adjusted so that they do not overlap.
sns.swarmplot(x ='day', y ='total_bill', data = df,ax=ax[2,1]).set_title('Swarm Plot')

#Combining the idea of a violin plot and a stripplot to form this plot
sns.violinplot(x ='day', y ='total_bill', data = df,ax=ax[3,0]) 
sns.swarmplot(x ='day', y ='total_bill', data = df, color ='black',ax=ax[3,0]).set_title('Combined Plot')

# Density Plot
sns.kdeplot(df['tip'], df['total_bill'],ax=ax[3,1])

#boxenplot
sns.boxenplot(x="day", y="total_bill",color="b", scale="linear", data=df,ax=ax[4,0])

#Ridgeplot
sns.pointplot(x="day", y="total_bill",color="b", hue="sex", data=df,ax=ax[4,1])

#catplot
#General plot -  provides a parameter called 'kind' to choose the kind of plot ,better that writing the plots separately. 
#The kind parameter can be bar, violin, swarm etc.
sns.catplot(x ='day', y ='total_bill', data = df, kind ='bar')
131/13:
sns.set_style('whitegrid') 


#Data - 'iris'
df = sns.load_dataset('iris') 
print(df.head())

#Displot- used for univariant set of observations and visualizes it through a histogram 
#i.e. only one observation and hence we choose one particular column of the dataset.
#KDE is a way to estimate the probability density function (PDF) of the random variable that underlies the sample.
#KDE is a means of data smoothing.
#bins is used to set the number of bins you want in your plot and it actually depends on your dataset.
#color is used to specify the color of the plot
sns.distplot(df['petal_length'], kde = True, color ='red', bins = 30).set_title('Dist Plot') 

#Joinplot/jointgrid- draw a plot of two variables with bivariate and univariate graphs. It basically combines two different plots.
#Plot a bi-variate distribution along with marginal distributions in the same plot
#Joint Distribution of two variables can be visualised using scatter plot/regplot or kdeplot.
#Marginal Distribution of variables can be visualised by histograms and/or kde plot
#KDE shows the density where the points match up the most 
#The Axes-level function to use for joint distribution must be passed to JointGrid.plot_joint(). 
#The Axes-level function to use for marginal distribution must be passed to JointGrid.plot_marginals()

jointgrid = sns.JointGrid(x='petal_length', y='petal_width', data=df)
jointgrid.plot_joint(sns.scatterplot)
jointgrid.plot_marginals(sns.distplot)

#jointplot() to plot bi-variate distribution along with marginal distributions. 
#It uses JointGrid() and JointGrid.plot_joint() in the background.
g=sns.jointplot(x = 'petal_length',y = 'petal_width',data = df,kind = 'hex') 
g.fig.suptitle('Joint Plot')

#Pairplot-  pairwise relation across the entire dataframe
#hue sets up the categorical separation between the entries in the dataset.
#palette is used for designing the plots.
g=sns.pairplot(df, hue ="species", palette ='coolwarm') 
g.fig.suptitle("Pair Plot 1")
g.add_legend()


#PairGrid() - creates Axes for each pair of variables 
#PairGrid.map() - draws the plot on each Axes using data corresponding to that pair of variables
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_offdiag(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)

#Different kind of plots on Upper Triangular Axes, Diagonal Axes and Lower Triangular Axes.
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_upper(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)
pairgrid = pairgrid.map_lower(sns.kdeplot)

#Avoid Redundancy
g = sns.PairGrid(df, diag_sharey=False, corner=True)
g.map_lower(sns.scatterplot)
g.map_diag(sns.kdeplot)
131/14:
#Rugplot - plots datapoints in an array as sticks on an axis.It takes a single column.
#Instead of drawing a histogram it creates dashes all across the plot. 
sns.rugplot(df['petal_length'])
131/15:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
131/16:
#IV. Matrix Plots

fig, ax=plt.subplots(nrows=2,ncols=2, figsize=(15,10))

#Data
df1 = sns.load_dataset('flights') 
df2=sns.load_dataset('iris')

df11 = pd.pivot_table(values ='passengers', index ='month', columns ='year', data = df1) 

# calculates correlations between columns in  the dataframe.
dfc1 = df1.corr() 
dfc2 = df2.corr()


#Heatmaps - matrix plot. 
# plot a heatmap of the correlated data 
#annot - annotate the actual value
#cmap is used for the colour mapping - coolwarm/plasma/magma etc.
#linewidth -  width of the lines separating the cells.
#linecolor -  colour of the lines separating the cells.



sns.heatmap(df11,cmap='YlGnBu' , linecolor ='r', linewidths = 0.5, annot=True,fmt='d',square=True,ax=ax[0,0]).set_title('Heat Map Flights')

sns.heatmap(dfc2,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')

#lower Triangle Display
mask1 = np.triu(dfc2)
sns.heatmap(dfc2, annot=True, mask=mask1,ax=ax[1,0],cmap='coolwarm').set_title('Heat Map Lower Triangle')

#Upper Triangle Display

mask2 = np.tril(dfc2)
sns.heatmap(dfc2, annot=True, cmap='YlGnBu',mask=mask2,ax=ax[1,1]).set_title('Heat Upper Triangle')

#Cluster maps - used hierarchical clustering, it performs the clustering based on the similarity of the rows and columns.
#months and years clustered together on the basis of number of passengers that traveled in a specific month.
sns.clustermap(df11, cmap ='RdYlGn')

#standard_scale = 1 normalises the data from 0 to 1 range. 
sns.clustermap(df11, cmap ='plasma', standard_scale = 1)
131/17:
sns.set_style('whitegrid') 


#Data - 'iris'
df = sns.load_dataset('iris') 
print(df.head())

#Displot- used for univariant set of observations and visualizes it through a histogram 
#i.e. only one observation and hence we choose one particular column of the dataset.
#KDE is a way to estimate the probability density function (PDF) of the random variable that underlies the sample.
#KDE is a means of data smoothing.
#bins is used to set the number of bins you want in your plot and it actually depends on your dataset.
#color is used to specify the color of the plot
sns.distplot(df['petal_length'], kde = True, color ='red', bins = 30).set_title('Dist Plot') 

#Joinplot/jointgrid- draw a plot of two variables with bivariate and univariate graphs. It basically combines two different plots.
#Plot a bi-variate distribution along with marginal distributions in the same plot
#Joint Distribution of two variables can be visualised using scatter plot/regplot or kdeplot.
#Marginal Distribution of variables can be visualised by histograms and/or kde plot
#KDE shows the density where the points match up the most 
#The Axes-level function to use for joint distribution must be passed to JointGrid.plot_joint(). 
#The Axes-level function to use for marginal distribution must be passed to JointGrid.plot_marginals()

jointgrid = sns.JointGrid(x='petal_length', y='petal_width', data=df)
jointgrid.plot_joint(sns.scatterplot)
jointgrid.plot_marginals(sns.distplot)

#jointplot() to plot bi-variate distribution along with marginal distributions. 
#It uses JointGrid() and JointGrid.plot_joint() in the background.
g=sns.jointplot(x = 'petal_length',y = 'petal_width',data = df,kind = 'hex') 
g.fig.suptitle('Joint Plot')

#Pairplot-  pairwise relation across the entire dataframe
#hue sets up the categorical separation between the entries in the dataset.
#palette is used for designing the plots.
g=sns.pairplot(df, hue ="species", palette ='coolwarm') 
g.fig.suptitle("Pair Plot 1")
g.add_legend()


#PairGrid() - creates Axes for each pair of variables 
#PairGrid.map() - draws the plot on each Axes using data corresponding to that pair of variables
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_offdiag(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)

#Different kind of plots on Upper Triangular Axes, Diagonal Axes and Lower Triangular Axes.
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_upper(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)
pairgrid = pairgrid.map_lower(sns.kdeplot)

#Avoid Redundancy
g = sns.PairGrid(df, diag_sharey=False, corner=True)
g.map_lower(sns.scatterplot)
g.map_diag(sns.kdeplot)
131/18:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.pyplot import figure
import seaborn as sns
131/19:
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from matplotlib.pyplot import figure
import seaborn as sns
%matplotlib inline
131/20: gdp = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv)
131/21: gdp = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
131/22: gdp.head()
131/23: gdp.describe()
131/24:
pd.set_option_display("max_coloums",100)

#display.[chop_threshold, colheader_justify, column_space, date_dayfirst,date_yearfirst, encoding, expand_frame_repr, float_format]
131/25:
pd.set_option.display("max_coloums",100)

#display.[chop_threshold, colheader_justify, column_space, date_dayfirst,date_yearfirst, encoding, expand_frame_repr, float_format]
131/26:
pd.set_option('display.max_rows',100)

#display.[chop_threshold, colheader_justify, column_space, date_dayfirst,date_yearfirst, encoding, expand_frame_repr, float_format]
131/27: gdp.head()
131/28:
pd.set_option('display.max_rows',100)

#display.[chop_threshold, colheader_justify, column_space, date_dayfirst,date_yearfirst, encoding, expand_frame_repr, float_format]
131/29: gdp.head()
131/30: gdp = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
131/31:
pd.set_option('display.max_rows',100)

#display.[chop_threshold, colheader_justify, column_space, date_dayfirst,date_yearfirst, encoding, expand_frame_repr, float_format]
131/32: gdp.head()
131/33:
gdp.set_option('display.max_rows',100)

#display.[chop_threshold, colheader_justify, column_space, date_dayfirst,date_yearfirst, encoding, expand_frame_repr, float_format]
131/34:
pd.set_option('display.max_rows',100)

#display.[chop_threshold, colheader_justify, column_space, date_dayfirst,date_yearfirst, encoding, expand_frame_repr, float_format]
131/35: gdp.head()
131/36: gdp = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
131/37: pd.set_option('display.max_rows',100)
131/38: gdp.head()
131/39: gdp()
131/40: gdp.all()
131/41: gdp.head()
131/42: gdp
131/43: sns.histplot(x=year,y=gdp)
131/44: sns.histplot(x=year,y=gdp,data=gdp)
131/45: sns.histplot(x='year',y=gdp,data=gdp)
131/46: sns.histplot(x='year',y='gdp',data=gdp)
131/47: sns.histplot(x=year,y=gdp,data=gdp)
131/48: sns.histplot(data=gdp,x,y)
131/49: sns.histplot(data=gdp,x=none,y=none)
131/50: sns.histplot(data=gdp,x=None,y=None)
131/51:
sns.histplot(
    data=None,
    *,
    x=None,
    y=None,
    hue=None,
    weights=None,
    stat='count',
    bins='auto',
    binwidth=None,
    binrange=None,
    discrete=None,
    cumulative=False,
    common_bins=True,
    common_norm=True,
    multiple='layer',
    element='bars',
    fill=True,
    shrink=1,
    kde=False,
    kde_kws=None,
    line_kws=None,
    thresh=0,
    pthresh=None,
    pmax=None,
    cbar=False,
    cbar_ax=None,
    cbar_kws=None,
    palette=None,
    hue_order=None,
    hue_norm=None,
    color=None,
    log_scale=None,
    legend=True,
    ax=None,
    **kwargs,
)
131/52:
sns.histplot(
    data=None,
    x=None,
    y=None,
    hue=None,
    weights=None,
    stat='count',
    bins='auto',
    binwidth=None,
    binrange=None,
    discrete=None,
    cumulative=False,
    common_bins=True,
    common_norm=True,
    multiple='layer',
    element='bars',
    fill=True,
    shrink=1,
    kde=False,
    kde_kws=None,
    line_kws=None,
    thresh=0,
    pthresh=None,
    pmax=None,
    cbar=False,
    cbar_ax=None,
    cbar_kws=None,
    palette=None,
    hue_order=None,
    hue_norm=None,
    color=None,
    log_scale=None,
    legend=True,
    ax=None,
    **kwargs,
)
131/53:
sns.histplot(
    data=None,
    x=None,
    y=None,
    hue=None,
    weights=None,
    stat='count',
    bins='auto',
    binwidth=None,
    binrange=None,
    discrete=None,
    cumulative=False,
    common_bins=True,
    common_norm=True,
    multiple='layer',
    element='bars',
    fill=True,
    shrink=1,
    kde=False,
    kde_kws=None,
    line_kws=None,
    thresh=0,
    pthresh=None,
    pmax=None,
    cbar=False,
    cbar_ax=None,
    cbar_kws=None,
    palette=None,
    hue_order=None,
    hue_norm=None,
    color=None,
    log_scale=None,
    legend=True,
    ax=None,
  
)
131/54:
sns.histplot(
    data=None,
    x=None,
    y=None,
    hue=None,
    weights=None,
    stat='count',
    bins='auto',
    binwidth=None,
    binrange=None,
    discrete=None,
    cumulative=False,
    common_bins=True,
    common_norm=True,
    multiple='layer',
    element='bars',
    fill=True,
    shrink=1,
    kde=False,
    kde_kws=None,
    line_kws=None,
    thresh=0,
    pthresh=None,
    pmax=None,
    cbar=False,
    cbar_ax=None,
    cbar_kws=None,
    palette=None,
    hue_order=None,
    hue_norm=None,
    color=red,
    log_scale=None,
    legend=True,
    ax=None,
  
)
131/55:
sns.histplot(
    data=None,
    x=None,
    y=None,
    hue=None,
    weights=None,
    stat='count',
    bins='auto',
    binwidth=None,
    binrange=None,
    discrete=None,
    cumulative=False,
    common_bins=True,
    common_norm=True,
    multiple='layer',
    element='bars',
    fill=True,
    shrink=1,
    kde=False,
    kde_kws=None,
    line_kws=None,
    thresh=0,
    pthresh=None,
    pmax=None,
    cbar=False,
    cbar_ax=None,
    cbar_kws=None,
    palette=None,
    hue_order=None,
    hue_norm=None,
    color=Red,
    log_scale=None,
    legend=True,
    ax=None,
  
)
131/56: sns.histplot(data=gdp,x=year,y=None)
131/57: sns.histplot(data=gdp,x='year',y=None)
131/58: sns.histplot(data=gdp,x='year',y='GDP')
131/59: sns.histplot(data=gdp,x='year',y='GDP',bins='auto')
131/60: sns.pairplot(data=gdp,x='year',y='GDP',bins='auto')
131/61: sns.pairplot(data=gdp,bins='auto')
131/62: sns.pairplot(data=gdp)
131/63: sns.catplot(x="year", y="gdp", data=gdp)
131/64: sns.catplot(x="year", y="gdp",hue="class", kind="bar", data=gdp)
131/65: sns.catplot(x="year", y="gdp",hue="gdp", kind="year", data=gdp)
131/66: sns.catplot(x="year", y="gdp",hue="gdp", kind="bar", data=gdp)
131/67: sns.factorplot(x="year", y="gdp",hue="gdp", kind="bar", data=gdp)
131/68: sns.barplot(x="year",y="gdp", ci=None, data=gdp)
131/69: gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
131/70: pd.set_option('display.max_rows',100)
131/71: gdp1.describe()
131/72: sns.pairplot(data=gdp)
131/73: sns.pairplot(data=gdp1)
131/74: sns.barplot(x="year",y="gdp", ci=None, data=gdp1)
131/75: sns.barplot(x="year",y="GDP", ci=None, data=gdp1)
131/76: sns.barplot(x="Year",y="GDP", ci=None, data=gdp1)
131/77: sns.barplot(x="Year",y="GDP", data=gdp1)
131/78: sns.barplot(data=gdp1, x="Year",y="GDP",bins=10 )
131/79: sns.barplot(data=gdp1, x="Year",y="GDP",ci= sd,bins=10 )
131/80: sns.barplot(data=gdp1, x="Year",y="GDP",ci= "sd',)
131/81: sns.barplot(data=gdp1, x="Year",y="GDP",ci= "sd')
131/82: sns.barplot(data=gdp1, x="Year",y="GDP")
131/83: sns.catplot(x="Year", kind="count", palette="ch:.25", data=gdp1)
131/84: sns.catplot(x="Year", y="GDP",kind="count", palette="ch:.25", data=gdp1)
131/85: sns.catplot(x="Year", y="GDP",palette="ch:.25", data=gdp1)
131/86: sns.catplot(x="Year", y="GDP",kind = bar ,palette="ch:.25", data=gdp1)
131/87: sns.catplot(x="Year", y="GDP",kind = "bar" ,palette="ch:.25", data=gdp1)
131/88: sns.catplot(x="Year", y="GDP",kind = "bar" ,palette="ch:25", data=gdp1)
131/89: sns.catplot(x="Year", y="GDP",kind = "bar" ,bins=10, data=gdp1)
131/90: sns.distplot(x="Year", y="GDP",kind = "bar" ,bins=10, data=gdp1)
131/91: sns.histplot(x="Year", y="GDP",kind = "bar" ,bins=10, data=gdp1)
131/92: sns.histplot(x="Year", y="GDP",bins=10, data=gdp1)
131/93: sns.distplot(x="Year", y="GDP",bins=10, data=gdp1)
131/94: sns.distplot(x="Year",bins=10, data=gdp1)
131/95: sns.distplot(x="Year",bins=10,)
131/96: sns.distplot(gdp1["Year"],bins=10,)
131/97: sns.distplot(gdp1["Year","GDP"],bins=10,)
131/98: sns.distplot(gdp1["Year"],bins=10,)
131/99: sns.displot(gdp1["Year"],bins=10,)
131/100: sns.jointplot(data=gdp1, x="Year",y="GDP")
131/101: sns.jointplot(data=gdp1, x="Year",y="GDP",kind = "bar" )
131/102: sns.jointplot(data=gdp1, x="Year",y="GDP", kind = "bar" )
131/103: sns.jointplot(data=gdp1, x="Year",y="GDP" )
131/104:
sns.set_style('whitegrid') 


#Data - 'iris'
df = sns.load_dataset('iris') 
print(df.head())

#Displot- used for univariant set of observations and visualizes it through a histogram 
#i.e. only one observation and hence we choose one particular column of the dataset.
#KDE is a way to estimate the probability density function (PDF) of the random variable that underlies the sample.
#KDE is a means of data smoothing.
#bins is used to set the number of bins you want in your plot and it actually depends on your dataset.
#color is used to specify the color of the plot
sns.distplot(df['petal_length'], kde = True, color ='red', bins = 30).set_title('Dist Plot') 

#Joinplot/jointgrid- draw a plot of two variables with bivariate and univariate graphs. It basically combines two different plots.
#Plot a bi-variate distribution along with marginal distributions in the same plot
#Joint Distribution of two variables can be visualised using scatter plot/regplot or kdeplot.
#Marginal Distribution of variables can be visualised by histograms and/or kde plot
#KDE shows the density where the points match up the most 
#The Axes-level function to use for joint distribution must be passed to JointGrid.plot_joint(). 
#The Axes-level function to use for marginal distribution must be passed to JointGrid.plot_marginals()

jointgrid = sns.JointGrid(x='petal_length', y='petal_width', data=df)
jointgrid.plot_joint(sns.scatterplot)
jointgrid.plot_marginals(sns.distplot)

#jointplot() to plot bi-variate distribution along with marginal distributions. 
#It uses JointGrid() and JointGrid.plot_joint() in the background.
g=sns.jointplot(x = 'petal_length',y = 'petal_width',data = df,kind = 'hex') 
g.fig.suptitle('Joint Plot')

#Pairplot-  pairwise relation across the entire dataframe
#hue sets up the categorical separation between the entries in the dataset.
#palette is used for designing the plots.
g=sns.pairplot(df, hue ="species", palette ='coolwarm') 
g.fig.suptitle("Pair Plot 1")
g.add_legend()


#PairGrid() - creates Axes for each pair of variables 
#PairGrid.map() - draws the plot on each Axes using data corresponding to that pair of variables
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_offdiag(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)

#Different kind of plots on Upper Triangular Axes, Diagonal Axes and Lower Triangular Axes.
pairgrid = sns.PairGrid(data=df)
pairgrid = pairgrid.map_upper(sns.scatterplot)
pairgrid = pairgrid.map_diag(plt.hist)
pairgrid = pairgrid.map_lower(sns.kdeplot)

#Avoid Redundancy
g = sns.PairGrid(df, diag_sharey=False, corner=True)
g.map_lower(sns.scatterplot)
g.map_diag(sns.kdeplot)
131/105: sns.displot(gdp1["GDP"],bins=10,)
131/106: sns.displot(gdp1["GDP"])
131/107:
sns.pairplot(data=gdp1)
sns.displot(gdp1["GDP"])
131/108: sns.countplot(gdp1["GDP"])
131/109: sns.countplot(gdp1)
131/110: sns.countplot(data=gdp1)
131/111: sns.countplot(data=gdp1,'Year')
131/112: sns.countplot('Year',data=gdp1)
131/113: sns.countplot('GDP',data=gdp1)
131/114: sns.barplot(x='GDP',data=gdp1)
131/115: sns.barplot(x='GDP',y='Year',data=gdp1)
131/116: sns.jointplot(x='GDP',y='Year',data=gdp1)
131/117: sns.jointplot(x='GDP',y='Year',kind='scatter',data=gdp1)
131/118: sns.jointplot(x='GDP',y='Year',kind='bar',data=gdp1)
131/119: sns.jointplot(x='GDP',y='Year',kind='hist',data=gdp1)
131/120: sns.jointplot(x='GDP',y='Year',kind='Kde',data=gdp1)
131/121: sns.jointplot(x='GDP',y='Year',kind='reg',data=gdp1)
131/122: sns.jointplot(x='GDP',y='Year',kind='hist',data=gdp1)
131/123: sns.jointplot(x='GDP',y='Year',data=gdp1)
131/124: sns.jointplot(x='GDP',y='Year',kind='hex',data=gdp1)
131/125: sns.catplot(x ='Year', y ='Gdp', data = df, kind ='bar')
131/126: sns.catplot(x ='Year', y ='GDP', data = df, kind ='bar')
131/127: sns.catplot(x ='Year', y ='GDP', data = df, kind ='bar')
131/128: sns.catplot(x ='Year', y ='GDP', data = df)
131/129: sns.catplot(x ='Year', y ='GDP', data = gdp1, kind ='bar')
131/130: sns.catplot(x ='Year', y ='GDP', data = gdp1, kind ='bar',bins=10)
131/131: sns.catplot(x ='Year', y ='GDP', data = gdp1, kind ='pointplot')
131/132: sns.catplot(x ='Year', y ='GDP', data = gdp1, kind ='point')
131/133: sns.catplot(x ='Year', y ='GDP', data = gdp1, kind ='line')
131/134: tnc = (r"C:\Users\Admin\Desktop\titanic ml\train.csv)
131/135: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\titanic ml\train.csv)
131/136: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\titanic ml\train.csv")
131/137: tnc.head()
131/138: sns.lineplot(x="GDP", y="Year", hue="GDP", style="time", data=df,ax=ax[0]).set_title("Line Plot")
131/139: sns.lineplot(x="Year", y="Year", hue="GDP", style="time", data=df,ax=ax[0]).set_title("Line Plot")
131/140: sns.lineplot(x="Year", y="Year", hue="GDP", style="time", data=gdp1).set_title("Line Plot")
131/141: sns.lineplot(x="Year", y="GDP", hue="GDP", data=gdp1).set_title("Line Plot")
131/142: sns.lineplot(x="Year", y="GDP", data=gdp1).set_title("Line Plot")
131/143: sns.histplot("Pclass",data="tnc")
131/144: sns.histplot(x="Pclass",data="tnc")
131/145: sns.histplot(x = "Pclass",data="tnc")
131/146: sns.jointplot(x = "Pclass", data="tnc")
131/147: sns.jointplot(x = "Pclass",hue="survived" data="tnc")
131/148: sns.jointplot(x = "Pclass", hue = "Survived" data="tnc")
131/149: sns.displot(x = "Pclass", hue = "Survived" data="tnc")
131/150:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
131/151: sns.displot(x = "Pclass", hue = "Survived" data="tnc")
131/152: sns.countplot(x = "Pclass", hue = "Survived" data="tnc")
131/153: sns.displot(x = "Pclass", y =  hue = "Survived" data="tnc")
131/154: sns.displot(x = "Pclass", hue = "Survived" data="tnc")
131/155: sns.displot(x = "Pclass", hue = "Survived", data="tnc")
131/156: sns.displot(x = "Pclass", hue = "Survived", data="tnc")
131/157: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\titanic ml\train.csv")
131/158: tnc.head()
131/159: sns.displot(x = "Pclass", hue = "Survived", data="tnc")
131/160: sns.displot(tnc["Pclass"], hue = "Survived", data="tnc")
131/161: sns.displot(tnc["Pclass"], hue = "Survived",)
131/162: sns.displot(tnc["Pclass"], hue = "Survived")
131/163: sns.displot(tnc["Pclass"])
131/164: sns.displot(tnc["Pclass"],hue="Survived")
131/165: sns.displot(tnc["Pclass"],hue="Survived")
131/166: sns.jointplot(tnc["Pclass"],hue="Survived")
131/167: sns.plot(tnc["Pclass"],hue="Survived")
131/168: sns.catplot(tnc["Pclass"],hue="Survived")
131/169: sns.lineplot(x="Year", y="GDP", data=gdp1).set_title("Line Plot")
131/170: sns.displot(gdp1["GDP"])
131/171: sns.jointplot(gdp1["GDP"] )
131/172: sns.catplot(x ="Sex", hue ="Survived",  kind ="count", data = tnc)
131/173: sns.catplot(x ="Pclass", hue ="Survived",  kind ="count", data = tnc)
131/174: sns.catplot(x ="GDP", y='Year'  kind ="count", data = tnc)
131/175: sns.catplot(x ="GDP", y='Year', data = tnc)
131/176: sns.catplot(x ="GDP", y='Year', data = gdp1)
131/177: sns.catplot(x ="GDP", y='Year',Kind="bar" data = gdp1)
131/178: sns.catplot(x ="GDP", y='Year',Kind="bar",data = gdp1)
131/179: sns.catplot(x ="GDP", y='Year',kind="bar",data = gdp1)
131/180: sns.catplot(x ="Year", y='GDP',kind="bar",data = gdp1)
131/181: sns.catplot(x ="Year", y='GDP',kind="bar", bin=10, data = gdp1)
131/182: sns.catplot(x ="Year", y='GDP',kind="bar", bins=10, data = gdp1)
131/183: sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1)
131/184: sns.boxenplot(x ="Year", y='GDP',kind="bar", data = gdp1)
131/185: sns.boxenplot(x ="Year", y='GDP', data = gdp1)
131/186: sns.boxenplot(x ="Year", y='GDP',  scale='exponential',data = gdp1)
131/187: sns.boxenplot(x ="Year", y='GDP',data = gdp1)
131/188: sns.lineplot(x="Cabin", y="Fare", data=gdp1).set_title("Line Plot")
131/189: sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[4,1]
131/190: sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[4,1])
131/191: sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1)
131/192:
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1) 
sns.displot(gdp1["GDP"])
sns.lineplot(x="Year", y="GDP", data=gdp1).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[4,1])
131/193:
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[1,1) 
sns.displot(gdp1["GDP"])
sns.lineplot(x="Year", y="GDP", data=gdp1).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[4,1])
131/194:
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[1,1]) 
sns.displot(gdp1["GDP"])
sns.lineplot(x="Year", y="GDP", data=gdp1).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[4,1])
131/195:
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[1,1]) 
sns.displot(gdp1["GDP"],ax=ax[1,2])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[2,1]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[2,2])
131/196:
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[1,1]) 
sns.displot(gdp1["GDP"],ax=ax[2,2])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[2,1]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[2,2])
131/197:
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[1,1]) 
sns.displot(gdp1["GDP"],ax=ax[2,1])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[3,1]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[4,1])
131/198:
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[0,0]) 
sns.displot(gdp1["GDP"],ax=ax[0,1])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/199:
sns.displot(gdp1["GDP"],ax=ax[0,0])
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[0,1]) 

sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/200: sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1)
131/201:
sns.displot(gdp1["GDP"],ax=ax[0,0])
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,2])
131/202:
sns.displot(gdp1["GDP"],ax=ax[0,0])
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1)
131/203:
sns.displot(gdp1["GDP"],ax=ax[0,0])
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP").set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1)
131/204:
sns.displot(gdp1["GDP"],ax=ax[0,0])
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1)
131/205: sns.lineplot(x="Cabin", y="Fare", data=gdp1).set_title("Line Plot")
131/206:
sns.displot(gdp1["GDP"],ax=ax[0,0])
sns.catplot(x ="Year", y='GDP',kind="bar", data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/207:
sns.displot(gdp1["GDP"],ax=ax[0,0])
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/208:
sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/209: sns.histplot(gdp1["GDP"],ax=ax[0,0])
131/210: sns.histplot(gdp1["GDP"])
131/211:
sns.histplot(gdp1["GDP"],ax=ax[0,1])
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/212:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,1])
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/213:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[0,1]) 
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/214:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[1,0]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[0,1]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/215:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/216:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/217:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/218:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',bins=10, data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/219:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',capsize=10, data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/220:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',capsize=10,ci=10, data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/221:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',capsize=10,ci=30, data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/222:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',capsize=10,ci='sd', data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/223:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='sd', data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="b", data=gdp1,ax=ax[1,1])
131/224: sns.scatterplot(x="Cabin", y="Fare", data=gdp1).set_title("Line Plot")
131/225:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='sd', data = gdp1,ax=ax[1,0]) 
sns.boxenplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1])
131/226: sns.catplot(x ="Embarked", hue ="Survived",  kind ="count", data = tnc)
131/227:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0])
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='sd', data = gdp1,ax=ax[1,0]) 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1])
131/228:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='sd', data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
131/229:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='30', data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
131/230: sns.pairplot(tnc)
131/231: tnc.corr()
131/232: tnc.corr(method=histogram_intersection)
131/233: tnc.corr()
131/234: sns.heatmap(tnc.corr(),cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')
131/235:
cat = tnc.corr()
sns.heatmap(cat,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')
131/236:
cat = tnc.corr()
a = cat["survived"] 
sns.heatmap(a,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')
131/237:
cat = tnc.corr()
a = cat["survived"],cat["age"] 
sns.heatmap(a,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')
131/238:
cat = tnc.corr() 
sns.heatmap(a,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')
131/239:
cat = tnc.corr() 
sns.heatmap(cat,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')
133/1:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
133/2: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\titanic ml\train.csv")
133/3: tnc.corr()
133/4:
cat = tnc.corr() 
sns.heatmap(cat,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,ax=ax[0,1]).set_title('Heat Map Iris')
133/5:
cat = tnc.corr() 
sns.heatmap(cat,cmap='coolwarm' , linecolor ='black', linewidths = 1, annot=True,).set_title('Heat Map Iris')
133/6:

sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='30',dodge=False, data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/7:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='30', dodge=False, data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/8:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
133/9:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='30', dodge=False, data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/10:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='3', dodge=False, data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/11:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='366', dodge=False, data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/12:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',ci='0', dodge=False, data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/13:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = gdp1,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/14:
fg = sns.factorplot(x='Year', y='GDP', hue='None',
                        size=6,  aspect=2,
                        kind='bar', 
                        width=10, # Factorplot passes arguments through
                        data=gdp1)
133/15:
fg = sns.factorplot(x='Year', y='GDP', 
                        size=6,  aspect=2,
                        kind='bar', 
                        width=10, # Factorplot passes arguments through
                        data=gdp1)
133/16:
fg = sns.factorplot(x='Year', y='GDP', 
                        size=6,  aspect=2,
                        kind='bar', 
                        width=1, # Factorplot passes arguments through
                        data=gdp1)
133/17:
fg = sns.catplot(x='Year', y='GDP', 
                        size=6,  aspect=2,
                        kind='bar', 
                        width=1, # Factorplot passes arguments through
                        data=gdp1)
133/18:
fg = sns.catplot(x='Year', y='GDP', 
                        height=6,  aspect=2,
                        kind='bar', 
                        width=1, # Factorplot passes arguments through
                        data=gdp1)
133/19:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')

ax.set_box_aspect(10/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/20:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')

ax.set_box_aspect(1/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/21:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')

ax.set_box_aspect(100/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/22:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')

ax.set_box_aspect(50/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/23:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(50/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/24:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(10/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/25:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(20/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/26:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(50/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/27:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(100/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/28:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(10/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/29:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(1/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/30:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(5/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/31:
import pylab as plt
import seaborn as sns

tips = sns.load_dataset("tips")
fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(15/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/32:
import pylab as plt
import seaborn as sns


ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(15/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/33:
import pylab as plt
import seaborn as sns


fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(15/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/34:
import pylab as plt
import seaborn as sns


fig, ax = plt.subplots()

ax = sns.barplot(data=gdp1, ax=ax, x='Year[2010]', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(15/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/35: gdp1.head()
133/36: gdp1
133/37: gdp1[47,2]
133/38: gdp1[0:47,2]
133/39: gdp1[0:47,1:2]
133/40: gdp1[0:5,1:5]
133/41: gdp1[0:5,]
133/42: gdp1[0:,1]
133/43: u=gdp1.iloc[0:,1]
133/44:
u=gdp1.iloc[0:,1]
u
133/45:
u=gdp1.iloc[0:5,1]
u
133/46:
u=gdp1.iloc[1:5,1]
u
133/47:
u=gdp1.iloc[1:5,1:2]
u
133/48:
u=gdp1.iloc[1:5,:]
u
133/49:
u=gdp1.iloc[47:58,:]
u
133/50:
u=gdp1.iloc[47:59,:]
u
133/51:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/52:
u=gdp1.iloc[47:59,1:]
u
133/53:
u=gdp1.iloc[47:59,1:3]
u
133/54:
u=gdp1.iloc[47:59,2:]
u
133/55:
u=gdp1.iloc[47:59,:2]
u
133/56:
u=gdp1.iloc[47:59,:3]
u
133/57:
u=gdp1.iloc[47:59,2:3]
u
133/58:
u=gdp1.iloc[47:59,0:3]
u
133/59:
u=gdp1.iloc[47:59,1:3]
u
133/60:
u=gdp1.iloc[47:59,1:2]
u
133/61:
u=gdp1.iloc[47:59,0:2]
u
133/62:
u=gdp1.iloc[47:59,0:0]
u
133/63:
u=gdp1.iloc[47:59,1:0]
u
133/64:
u=gdp1.iloc[47:59,1:1]
u
133/65:
u=gdp1.iloc[47:59,1:]
u
133/66:
u=gdp1.iloc[47:59,0:]
u
133/67:
u=gdp1.iloc[47:59,0:1]
u
133/68:
u=gdp1.iloc[47:59,0:2]
u
133/69:
u=gdp1.iloc[47:59,:]
u
133/70:
import pylab as plt
import seaborn as sns


fig, ax = plt.subplots()
ax = sns.barplot(data=gdp1, ax=ax, x='Year[2010]', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(15/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/71:
import pylab as plt
import seaborn as sns


fig, ax = plt.subplots()
ax = sns.barplot(data=gdp1, ax=ax, x='Year', y='GDP')
fig.set_size_inches(18.5, 10.5)
ax.set_box_aspect(15/len(ax.patches)) #change 10 to modify the y/x axis ratio
plt.show()
133/72:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot").set_box_aspect(15/len(ax.patches))
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/73:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_box_aspect(15/len(ax.patches))
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/74:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/75:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/76:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP',size=10, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/77:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", size=10, data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/78:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", size='10', data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/79:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", size=10, data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/80:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", size=0, data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
133/81:
gdp1 = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\ML_CLASS_6_SEABORN\India_GDP_Data.csv")
sns.set_style('darkgrid') 
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)

sns.histplot(gdp1["GDP"],ax=ax[0,0]).set_title("hist Plot")
sns.lineplot(x="Year", y="GDP", data=gdp1,ax=ax[0,1]).set_title("Line Plot")
sns.barplot(x ="Year", y='GDP', dodge=False, data = u,ax=ax[1,0]).set_title("bar Plot") 
sns.kdeplot(x="Year", y="GDP", color ="r", data=gdp1,ax=ax[1,1]).set_title("kde Plot")
136/1:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
136/2: tnc.isnull()
136/3: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\titanic ml\train.csv")
136/4: tnc.isnull()
136/5: tnc.isnull?
136/6: tnc.isna()
136/7: tnc.isna().isna()
136/8: tnc.describe(tnc.isna().isna())
136/9: tnc.describe(tnc.isna())
136/10:
null_counts = tnc.isnull().sum()
null_counts[null_counts > 0].sort_values(ascending=False)
136/11: null_counts = tnc.isnull().sum()
136/12: null_counts = tnc.isnull().sum()
136/13:
null_counts = tnc.isnull().sum()
null_counts
136/14:
null_counts = tnc.isnull().sum()
null_counts
null_counts[null_counts > 0].sort_values(ascending=False)
136/15:
null_counts = tnc.isnull().sum()
null_counts
null_counts[null_counts > 0].sort_values(ascending=True)
136/16:
null_counts = tnc.isnull().sum()
null_counts
null_counts[null_counts > 0].sort_values(ascending=False)
136/17:
null_counts = tnc.isnull().sum()
null_counts[null_counts>0].sort_values(ascending=False)
136/18:
null_counts = tnc.isnull().sum()
null_counts.sort_values(ascending=False)
136/19:
null_counts = tnc.isnull().sum()
null_counts>0.sort_values(ascending=False)
136/20:
null_counts = tnc.isnull().sum()
[null_counts>0].sort_values(ascending=False)
136/21:
null_counts = tnc.isnull().sum()
null_counts[null_counts>0].sort_values(ascending=False)
136/22: sns.barplot(x="Cabin", y="Fare", data=gdp1).set_title("Line Plot")
136/23: sns.barplot(x="Cabin", y="Fare", data=tnc).set_title("Line Plot")
136/24:
sns.set_width(18.5,7)
sns.barplot(x="Cabin", y="Fare", data=tnc).set_title("bar Plot")
136/25:
fig.set_width(18.5,7)
sns.barplot(x="Cabin", y="Fare", data=tnc).set_title("bar Plot")
136/26:
fig , ax= ax.set_width(18.5,7)
sns.barplot(x="Cabin", y="Fare", data=tnc).set_title("bar Plot")
136/27:
fig.set_size_inches(18.5, 10.5)
sns.barplot(x="Cabin", y="Fare", data=tnc).set_title("bar Plot")
136/28:
fig = plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.barplot(x="Cabin", y="Fare", data=tnc).set_title("bar Plot")
136/29:
fig , a = plt.subplots()
fig.set_size_inches(18.5, 10.5)
a = sns.barplot(x="Cabin", y="Fare", data=tnc).set_title("bar Plot")
136/30: tnc.head()
136/31:
for i in tnc["Sibsp"]:
    if i ==0:
        print(i)
136/32:
for i in tnc["SibSp"]:
    if i ==0:
        print(i)
136/33:
for i in tnc["SibSp"]:
    if i ==0:
        print(index(i))
136/34:
for i in tnc["SibSp"]:
    if i ==0:
        print(index.(i))
136/35:
for i in tnc["SibSp"]:
    if i ==0:
        print(index(i))
136/36:
for i in tnc["SibSp"]:
    if i ==0:
        print(i(index))
136/37:
for i in tnc["SibSp"]:
    if i ==0:
        print(i.index)
136/38:
for i in tnc["SibSp"]:
    if i ==0:
        print(i.index())
136/39:
for i in tnc["SibSp"]:
    if i ==0:
        print(i.index("i"))
136/40:
for i in tnc["SibSp"]:
    if i ==0:
        print(index("i"))
136/41:
for i in tnc["SibSp"]:
    if i ==0:
        print(id("i"))
136/42:
for i in tnc["SibSp"]:
    if i ==0:
        print(tnc.index("i"))
136/43:
for i in tnc["SibSp"]:
    if i ==0:
        print(tnc.index(i))
136/44:
for i in tnc["SibSp"]:
    if i ==0:
        print(tnc["SibSp"].index(i))
136/45:
for i in tnc["SibSp"]:
    if i ==0:
        print(tnc["SibSp"].index("i"))
136/46:
for i in tnc["SibSp"]:
    if i ==0:
        tnc["Age"].mean()
136/47:
for i in tnc["SibSp"]:
    if i ==0:
        a=tnc["Age"].mean()
        a
136/48:
for i in tnc["SibSp"]:
    if i ==0:
        a=tnc["Age"].mean()
        print(a)
136/49:
for i in tnc["SibSp"]:
    if i ==0:
        a=tnc["Age"].mean()
print(a)
136/50: q , r = divmod(4,9)
136/51: q , r = divmod(4,9)
136/52:
q , r = divmod(4,9)
print(q,r)
136/53:
q , r = divmod(12,9)
print(q,r)
136/54:
q , r = divmod(28,9)
print(q,r)
136/55:
q , r = divmod(2,17)
print(q,r)
136/56:
q , r = divmod(4,17)
print(q,r)
136/57:
q , r = divmod(6,17)
print(q,r)
136/58:
q , r = divmod(8,17)
print(q,r)
136/59:
q , r = divmod(10,17)
print(q,r)
136/60:
q , r = divmod(12,17)
print(q,r)
136/61:
q , r = divmod(14,17)
print(q,r)
136/62:
q , r = divmod(16,17)
print(q,r)
136/63:
q , r = divmod(18,17)
print(q,r)
136/64:
q , r = divmod(133,141)
print(q,r)
136/65:
q , r = divmod(152,141)
print(q,r)
136/66:
q , r = divmod(171,141)
print(q,r)
136/67:
for i in range(0,2000):
    q , r = divmod(i,141)
    if r == 1:
        print(q,r)
136/68:
for i in range(0,2000):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r)
136/69:
for i in range(0,2000):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r)
136/70:
for i in range(0,2000):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r)
        print(i if r ==1)
136/71:
for i in range(0,2000):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r,i)
136/72:
for i in range(0,2000):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r,)
136/73:
for i in range(0,2000):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r)
136/74:
for i in range(0,200):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r)
136/75:
for i in range(0,200):
    q , r = divmod(9^i,141)
    if r == 1:
        print(q,r,i)
136/76:
for i in range(0,2000):
    q , r = divmod(19^i,141)
    if r == 1:
        print(q,r,i)
136/77:   q , r = divmod(342,141)
136/78: q , r = divmod(342,141)
136/79:
q , r = divmod(342,141)
print(q,r)
136/80:
for i in range(0,2000):
    q , r = divmod(19**i,141)
    if r == 1:
        print(q,r,i)
136/81:
for i in range(0,200):
    q , r = divmod(19**i,141)
    if r == 1:
        print(q,r,i)
136/82: i = 19* 46
136/83:
i = 19* 46
i
136/84:
for i in range(0,200):
    q , r = divmod(19**i,141)
    if r == 1:
        print(q,r,i)
        
q , r = divmod(342,141)
print(q,r)

i = 19* 46
print(i)
136/85:
for i in range(0,200):
    q , r = divmod(19**i,141)
    if r == 1:
        print(q,r,i,"\n")
        
q , r = divmod(342,141)
print(q,r)

i = 19* 46
print(i)
136/86:
for i in range(0,200):
    q , r = divmod(19**i,141)
    if r == 1:
        print(q,r,i,"\n")
        
q , r = divmod(874,141)
print(q,r)

i = 19* 46
print(i)
136/87:
i = 19* 45
i
136/88:
for i in range(0,200):
    q , r = divmod(19**i,141)
    if r == 1:
        print(q,r,i,"\n")
        
q , r = divmod(855,141)
print(q,r)

i = 19* 46
print(i)
137/1: treepersqkm={usa:40,britain:10,india:30,brazil:20}
137/2: treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/3:
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
def moretress(treepersqkm):
    for i in treepersqkm:
        if i > 40:
            print(i)
137/4:
def moretress(treepersqkm):
    for i in treepersqkm:
        if i > 40:
            print(i)
            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/5:
def moretress(treepersqkm):
    for i in treepersqkm:
        if i > 40:
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/6:
def moretress(a):
    for i in treepersqkm:
        if i > 40:
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/7:
def moretress(a):
    for i in treepersqkm:
        if i.value > 40:
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/8:
def moretress(a):
    for i in treepersqkm:
        if i :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/9:
def moretress(a):
    for i in treepersqkm:
        if i[usa] :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/10:
def moretress(a):
    for i in treepersqkm:
        if i["usa"] :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/11:
def moretress(a):
    for i in treepersqkm:
        if i :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/12:
def moretress(a):
    for i in treepersqkm:
        if i[usa] :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/13:
def moretress(a):
    for i in treepersqkm:
        if i{usa} :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/14:
def moretress(a):
    for i in treepersqkm:
        if i usa :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/15:
def moretress(a):
    for i in treepersqkm:
        if i[0] :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/16:
def moretress(a):
    for i in treepersqkm:
        if i[1] :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/17:
def moretress(a):
    for i in treepersqkm:
        if i[1] > 20 :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}
137/18:
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20 :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}  
lst = []
137/19:
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20 :
            print(treepersqkm[i])
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}  
lst = []
137/20:
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20 :
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}  
lst = []
137/21:
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20 :
            lst.append(i)
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40,"britain":10,"india":30,"brazil":20}  
lst = []
137/22:
def rps_game(a,b,c,d):

     if(c=="rock"):

                   if(d=="rock"):

                               print("It's A Tie")

                  elif(d=="paper"):

                              print(b," is the winner")

                  else:

                              print(a," is the winner")

      elif(c=="paper"):

                 if(d=="paper"):

                               print("It's A Tie")

                  

  elif(d=="scissor"):

                               print(b," is the winner")

                 else:

                               print(a," is the winner")

    elif(c=="scissor"):

               if(d=="scissor"):

                               print("It's A Tie")

              elif(d=="rock"):

                               print(b," is the winner")

               else:

                               print(a," is the winner")

def new_game():

    a=str(input("Enter The Name Of the Player1 "))

    c=str(input("Enter Rock, Paper or Scissor "))

    b=str(input("Enter The Name Of the Player2 "))

    d=str(input("Enter Rock, Paper or Scissor "))

    e=["rock","scissor","paper"]

    if((c not in e)or(d not in e)):

        print("invalid entry dude")

    else:

        rps_game(a,b,c,d)

new_game()

for i in range(100):

    f=input("Do you wish to start a new game Y or N ?")

    if f=="y":

        new_game()

    elif f=="n":

        print("Done dude")

        break

    else:

        print("invalid entry")

        break
137/24:
list = [{ "name" : "Nandini", "age" : 20},
{ "name" : "Manjeet", "age" : 24 },
{ "name" : "Nikhil" , "age" : 19 }]

print("The list printed sorting by age in descending order: ")
print(sorted(list, key = lambda i: i['age'],reverse=True))
137/25:
list = [{ "name" : "Nandini", "age" : 20},
{"name" : "Manjeet", "age" : 24 },
{"name" : "Nikhil" , "age" : 19 }]

print("The list printed sorting by age in descending order: ")
print(sorted(list, key = lambda i: i['age'],reverse=True))
137/26:
while i in range (0,100):
    i.sum()
137/27:
while i in range(0,100):
    i.sum()
137/28:
i = 0
while i in range(0,100):
    i.sum()
137/29:
i = 0
while i in range(0,100):
    i.sum(i)
137/30:
i = 0
while (i<=100):
    i++
    i.sum()
137/31:
i = 0
while (i<=100):
    i = i++
    i.sum()
137/32:
i = 0
while (i<=100):
    i = i++
i.sum()
137/33:
i = 0
while (i<=100):
    print(i)
i.sum()
137/34:
i = 0
while (i<=100):
    i++
    print(i)
i.sum()
137/35:
i = 0
while (i<=100):
    i=i++
    print(i)
i.sum()
137/36:
i = 0
while (i<=100):
    i+
    print(i)
i.sum()
137/37:
i = 0
while (i<=100):
    i+ = i
    print(i)
i.sum()
137/38:
i = 0
while (i<=100):
    i+ == i
    print(i)
i.sum()
137/39:
count = 1


while count < 5:
    count += 1
    int(input("Enter a value: "))
139/1:
from functools import reduce

>>> numbers = [0, 1, 2, 3, 4]

>>> reduce(my_add, numbers)
0 + 1 = 1
1 + 2 = 3
3 + 3 = 6
6 + 4 = 10
10
139/2:
from functools import reduce

 numbers = [0, 1, 2, 3, 4]

 reduce(my_add, numbers)
0 + 1 = 1
1 + 2 = 3
3 + 3 = 6
6 + 4 = 10
10
139/3:
from functools import reduce

numbers = [0, 1, 2, 3, 4]

reduce(my_add, numbers)
0 + 1 = 1
1 + 2 = 3
3 + 3 = 6
6 + 4 = 10
10
139/4:
from functools import reduce

numbers = [0, 1, 2, 3, 4]

reduce(my_add, numbers)
139/5:
from functools import reduce

numbers = [0, 1, 2, 3, 4]
my_add(5, 5)
reduce(my_add, numbers)
139/6:
from functools import reduce

def my_add(a, b):
    result = a + b
    print(f"{a} + {b} = {result}")
   return result
numbers = [0, 1, 2, 3, 4]
my_add(5, 5)
reduce(my_add, numbers)
139/8:
from functools import reduce

def my_add(a, b):
    result = a + b
    print(f"{a} + {b} = {result}")
    return result
numbers = [0, 1, 2, 3, 4]
my_add(5, 5)
reduce(my_add, numbers)
139/9:
i = 0
while (i<=100):
    i= i+1
    print(i)
i.sum()
139/10:
while (i<=100):
    i= i+1
    print(i)
i.sum()
139/11:
while (i<=100):
    i= i+1
    print(i)
139/12:
while (i<=100):
    i= i+1
    print(i)
139/13:
while (i<=100):
    i= i+1
    print(i)
139/14:
i = 0
while (i<=100):
    i= i+1
    print(i)
139/15:
i = 0
while (i<=100):
    i= i+1
    print(i)
sum(i)
139/16:
i = 0
while (i<=100):
    i= i+1
    print(i)
i.sum()
139/17:
i = 0
while (i<=100):
    i= i+1
    print(i)
i.add()
139/18:
i = 0
while (i<=100):
    i= i+1
    print(i)
i.Add()
139/19:
from functools import reduce

def my_add(a, b):
    result = a + b
    print(f"{a} + {b} = {result}")
    return result
numbers = [0, 1, 2, 3, 4]

reduce(my_add, i)
139/20:
lst =[]
i = 0
while (i<=100):
    i= i+1
    i.append(lst)
    print(i)
139/21:
lst =[]
i = 0
while (i<=100):
    i= i+1
    lst.append(i)
    print(i)
139/22:
lst =[]
i = 0
while (i<=99):
    i= i+1
    lst.append(i)
    print(i)
139/23:
from functools import reduce

def my_add(a, b):
    result = a + b
    print(f"{a} + {b} = {result}")
    return result
numbers = [0, 1, 2, 3, 4]

reduce(my_add, lst)
139/24:
from functools import reduce

def my_add(a, b):
    result = a + b
    return result
numbers = [0, 1, 2, 3, 4]

reduce(my_add, lst)
139/25:
from functools import reduce

lst =[]
i = 0
while (i<=99):
    i= i+1
    lst.append(i)
    
def my_add(a, b):
    result = a + b
    return result

reduce(my_add, lst)
139/26:
j =2
if j%2 == 0:
    print("this is even number:\t " ,j)
else:
    print("this is odd number:\t",j)
139/27:
j = [2,3,4]
if j%2 == 0:
    print("this is even number:\t " ,j)
else:
    print("this is odd number:\t",j)
139/28:
j = [2,3,4]
for j in j:
    if j%2 == 0:
    print("this is even number:\t " ,j)
else:
    print("this is odd number:\t",j)
139/29:
j = [2,3,4]
for j in j:
    if j%2 == 0:
    print("this is even number:\t " ,j)
    else:
    print("this is odd number:\t",j)
139/30:
j = [2,3,4]
for j in j:
    if j%2 == 0:
    print("this is even number:\t " ,j)
    else:
    print("this is odd number:\t",j)
139/31:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("this is even number:\t " ,j)
    else:
        print("this is odd number:\t",j)
139/32:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("this is even number:\t " ,j)
    else:
        print("this is odd number:\t" ,j)
139/33:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/34:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("{%s this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/35:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("{%d this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/36:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("%d this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/37:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("{%d} this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/38:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("{d} this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/39:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("{j} this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/40:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print(%d" this is even number:\t " ,j)
    else:
        print("this is odd number:\t " ,j)
139/41:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print(j "\t is even number \t ")
    else:
        print(j "\t this is odd number \t ")
139/42:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print(j "\t is even number ")
    else:
        print(j "\t this is odd number ")
139/43:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print(j "this is even number ")
    else:
        print(j "this is odd number ")
139/44:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("\t is even number:\t " ,j)
    else:
        print("\t this is odd number:\t " ,j)
139/45:
j = [2,3,4]
for j in j:
    if j%2 == 0:
        print("\t this is even number:\t " ,j)
    else:
        print("\t this is odd number:\t " ,j)
142/1: train.isnull().issum()
142/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
142/3: train = pd.read_csv('titanic_train.csv')
142/4: train.head()
142/5: train.isnull().issum()
142/6: train.isnull().sum()
142/7:
import cufflinks as cf
cf.go_offline()
142/8: train['Fare'].iplot(kind='hist',bins=30,color='green')
142/9:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
142/10: train = pd.read_csv('titanic_train.csv')
142/11: train.head()
142/12: train.isnull().sum()
142/13: sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')
142/14:
sns.set_style('whitegrid')
sns.countplot(x='Survived',data=train)
142/15:
sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')
142/16:
sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')
142/17: sns.distplot(train['Age'].dropna(),kde=False,color='darkred',bins=40)
142/18: train['Age'].hist(bins=30,color='darkred',alpha=0.3)
142/19: sns.countplot(x='SibSp',data=train)
142/20: train['Fare'].hist(color='green',bins=40,figsize=(8,4))
142/21:
import cufflinks as cf
cf.go_offline()
142/22:
!pip install cufflinks
import cufflinks as cf
cf.go_offline()
142/23:

import cufflinks as cf
cf.go_offline()
142/24: train['Fare'].iplot(kind='hist',bins=30,color='green')
142/25: train['Fare'].iplot(kind='hist',bins=1200,color='green')
142/26: train['Fare'].iplot(kind='hist',bins=12,color='green')
142/27: train['Fare'].iplot(kind='hist',bins=12,color='green')
142/28: train['Fare'].iplot(kind='hist',bins=12,color='green')
142/29: train['Fare'].iplot(kind='hist',bins=120,color='green')
142/30: train['Fare'].iplot(kind='hist',bins=12,color='green')
142/31: train['Fare'].iplot(kind='hist',bins=2,color='green')
142/32: train['Fare'].iplot(kind='hist',bins=[0,5,5],color='green')
142/33: train['Fare'].iplot(kind='hist',bins=[1,5,5],color='green')
142/34: train['Fare'].iplot(kind='hist',bins=[1,15,5],color='green')
142/35: train['Fare'].iplot(kind='hist',bins=[1,200,5],color='green')
142/36: train['Fare'].iplot(kind='hist',bins=[0,200,5],color='green')
142/37: train['Fare'].iplot(kind='hist',bins=[0,200,1],color='green')
142/38: train['Fare'].iplot(kind='hist',bins=[0,200,10],color='green')
142/39: train['Fare'].iplot(kind='hist',bins=[0,500,10],color='green')
142/40: train['Fare'].hist(color='green',bins=[0,200,10],figsize=(8,4))
142/41: train['Fare'].hist(color='green',bins=20,figsize=(8,4))
142/42: train['Fare'].hist(color='green',bins=10,figsize=(8,4))
142/43:
plt.figure(figsize=(12, 7))
sns.boxplot(x='Pclass',y='Age',data=train,palette='winter')
142/44: train.iplot(x='Pclass',y='age',kind='box',color='rgb')
142/45: train.iplot(x='Pclass',y='age',kind='box')
142/46: .iplot(x='Pclass',y='age',kind='box')
142/47: train.iplot(x='Pclass',y='age',kind='box')
142/48: train.iplot(x='Pclass',y='age',kind='box',data=train)
142/49: train['Pclass'].iplot(x='Pclass',y='age',kind='box')
142/50: train.iplot(x='Pclass',y='AGE',kind='box')
142/51: train.iplot(x='Pclass',y='Age',kind='box')
142/52: train.iplot((x='Pclass',y='Age',,kind='box')
142/53: train.iplot((x='Pclass',y='Age',kind='box')
142/54: train.iplot(x='Pclass',y='Age',kind='box')
142/55: train['Age']['Pclass'].iplot(x='Pclass',y='Age',kind='box')
142/56: train['Age'],['Pclass'].iplot(x='Pclass',y='Age',kind='box')
142/57: train['Age'].iplot(x='Pclass',y='Age',kind='box')
142/58: train['Age''Pclass'].iplot(x='Pclass',y='Age',kind='box')
142/59: train['Age','Pclass'].iplot(x='Pclass',y='Age',kind='box')
142/60: train['Age','Pclass'].iplot(x='Pclass',y='Age',kind='box')
142/61: train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
142/62:
def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    
    if pd.isnull(Age):

        if Pclass == 1:
            return 37

        elif Pclass == 2:
            return 29

        else:
            return 24

    else:
        return Age
142/63: train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
142/64: sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')
142/65: pd.get_dummies(train['Embarked'],drop_first=True).head()
142/66:
sex = pd.get_dummies(train['Sex'],drop_first=True)
embark = pd.get_dummies(train['Embarked'],drop_first=True)
142/67: train.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)
142/68: train.head()
142/69: train.drop('Cabin',axis=1,inplace=True)
142/70:

train.head()
142/71:

train.dropna(inplace=True)
142/72: train.info()
142/73: pd.get_dummies(train['Embarked'],drop_first=True).head()
142/74: pd.get_dummies(train['Embarked'],drop_first=True).head()
142/75:
sex = pd.get_dummies(train['Sex'],drop_first=True)
embark = pd.get_dummies(train['Embarked'],drop_first=True)
142/76: sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')
142/77: train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
143/1:
def evens(x):
    if x%2 == 0:
        print("TRUE")
    else:
        print("FALSE")
evens(2)
evens(3)
144/1:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
   ch = int(input("Enter your valid input here: "))
if ch == 1:
   choice_name = 'Rock'
elifch == 2:
   choice_name = 'paper'
else:
   choice_name = 'scissor'
# print user given choice
   print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
   comp_choice_name = 'Rock'
elifcomp_choice == 2:
   comp_choice_name = 'paper'
else:
   comp_choice_name = 'scissor'
   print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
   print("scissor wins =>", end = "")
   final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
   print("<== You are the winner ==>")
else:
   print("<== Computer wins ==>")
      print("Do you want to play again? (Y/N)")
      ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
   break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/2:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
   ch = int(input("Enter your valid input here: "))
if ch == 1:
   choice_name = 'Rock'
elifch == 2:
   choice_name = 'paper'
else:
   choice_name = 'scissor'
# print user given choice
   print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
   comp_choice_name = 'Rock'
elifcomp_choice == 2:
   comp_choice_name = 'paper'
else:
   comp_choice_name = 'scissor'
   print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
   print("scissor wins =>", end = "")
   final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
   print("<== You are the winner ==>")
else:
   print("<== Computer wins ==>")
      print("Do you want to play again? (Y/N)")
      ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
   break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/3:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elifch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
   print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elifcomp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/5:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elifch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elifcomp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/6:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elifch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elifcomp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/7:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elifcomp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/8:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elifcomp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/9:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/10:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/11:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
   final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
   final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/13:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/14:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/15:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/16:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
            break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/17:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/18:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/19:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/20:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/21:
my_lst = [('banana','30','100'), ('apple','10','200'), ('baby','20','300')]
print('orginal list',my_lst)
my_lst.sort(key=lambda x:x[1], reverse= True)
print(my_lst)
144/22:
print("The list printed sorting by age: ")
print(sorted(list, key = lambda i: i['age']))
144/23:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/24:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
    break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/25:
# importing required random module
import random
print("The Rules of Rock paper scissor game will be follows: \n"
+"Rock vs paper --> paper wins \n"
+"Rock vs scissor --> Rock wins \n"
+"paper vs scissor --> scissor wins \n")
while True:
    print("Now please enter your choice no. \n 1. Rock \n 2. paper \n 3. scissor \n")
# take the input from user
ch = int(input("Now Your turn: "))
while ch> 3 or ch< 1:
    ch = int(input("Enter your valid input here: "))
if ch == 1:
    choice_name = 'Rock'
elif ch == 2:
    choice_name = 'paper'
else:
    choice_name = 'scissor'
# print user given choice
    print("Your choice is: " + choice_name)
print("\nNow its computer turn to initiate.......")
# Computer will select randomly any number
# among values 1, 2 and 3. Using randint method
# of random module
comp_choice = random.randint(1, 3)
# loopingwill continue until comp_choice value
# is equal to the choice value
while comp_choice == ch:
    comp_choice = random.randint(1, 3)
# initialize value of the variable comp_choice_name
# variable corresponding to the choice value
if comp_choice == 1:
    comp_choice_name = 'Rock'
elif comp_choice == 2:
    comp_choice_name = 'paper'
else:
    comp_choice_name = 'scissor'
    print("So computer choice is: " + comp_choice_name)
    print(choice_name + " V/s " + comp_choice_name)
   # condition for winning the game
if((ch == 1 and comp_choice == 2) or
   (ch == 2 and comp_choice ==1 )):
    print("paper wins => ", end = "")
    final_result = "paper"
elif((ch == 1 and comp_choice == 3) or
   (ch == 3 and comp_choice == 1)):
    print("Rock wins =>", end = "")
    final_result = "Rock"
else:
    print("scissor wins =>", end = "")
    final_result = "scissor"
   # Printing either user or computer wins
if final_result == choice_name:
    print("<== You are the winner ==>")
else:
    print("<== Computer wins ==>")
    print("Do you want to play again? (Y/N)")
    ans = input()
      # if user input n or N then condition is True
if ans == 'n' or ans == 'N':
       break
   # after exiting from the while loop
print("\nThanks for sharing time with us...")
144/26:
# import random module 
import random 

# Print multiline instruction 
# performstring concatenation of string 
print("Winning Rules of the Rock paper scissor game as follows: \n"
                                +"Rock vs paper->paper wins \n"
                                + "Rock vs scissor->Rock wins \n"
                                +"paper vs scissor->scissor wins \n") 

while True: 
    print("Enter choice \n 1. Rock \n 2. paper \n 3. scissor \n") 
    
    # take the input from user 
    choice = int(input("User turn: ")) 

    # OR is the short-circuit operator 
    # if any one of the condition is true 
    # then it return True value 
    
    # looping until user enter invalid input 
    while choice > 3 or choice < 1: 
        choice = int(input("enter valid input: ")) 
        

    # initialize value of choice_name variable 
    # corresponding to the choice value 
    if choice == 1: 
        choice_name = 'Rock'
    elif choice == 2: 
        choice_name = 'paper'
    else: 
        choice_name = 'scissor'
        
    # print user choice 
    print("user choice is: " + choice_name) 
    print("\nNow its computer turn.......") 

    # Computer chooses randomly any number 
    # among 1 , 2 and 3. Using randint method 
    # of random module 
    comp_choice = random.randint(1, 3) 
    
    # looping until comp_choice value 
    # is equal to the choice value 
    while comp_choice == choice: 
        comp_choice = random.randint(1, 3) 

    # initialize value of comp_choice_name 
    # variable corresponding to the choice value 
    if comp_choice == 1: 
        comp_choice_name = 'Rock'
    elif comp_choice == 2: 
        comp_choice_name = 'paper'
    else: 
        comp_choice_name = 'scissor'
        
    print("Computer choice is: " + comp_choice_name) 

    print(choice_name + " V/s " + comp_choice_name) 

    # condition for winning 
    if((choice == 1 and comp_choice == 2) or
    (choice == 2 and comp_choice ==1 )): 
        print("paper wins => ", end = "") 
        result = "paper"
        
    elif((choice == 1 and comp_choice == 3) or
        (choice == 3 and comp_choice == 1)): 
        print("Rock wins =>", end = "") 
        result = "Rock"
    else: 
        print("scissor wins =>", end = "") 
        result = "scissor"

    # Printing either user or computer wins 
    if result == choice_name: 
        print("<== User wins ==>") 
    else: 
        print("<== Computer wins ==>") 
        
    print("Do you want to play again? (Y/N)") 
    ans = input() 


    # if user input n or N then condition is True 
    if ans == 'n' or ans == 'N': 
        break
    
# after coming out of the while loop 
# we print thanks for playing 
print("\nThanks for playing")
144/27:
# import random module 
import random 

# Print multiline instruction 
# performstring concatenation of string 
print("Winning Rules of the Rock paper scissor game as follows: \n"
                                +"Rock vs paper->paper wins \n"
                                + "Rock vs scissor->Rock wins \n"
                                +"paper vs scissor->scissor wins \n") 

while True: 
    print("Enter choice \n 1. Rock \n 2. paper \n 3. scissor \n") 
    
    # take the input from user 
    choice = int(input("User turn: ")) 

    # OR is the short-circuit operator 
    # if any one of the condition is true 
    # then it return True value 
    
    # looping until user enter invalid input 
    while choice > 3 or choice < 1: 
        choice = int(input("enter valid input: ")) 
        

    # initialize value of choice_name variable 
    # corresponding to the choice value 
    if choice == 1: 
        choice_name = 'Rock'
    elif choice == 2: 
        choice_name = 'paper'
    else: 
        choice_name = 'scissor'
        
    # print user choice 
    print("user choice is: " + choice_name) 
    print("\nNow its computer turn.......") 

    # Computer chooses randomly any number 
    # among 1 , 2 and 3. Using randint method 
    # of random module 
    comp_choice = random.randint(1, 3) 
    
    # looping until comp_choice value 
    # is equal to the choice value 
    while comp_choice == choice: 
        comp_choice = random.randint(1, 3) 

    # initialize value of comp_choice_name 
    # variable corresponding to the choice value 
    if comp_choice == 1: 
        comp_choice_name = 'Rock'
    elif comp_choice == 2: 
        comp_choice_name = 'paper'
    else: 
        comp_choice_name = 'scissor'
        
    print("Computer choice is: " + comp_choice_name) 

    print(choice_name + " V/s " + comp_choice_name) 

    # condition for winning 
    if((choice == 1 and comp_choice == 2) or
    (choice == 2 and comp_choice ==1 )): 
        print("paper wins => ", end = "") 
        result = "paper"
        
    elif((choice == 1 and comp_choice == 3) or
        (choice == 3 and comp_choice == 1)): 
        print("Rock wins =>", end = "") 
        result = "Rock"
    else: 
        print("scissor wins =>", end = "") 
        result = "scissor"

    # Printing either user or computer wins 
    if result == choice_name: 
        print("<== User wins ==>") 
    else: 
        print("<== Computer wins ==>") 
        
    print("Do you want to play again? (Y/N)") 
    ans = input() 


    # if user input n or N then condition is True 
    if ans == 'n' or ans == 'N': 
        break
    
# after coming out of the while loop 
# we print thanks for playing 
print("\nThanks for playing")
145/1:
marks = eval(input('enter elements in ():'))
sum= 0
n= len(marks)
for i range(n):
    sum = sum +marks[i]
print(sum)
print(sum/n)
print(.max(marks))
print(.min(marks))
145/2:
marks = eval(input('enter elements in ():'))
sum= 0
n= len(marks)
for i range(n):
    sum = sum +marks[i]
print(sum)
print(sum/n)
print(.max(marks))
print(.min(marks))
145/3:
marks=eval(input('Enter Elements in (): '))
sum=0
n=len(marks)
for i in range(n) :
sum=sum+marks[i]
print(' Sum of All Marks : ', sum)
print(' Average of the course : ',sum/n)
print('Highest Marks : ',max(marks))
print('Lowest Marks : ',min(marks))
145/4:
marks=eval(input('Enter Elements in (): '))
sum=0
n=len(marks)
for i in range(n) :
    sum=sum+marks[i]
print(' Sum of All Marks : ', sum)
print(' Average of the course : ',sum/n)
print('Highest Marks : ',max(marks))
print('Lowest Marks : ',min(marks))
145/5:
marks=eval(input('Enter Elements in (): '))
sum=0
n=len(marks)
for i in range(n) :
    sum=sum+marks[i]
print(' Sum of All Marks : ', sum)
print(' Average of the course : ',sum/n)
print('Highest Marks : ',max(marks))
print('Lowest Marks : ',min(marks))
145/6:
marks=eval(input('Enter Elements in (): '))
sum=0
n=len(marks)
for i in range(n) :
    sum=sum+marks[i]
print(' Sum of All Marks : ', sum)
print(' Average of the course : ',sum/n)
print('Highest Marks : ',max(marks))
print('Lowest Marks : ',min(marks))
145/7:
marks=eval(input('Enter Elements in (): '))
sum=0
n=len(marks)
for i in range(n) :
    sum=sum+marks[i]
print(' Sum of All Marks : ', sum)
print(' Average of the course : ',sum/n)
print('Highest Marks : ',max(marks))
print('Lowest Marks : ',min(marks))
143/2:
i = 0
for i in range(1,200):
  q , r = divmod(9**i,141)
  if r == 1:
    print(q,r,i)
143/3:
i = 0
for i in range(1,200):
  q , r = divmod(9**i,141)
  if r == 1:
    print(q,r,i)
143/4:
i = 0
for i in range(1,200):
    q,r = divmod(9**i,141)
    if r == 1:
    print(q,r,i)
143/5:
i = 0
for i in range(1,200):
    q,r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/6:
i = 0
for i in range(1,200):
    q,r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/7:

for i in range(1,200):
    q,r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/8:

for i in range(1,200):
    q,r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/9:

for i in range(1,2000):
    q,r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/10:

for i in range(1,2000):
     q , r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/12:

for i in range(1,2000):
     q , r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/14:

for i in range(1,2000):
     q , r = divmod(9**i,141)
        if r == 1:
        print(q,r,i)
143/15:

for i in range(1,2000):
    q , r = divmod(9**i,141)
        if r == 1:
        print(q,r,i)
143/16:

for i in range(1,2000):
    q , r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
143/17:

for i in range(1,2000):
    q , r = divmod(9**i,141)
    if r == 1:
    print(q,r,i)
143/18:

for i in range(1,2000):
    q , r = divmod(9**i,141)
    if r == 1:
        print(q,r,i)
145/8:
marks=eval(input('Enter Elements in (): '))
sum=0
n=len(marks)
for i in range(n) :
    print(marks[i])
    sum=sum+marks[i]
print(' Sum of All Marks : ', sum)
print(' Average of the course : ',sum/n)
print('Highest Marks : ',max(marks))
print('Lowest Marks : ',min(marks))
145/9:
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20000 :
            lst.append(i)
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40000,"britain":10000,"india":30000,"brazil":20000}  
lst = []
145/10:
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20000 :
            lst.append(i)
            print(i)
moretress(treepersqkm)            
treepersqkm={"usa":40000,"britain":10000,"india":30000,"brazil":20000}  
lst = []
145/11:
treepersqkm={"usa":40000,"britain":10000,"india":30000,"brazil":20000} 
lst = []
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20000 :
            lst.append(i)
            print(i)
moretress(treepersqkm)
143/19:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
143/20: train = pd.read_csv('titanic_train.csv')
143/21: train.head()
143/22: train.isnull().sum()
143/23: sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')
143/24:
sns.set_style('whitegrid')
sns.countplot(x='Survived',data=train)
143/25:
sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')
143/26:
sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')
143/27: sns.distplot(train['Age'].dropna(),kde=False,color='darkred',bins=40)
143/28: train['Age'].hist(bins=30,color='darkred',alpha=0.3)
143/29: sns.countplot(x='SibSp',data=train)
143/30: train['Fare'].hist(color='green',bins=10,figsize=(8,4))
143/31:
import cufflinks as cf
cf.go_offline()
143/32: import cufflinks as cf
143/33: train['Fare'].iplot(kind='hist',bins=[0,500,10],color='green')
143/34:
import cufflinks as cf
cf.go_offline()
143/35:
plt.figure(figsize=(12, 7))
sns.boxplot(x='Pclass',y='Age',data=train,palette='winter')
143/36:
def impute_age(cols):
    Age = cols[0]
    Pclass = cols[2]
    
    if pd.isnull(Age):

        if Pclass == 1:
            return 37

        elif Pclass == 2:
            return 29

        else:
            return 24

    else:
        return Age
143/37: train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
143/38:
def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    
    if pd.isnull(Age):

        if Pclass == 1:
            return 37

        elif Pclass == 2:
            return 29

        else:
            return 24

    else:
        return Age
143/39: train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
143/40: sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')
143/41:
def impute_age(col):
    Age = cols[0]
    Pclass = cols[1]
    
    if pd.isnull(Age):

        if Pclass == 1:
            return 37

        elif Pclass == 2:
            return 29

        else:
            return 24

    else:
        return Age
143/42: train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
143/43:
def impute_age(cols):
    Age = cols[0]
    Pclass = cols[1]
    
    if pd.isnull(Age):

        if Pclass == 1:
            return 37

        elif Pclass == 2:
            return 29

        else:
            return 24

    else:
        return Age
143/44: train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)
146/1: sns.heatmap(tnc)
146/2:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
146/3: sns.heatmap(tnc)
146/4: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\titanic ml\train.csv")
146/5: tnc.head()
146/6: sns.heatmap(tnc)
146/7: sns.heatmap(tnc.isnull())
146/8: sns.boxplot(x="Age",y='Pclass')
146/9: sns.boxplot(x="Age",y='Pclass',data=tnc)
146/10: sns.boxplot(x="Age",y='Pclass',data=tnc,pallete =winter)
146/11: sns.boxplot(x="Age",y='Pclass',data=tnc,palatte ='winter')
146/12: sns.boxplot(x="Age",y='Pclass',data=tnc,palette ='winter')
146/13: sns.boxplot(x="Pclass",y='Age',data=tnc,palette ='winter')
146/14: sns.boxplot(x="Pclass",y='Age',data=tnc)
146/15: sns.boxplot(x="Pclass",y='Age',data=tnc,palette ='winter')
146/16: def impute_age(cols):
146/17:
def impute_age(cols):
    Age=col[0]
    Pclass=col[1]
    if tnc.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else Pclass ==3:
            return 24
    else:
        return Age
146/18:
def impute_age(cols):
    Age=col[0]
    Pclass=col[1]
    if tnc.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
146/19: tnc["Age"]=tnc[["Age","Pclass"]].apply(impute_age)
146/20:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if tnc.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
146/21: tnc["Age"]=tnc[["Age","Pclass"]].apply(impute_age)
146/22: tnc["Age"]=tnc[["Age","Pclass"]].apply(impute_age,axis=1)
146/23:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
146/24: tnc["Age"]=tnc[["Age","Pclass"]].apply(impute_age,axis=1)
146/25: sns.heatmap(tnc.isnull())
146/26: sns.boxplot(x="Pclass",y='cabin',data=tnc,palette ='winter')
146/27: sns.boxplot(x="Pclass",y='Cabin',data=tnc,palette ='winter')
146/28: sns.boxplot(x="Fare",y='Cabin',data=tnc,palette ='winter')
146/29: sns.barplot(x="Fare",y='Cabin',data=tnc,palette ='winter')
146/30: sns.barplot(x="Pclass",y='Cabin',data=tnc,palette ='winter')
146/31: sns.barplot(x="Pclass",y='Cabin',data=tnc)
146/32:
sns.set_size(18,7)
sns.barplot(x="Pclass",y='Cabin',data=tnc)
146/33:
sns.set_style(18,7)
sns.barplot(x="Pclass",y='Cabin',data=tnc)
146/34:
fig, ax =plt.subplots(nrows=2,ncols=2)
fig.set_size_inches(18.5, 10.5)
sns.barplot(x="Pclass",y='Cabin',data=tnc)
146/35:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.barplot(x="Pclass",y='Cabin',data=tnc)
146/36:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.regplot(x="Pclass",y='Cabin',data=tnc)
146/37:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.kdeplot(x="Pclass",y='Cabin',data=tnc)
146/38:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.plot(x="Pclass",y='Cabin',data=tnc)
146/39:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.histplot(x="Pclass",y='Cabin',data=tnc)
146/40:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.countplot(x="Pclass",y='Cabin',data=tnc)
146/41:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.count(x="Pclass",y='Cabin',data=tnc)
146/42:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Pclass",y='Cabin',data=tnc)
146/43:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Pclass",y='Cabin',data=tnc,kind = count)
146/44:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Pclass",y='Cabin',data=tnc,kind = 'count')
146/45:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Pclass",data=tnc,kind = 'count')
146/46:
fig, ax =plt.subplots()
fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Cabin",data=tnc,kind = 'count')
146/47: sns.distplot(df['Age'].dropna(),kde=False,color='darkred',bins=40)
146/48: sns.distplot(tnc['Age'].dropna(),kde=False,color='darkred',bins=40)
146/49:
fig, ax =plt
fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Cabin",data=tnc,kind = 'count')
146/50:
fig, ax 
fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Cabin",data=tnc,kind = 'count')
146/51:

fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Cabin",data=tnc,kind = 'count')
146/52: Cabin_test = tnc["Cabin"].copy()
146/53: cabin_test = tnc["Cabin"].copy()
146/54: cabin_test
146/55: cabin_test.(0,1)
146/56: cabin_test,str.slice(0,1)
146/57: cabin_only["Deck"] = cabin_test["Cabin"].str.slice(0,1)
146/58: cabin_only["Deck"] = cabin_test["cabin_test"].str.slice(0,1)
146/59: cabin_only["Deck"] = cabin_test[].str.slice(0,1)
146/60: cabin_only["Deck"] = cabin_test.str.slice(0,1)
146/61: cabin_test[cabin_test] = tnc["Cabin"].copy()
146/62: cabin_test["cabin_test"] = tnc["Cabin"].copy()
146/63: cabin_only["Deck"] = cabin_test[cabin_test].str.slice(0,1)
146/64:
cabin_test["cabin"] = tnc["Cabin"].copy()
cabin_test["Cabin"] = cabin_test["Cabin"].isnull().apply(lambda x: not x)
146/65:
cabin_test["cabin"] = tnc["Cabin"].copy()
cabin_test["cabin"] = cabin_test["cabin"].isnull().apply(lambda x: not x)
146/66: cabin_only["Deck"] = cabin_test[cabin_test].str.slice(0,1)
146/67: cabin_only["Deck"] = cabin_test[cabin].str.slice(0,1)
146/68: cabin_only["Deck"] = cabin_test["cabin"].str.slice(0,1)
146/69:
cabin_only = tnc[["Cabin"]].copy()
cabin_only["Cabin_Data"] = cabin_only["Cabin"].isnull().apply(lambda x: not x)

cabin_only["Deck"] = cabin_only["Cabin"].str.slice(0,1)
146/70: cabin_only["Deck"].value_counts()
146/71:

fig.set_size_inches(18.5, 10.5)
sns.catplot(x="Deck",data=cabin_only,kind = 'count')
146/72: sns.boxplot(x="deck",y='Fare',data=tnc,cabin_only,palette ='winter')
146/73: sns.boxplot(x="deck",y='Fare',palette ='winter')
146/74: m = tnc['fare']+cabin_only["Deck"]
146/75: m = tnc['Fare']+cabin_only["Deck"]
146/76: m = pd.dataframe(tnc['Fare'],cabin_only["Deck"])
146/77: m = pd.Dataframe(tnc['Fare'],cabin_only["Deck"])
146/78: m = pd.dataframe(tnc['Fare'],cabin_only["Deck"])
146/79: m = pd.dataFrame(tnc['Fare'],cabin_only["Deck"])
146/80: m = pd.DataFrame(tnc['Fare'],cabin_only["Deck"])
146/81: sns.boxplot(x="deck",y='Fare',data='m',palette ='winter')
146/82: sns.boxplot(x="deck",y='Fare',data=m,palette ='winter')
146/83: sns.boxplot(x="Deck",y='Fare',data=m, palette ='winter')
146/84:
m = pd.DataFrame(tnc['Fare'],cabin_only["Deck"]) 
m
146/85: tnc['Fare']
146/86: cabin_only["fare"]=tnc["Fare"]
146/87:

cabin_only
146/88: sns.boxplot(x="Deck",y='Fare',data=cabin_only, palette ='winter')
146/89: sns.boxplot(x="Deck",y='fare',data=cabin_only, palette ='winter')
146/90:
def impute_age(cols):
    Age=cols[0]
    print(cols[0])
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
146/91:
def impute_age(cols):
    Age=cols[0]
    print(cols[0])
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
Age=cols[0]
146/92:
def impute_age(cols):
    Age=cols[0]
    print(cols[0])
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
print(cols[0])
146/93:
def impute_age(cols):
    Age=cols[0]
    print(cols[0])
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
print(Age)
146/94:
def impute_age(cols):
    Age=cols[0]
  
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
        print(cols[0])
146/95:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
146/96:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
cols(0)
146/97:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
impute_age(0)
146/98:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
impute_age()
146/99:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
impute_age(tnc["age","Pclass"])
146/100:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
impute_age(tnc["Age","Pclass"])
146/101:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
impute_age(tnc[["Age","Pclass"]])
146/102:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
t=impute_age(tnc[["Age","Pclass"]])
146/103:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
t=impute_age(tnc[["Age","Pclass"]],axis=1)
146/104:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
t=impute_age(tnc[["Age","Pclass"]])
149/1:
import pandas as pd
import numpy as np
import cufflinks as cf
149/2: pd.read_csv("Train.csv")
149/3: byr = pd.read_csv("Train.csv")
149/4: byr.head()
149/5: byr.isnull().sum()
149/6: pd.iplot(byr)
149/7: pd.iplot(byr.isnull())
149/8: df.iplot(byr.isnull())
149/9:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
149/10: df.iplot(byr.isnull())
149/11: byr.iplot(byr.isnull())
149/12: byr.iplot()
149/13:
import pandas as pd
import numpy as np
import cufflinks as cf
149/14: byr.iplot()
149/15: byr.iplot(kind=heatmap)
149/16: byr.iplot(kind='heatmap')
149/17: byr.iplot(x='session_number',y='purchased'  ,kind='heatmap')
149/18: byr.describe()
149/19: byr.info()
149/20: byr.iplot(x='session_number',y='purchased',kind='heatmap')
149/21: byr.iplot(x='checked_out',y='time_spent',kind='heatmap')
149/22: byr.iplot(x='checked_out',y='time_spent')
149/23: byr.iplot(x=' purchased ',y='time_spent')
149/24: byr.iplot(x='purchased',y='time_spent')
149/25: byr.iplot(x='purchased',y='time_spent',kind=scatter)
149/26: byr.iplot(x='purchased',y='time_spent',kind='scatter')
149/27: byr.iplot(x='purchased',y='time_spent',kind='bar')
149/28:
import pandas as pd
import numpy as np
149/29: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/1:
import pandas as pd
import numpy as np
150/2: byr = pd.read_csv("Train.csv")
150/3: byr.head()
150/4: byr.isnull().sum()
150/5: byr.info()
150/6: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/7: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/8:
import pandas as pd
import numpy as np
import cufflinks as cf
150/9: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/10: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/11:
import pandas as pd
import numpy as np
import cufflinks as cf
150/12: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/13:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
150/14: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/15: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/16: byr['time_spent'].max()
150/17: i = 38494.025/60
150/18:
i = 38494.025/60
i
150/19: byr.describe()
150/20: sns.catplot(x='purchased',y='time_spent',kind='bar')
150/21:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
cf.go_offline()
150/22: sns.catplot(x='purchased',y='time_spent',kind='bar')
150/23: sns.catplot(x='purchased',y='time_spent',kind='bar',data=byr)
150/24: byr.iplot(x='purchased',y='time_spent',kind='bar')
150/25: byr.corr()
150/26: sns.catplot(x='purchased',y='time_spent',kind='scatter',data=byr)
150/27: sns.catplot(x='purchased',y='time_spent',kind='lineplot',data=byr)
150/28: sns.catplot(x='purchased',y='time_spent',kind='line',data=byr)
150/29: sns.catplot(x='purchased',y='time_spent',kind='box',data=byr)
150/30: byr.iplot(x='purchased',y='time_spent',kind='box')
150/31: byr.iplot(x='purchased',y='time_spent',kind='box')
150/32: byr.iplot(kind='box')
150/33: byr.iplot(kind='box',x='purchased',y='time_spent')
151/1:
list = [{ "name" : "mat", "age" : 20},
{"name" : "sky", "age" : 24 },
{"name" : "land" , "age" : 19 }]

print("The list printed sorting by age in descending order: ")
print(sorted(list, key = lambda i: i['age'],reverse=True))
151/2:
name=str(input("Pls enter name:"))
if( name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning ", name)
151/3:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
 if(list[i]) == 100:
print("There is a 100 at index no: ",i)
 break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
151/4:

a=[1,2,3,2]
b=[]
foriin a:
b.append(i*i)
print(b)
150/34: byr.head()
150/35: device_details.values_count()
150/36: byr[device_details].values_count()
150/37: byr["device_details"].values_count()
150/38: byr["device_details"].values()
150/39: byr["device_details"].info()
150/40: byr["device_details"].describe()
150/41: byr["device_details"].describe().unique()
150/42: byr["device_details"].describe()
150/43: byr["device_details"].str.split(' ', expand=True).stack().value_counts()
150/44: byr["device_details"].stack().value_counts()
150/45: byr[client_agent].str.split(' ', expand=True).stack().value_counts()
150/46: byr["client_agent"].str.split(' ', expand=True).stack().value_counts()
150/47: pd.set_option('display.max_rows',100)
150/48: byr["client_agent"].str.split(' ', expand=True).stack().value_counts()
150/49:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
150/50:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 999;
150/51: byr["client_agent"].str.split(' ', expand=True).stack().value_counts()
150/52: byr["device_details"].str.split(' ', expand=True).stack().value_counts()
150/53:
def impute_client_agent(cols):
    client_age=[0]
    device_details=[1]
    if client_agent.isnull():
        if device_details == Other - Other:
            return "other"
        elif device_details == Unknown - MobileWeb:
            return "unknown"
        else:
            return "think"
    else:
        return  client_agent
150/54:
def impute_client_agent(cols):
    client_age=[0]
    device_details=[1]
    if client_agent.isnull():
        if device_details == "Other - Other":
            return "other"
        elif device_details == "Unknown - MobileWeb":
            return "unknown"
        else:
            return "think"
    else:
        return  client_agent
146/105:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
150/55: byr["client_agent"]=byr[["client_agent","device_details"]].apply(impute_client_agent,axis=1)
150/56:
def impute_client_agent(cols):
    client_age=[0]
    device_details=[1]
    if client_agent.isnull():
        if device_details == "Other - Other":
            return "other"
        elif device_details == "Unknown - MobileWeb":
            return "unknown"
        else:
            return "think"
    else:
        return  client_agent
150/57: byr["client_agent"]=byr[["client_agent","device_details"]].apply(impute_client_agent,axis=1)
150/58:
def impute_client_agent(cols):
    client_age=[0]
    device_details=[1]
    if pd.isnull(client_agent):
        if device_details == "Other - Other":
            return "other"
        elif device_details == "Unknown - MobileWeb":
            return "unknown"
        else:
            return "think"
    else:
        return  client_agent
150/59: byr["client_agent"]=byr[["client_agent","device_details"]].apply(impute_client_agent,axis=1)
150/60: byr["client_agent"]=byr[["client_agent","device_details"]].apply(impute_client_agent,axis=1)
150/61:
def impute_agent(cols):
    client_age=[0]
    device_details=[1]
    if pd.isnull(client_agent):
        if device_details == "Other - Other":
            return "other"
        elif device_details == "Unknown - MobileWeb":
            return "unknown"
        else:
            return "think"
    else:
        return  client_agent
150/62: byr["client_agent"]=byr[["client_agent","device_details"]].apply(impute_agent,axis=1)
146/106:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
146/107: tnc["Age"]=tnc[["Age","Pclass"]].apply(impute_age,axis=1)
150/63: pd.isnull(client_agent)
150/64: byr.isnull(client_agent)
150/65: byr.isnull("client_agent")
150/66: byr.isnull(client_agent)
150/67: pd.isnull(client_agent)
146/108:  pd.isnull(Age)
150/68: byr["client_agen"]=byr[["client_agen","device_details"]].apply(impute_age,axis=1)
150/69: byr["client_agen"]=byr[["client_agent","device_details"]].apply(impute_age,axis=1)
150/70: byr["client_agent"]=byr[["client_agent","device_details"]].apply(impute_age,axis=1)
150/71:
def impute_age(cols):
    client_agent=cols[0]
    device_details=cols[1]
    if pd.isnull(client_agent):
        if device_details == "Other - Other":
            return "other"
        elif device_details == "Unknown - MobileWeb":
            return  "unknown"
        else:
            return  "think"
    else:
        return client_agent
150/72: byr["client_agent"]=byr[["client_agent","device_details"]].apply(impute_age,axis=1)
150/73: byr["client_agent"].isnull(),sum()
150/74: byr["client_agent"].isnull().sum()
150/75: byr["client_agent"]
150/76: byr.head()
150/77: byr
150/78: byr["device_details"].str.split(' ', expand=True).stack().value_counts()
155/1:
""""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn""""""
155/2:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
155/3:
# Part 1 - Data Preprocessing

# Importing the libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
155/4:
# Importing the dataset
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13]
y = dataset.iloc[:, 13]
155/5:

dataset.head()
155/6: dataset.isnull().sum()
155/7: X.head()
155/8: y.head()
155/9:
#Create dummy variables
geography=pd.get_dummies(X["Geography"],drop_first=True)
gender=pd.get_dummies(X['Gender'],drop_first=True)
155/10: geography
155/11:
## Concatenate the Data Frames

X=pd.concat([X,geography,gender],axis=1)

## Drop Unnecessary columns
X=X.drop(['Geography','Gender'],axis=1)
155/12:

X.head()
155/13:
# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
155/14:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
155/15: X_train
155/16:
import tensorflow
print(tensorflow.__version__)
155/17:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
155/18:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
155/19: classifier.summary()
155/20: X_train.shape
155/21:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
155/22: model_history.history.keys()
155/23:
# list all data in history

print(model_history.history.keys())
# summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
155/24:
# summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
155/25:
# Part 3 - Making the predictions and evaluating the model

# Predicting the Test set results
y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)
155/26: y_pred
155/27:
# Making the Confusion Matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
155/28: cm
155/29:
# Calculate the Accuracy
from sklearn.metrics import accuracy_score
score=accuracy_score(y_pred,y_test)
155/30: score
155/31:
"""!pip install pandas
!pip install matplotlib
!pip install sklearn
!pip install seaborn"""
159/1: device_details=pd.get_dummies(byr,"device_details"],drop_first=True)
159/2: device_details=pd.get_dummies(byr,"device_details",drop_first=True)
159/3:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
cf.go_offline()
159/4: byr = pd.read_csv("Train.csv")
159/5: device_details=pd.get_dummies(byr,"device_details",drop_first=True)
159/6: device_details
159/7: device_details=pd.get_dummies(byr["device_details"],drop_first=True)
159/8: device_details
159/9: sns.lineplot(x="date",y="time_spent")
159/10: sns.lineplot(x="date",y="time_spent",data="byr")
159/11: sns.boxplot(x="date",y="time_spent",data="byr")
159/12: sns.histplot(x="date",y="time_spent",data="byr")
159/13: byr.iplot(x="date",y="time_spent",data="byr")
159/14: byr.iplot(x="date",y="time_spent")
159/15: byr["date"].str.split().stack().values_count()
159/16: byr["date"].str.split('',expand=True).stack().values_count()
159/17: byr["date"].str.split('',expand=True).stack.values_count()
159/18: byr["date"].str.split('',expand=True).stack.value_count()
159/19: byr["date"].str.split('',expand=True).stack().value_count()
159/20: byr["date"].split('',expand=True).stack().value_count()
159/21: byr["date"].value_count()
159/22: byr["date"]value_count()
159/23: value_count(byr["date"])
159/24: byr["date"].str.split(' ', expand=True).stack().value_counts()
159/25: byr.shape()
159/26: byr.info()
159/27:
byr['date'] = pd.to_datetime(byr['date'])

byr['day_of_week'] = byr['my_dates'].dt.day_name()
159/28:
byr['date'] = pd.to_datetime(byr['date'])

byr['day_of_week'] = byr['date'].dt.day_name()
159/29: byr['day_of_week']
159/30: byr.iplot(x="'day_of_week'",y="time_spent")
159/31: byr.iplot(x="day_of_week",y="time_spent")
159/32: byr.iplot(x="day_of_week",y="time_spent",kind = "bar")
159/33: byr.iplot(x="day_of_week",y="time_spent",kind = "line")
159/34: byr.iplot(x="day_of_week",y="time_spent",kind = "scatter")
159/35: byr.iplot(x="day_of_week",y="time_spent",kind = "hist")
159/36: byr.iplot(x="day_of_week",y="time_spent",kind ="chart")
159/37: byr.iplot(x="day_of_week",y="time_spent",kind ="  scatter")
159/38: byr.iplot(x="day_of_week",y="time_spent",kind ="scatter")
159/39: byr.iplot(x="day_of_week",y="time_spent",kind =" bar")
159/40: byr.iplot(x="day_of_week",y="time_spent",kind ="bar")
159/41: byr.iplot(x="day_of_week",y="time_spent",kind ="box")
159/42: byr.iplot(x="day_of_week",y="time_spent",kind ="spread")
159/43: byr.iplot(x="day_of_week",y="time_spent",kind ="ratio")
159/44: byr.iplot(x="day_of_week",y="time_spent",kind ="heatmap")
159/45: byr.iplot(x="day_of_week",y="time_spent",kind ="surface")
159/46: byr.iplot(x="day_of_week",y="time_spent",kind ="histogram")
159/47: byr.iplot(x="day_of_week",y="time_spent",kind ="bubble")
159/48: byr.iplot(x="day_of_week",y="time_spent",kind ="bubble3d")
159/49: byr.iplot(x="day_of_week",y="time_spent",kind ="scatter3d")
159/50: byr.iplot(x="day_of_week",y="time_spent",kind ="scattergeo")
159/51: byr.iplot(x="day_of_week",y="time_spent",kind ="ohlc")
159/52: byr.iplot(x="day_of_week",y="time_spent",kind ="candle")
159/53: byr.iplot(x="day_of_week",y="time_spent",kind ="pie")
159/54: byr.iplot(x="day_of_week",y="time_spent",kind ="choroplet")
159/55: byr.iplot(x="day_of_week",y="time_spent",kind ="bar")
159/56: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='group')
159/57: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='stack')
159/58: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay')
159/59: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay',theme ='solar')
159/60: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay',theme ='pearl')
159/61: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay',theme ='pearl')
159/62: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay',theme ='white')
159/63: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay',theme ='solar')
161/1: fruit = input("enter the fruit name and weight: ["("this format")"]":)
161/2: fruit = input("enter the fruit name and weight: ["("this format")"]")
161/3: fruit = input("enter the fruit name and weight: [(this format)]")
161/4:
fruit = input("enter the fruit name and weight: [(this format)]")
fruit.sort(key=lambda x:x[1], reverse= True)
fruit.sort(key=lambda x:x[1], reverse= False)
161/5:
def getKey(item):
    return item[1]

sorted(fruit, key=getKey)
161/6:
fruit = input("enter the fruit name and weight: [(this format)]")
def getKey(item):
    return item[1]

sorted(fruit, key=getKey)
161/7:
fruit = input("enter the fruit name and weight: [(this format)]")
def getKey(item):
    return item[1]

sorted(l, key=getKey)
161/8:
fruit = [("mango",60),("apple",30)]
def getKey(item):
    return item[1]

sorted(l, key=getKey)
161/9:
fruit = [("mango",60),("apple",30)]
def getKey(item):
    return item[1]

sorted(fruit, key=getKey)
161/10:
fruit = [("mango",60),("apple",30)]
def getKey(item):
    return item[1]

sorted(fruit, key=getKey, reverse= False)
161/11:
fruit = [("mango",60),("apple",30)]
def getKey(item):
    return item[1]

sorted(fruit, key=getKey, reverse= True)
161/12:
fruit = [("mango",60),("apple",30)]
def order(item):
    return item[1]

sorted(fruit, key=getKey, reverse= True)
sorted(fruit, key=getKey, reverse= True)
161/13:
fruit = [("mango",60),("apple",30),("bannana",10)]
def order(item):
    return item[1]

sorted(fruit, key=order, reverse= True)
sorted(fruit, key=order, reverse= True)
161/14:
fruit = [("mango",60),("apple",30),("bannana",10)]
def order(item):
    return item[1]

sorted(fruit, key=order, reverse= True)
sorted(fruit, key=order, reverse= False)
161/15:
fruit = [("mango",60),("apple",30),("bannana",10)]
def order(item):
    return item[1]

print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order, reverse= False))
161/16:
fruit = [("mango",60),("apple",30),("bannana",10)]
def order(item):
    return item[2]

print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order, reverse= False))
161/17:
fruit = [("mango",60),("apple",30),("bannana",10)]
def order(item):
    return item[0]

print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order, reverse= False))
161/18:
fruit = [("mango",60),("apple",30),("bannana",10)]
def order(item):
    return item[1]

print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order, reverse= False))
161/19:
fruit = [("mango",60),("apple",30),("bannana",10)]
def order(item):
    return item[0]

print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order, reverse= False))
161/20:
fruit = [("mango",60),("apple",30),("bannana",10),("chikoo",10)]
def order(item):
    return item[0]

print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order, reverse= False))
161/21:
fruit = [("mango",60),("apple",30),("bannana",10),("chikoo",80)]
def order(item):
    return item[0]

print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order, reverse= False))
161/22:
fruit = [("mango",60),("apple",30),("bannana",10),("chikoo",80)]
def order(item):
    return item[0]

def order(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/23:
fruit = [("mango",60),("apple",30),("bannana",10),("chikoo",80)]
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/24:
fruit = input()
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/25:
fruit = input([])
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/26:
fruit =  [("mango",60),("apple",30),("bannana",10),("chikoo",80)]
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/27:
fruit = input([()])
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/28:
fruit = input([])
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/29:
fruit = input()
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/30:
ls=[]
fruit = input(ls)
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/31:
ls=[]
fruit = input(ls)
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/32:
ls=[]
fruit = input(ls)
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/33:
ls=[]
fruit = list(input())
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/34:
ls=[]
fruit = list(input())
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/35:
ls=[]
fruit = list(tuple(input()))
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/36:
ls=[]
fruit = list(tuple(input()))
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/37:
ls=[]
fruit = [("mango",60),("apple",30),("bannana",10),("chikoo",80)]
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/38: fruit[0]
161/39: item[0]
161/40:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:0:-1]
161/41:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[3:3:-]
161/42:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in tp:
    l=list(t)
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/43:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:3:-]
161/44:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:3:]
161/45:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[1:3:]
161/46:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[1:1:]
161/47:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:1:]
161/48:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:1:1]
161/49:
fruit = [("mango",60),("apple",30),("bannana",10),("chikoo",80)]
def order(item):
    return item[0]

def order1(item):
    return item[1]

print(sorted(fruit, key=order, reverse= False))
print(sorted(fruit, key=order, reverse= True))
print(sorted(fruit, key=order1, reverse= False))
161/50:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:1,1:1]
161/51:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:1:11:1]
161/52:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:1:1:1:1]
161/53:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0:1,1:1]
161/54:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0]
161/55:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0][1]
161/56:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
lst[0][3]
for i in range (0,3):
    lst[]
161/57:
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
for i in range (0,3):
    lst[i][3]=100
161/58:
lst2= []
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
for i in range (0,3):
    ls2 = lst[i][3]=100
161/59:
lst2= []
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
for i in range (0,3):
    ls2.append( lst[i][3]=100)
161/60:
lst2= []
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
for i in range (0,3):
    ls2.append( lst[i][3]=100))
161/61:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in p:
    l=list(t)
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/62:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in p:
    print(l=list(t))
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/63:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in p:
    l=list(t)
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/64:
lst2= []
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
list(lst)
161/65:
lst2= []
lst = [(10, 20, 40), (40, 50, 60), (70, 80, 90)]
list(lst)
lst[2]
161/66:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in p:
    l=list(t)
    print(l)
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/67:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in p:
    l=list(t)
    print(l[2])
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/68:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in p:
    l=list(t)
    print(l)
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/69:
p=[(10, 20, 40), (40, 50, 60), (70, 80, 90)]
res=[]
for t in p:
    l=list(t)
    l[2]=100
    tt=tuple(l)
    res.append(tt)
print(res)
161/70:
def average_tuple(nums):
result = [sum(x) / len(x) for x in zip(*nums)]
return result

nums = ((10, 10, 10, 12), (30, 45, 56, 45), (81, 80, 39, 32), (1, 2, 3, 4))
print ("Original Tuple: ")
print(nums)
print("\nAverage value of the numbers of the said tuple of tuples:\n",average_tuple(nums))

nums = ((1, 1, -5), (30, -15, 56), (81, -60, -39), (-10, 2, 3))
print ("\nOriginal Tuple: ")
print(nums)
print("\nAverage value of the numbers of the said tuple of tuples:\n",average_tuple(nums))
159/64: byr.iplot(x="day_of_week",y="purchased",kind ="bar", barmode='overlay',theme ='solar')
159/65: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay',theme ='solar')
159/66: day_of_week=pd.get_dummies(byr["day_of_week"],drop_first=True)
159/67: day_of_week
159/68: byr.iplot(x="day_of_week",y="time_spent",kind ="bar", barmode='overlay',theme ='solar')
159/69: byr.iplot(x="client_agent",y="time_spent",kind ="bar", barmode='overlay',theme ='solar')
159/70: byr
159/71: byr[day_of_week]=pd.get_dummies(byr["day_of_week"],drop_first=True)
159/72: byr["day_of_week"]=pd.get_dummies(byr["day_of_week"],drop_first=True)
159/73: byr
159/74: byr["device_details"]=pd.get_dummies(byr["device_details"],drop_first=True)
159/75: byr
159/76: pd.drop(byr["date"])
159/77: byr = byr.drop(["date","session_id","session_number","client_agent"])
159/78: byr = byr.drop(["date","session_id","session_number","client_agent"],axis=1)
159/79: byr
159/80: day_of_week
159/81: day_of_week=pd.get_dummies(byr["day_of_week"],drop_first=True)
159/82: byr
159/83: byr = byr.drop(["date","session_id","session_number","client_agent","device_details","day_of_week"],axis=1)
159/84: byr = byr.drop(["device_details","day_of_week"],axis=1)
159/85: byr
159/86: device_details=pd.get_dummies(byr["device_details"],drop_first=True)
159/87: device_details = pd.get_dummies(byr["device_details"],drop_first=True)
159/88: byr = pd.read_csv("Train.csv")
159/89: byr.head()
159/90: byr["device_details"].str.split(' ', expand=True).stack().value_counts()
159/91: device_details = pd.get_dummies(byr["device_details"],drop_first=True)
159/92: byr
159/93: byr = byr.drop(["client_agent"],axis=1)
159/94: byr =pd.concat(byr["day_of_week","device_details"])
159/95: byr =pd.concat(byr["day_of_week","device_details"],axis=1)
159/96: byr
159/97: byr =pd.concat(byr["day_of_week"],axis=1)
159/98: byr =pd.concat(byr[day_of_week],axis=1)
159/99: byr=pd.concat(byr[day_of_week],axis=1)
159/100: byr=pd.concat(byr[day_of_week,device_details],axis=1)
159/101: byr = byr.drop(["device_details"],axis=1)
159/102: byr=pd.concat(byr[day_of_week,device_details],axis=1)
159/103: byr=pd.concat([byr,day_of_week,device_details],axis=1)
159/104: byr
159/105: day_of_week
159/106:
byr['date'] = pd.to_datetime(byr['date'])

byr['day_of_week'] = byr['date'].dt.day_name()
159/107: byr['day_of_week']
159/108: day_of_week=pd.get_dummies(byr["day_of_week"],drop_first=True)
159/109: day_of_week
159/110: byr=pd.concat([byr,day_of_week,device_details],axis=1)
159/111: byr.head()
159/112: byr.info()
159/113: byr = byr.drop(["device_details"],axis=1)
159/114: byr = byr.drop(["session_id","session_number","date"],axis=1)
159/115: byr.info()
159/116: byr = byr.drop(["day_of_week"],axis=1)
159/117: byr.info()
159/118: byr
159/119: pd.byr.to_csv('buyer')
159/120: byr.to_csv('buyer')
159/121: byr["device_details"].str.split(' ', expand=True).stack().value_counts()
159/122: dataset = pd.read_csv('buyer')
159/123: dataset = pd.read_csv('buyer.csv')
159/124: buyer.info
159/125: buyer.info()
159/126: dataset = pd.read_csv('buyer.csv')
159/127: dataset.info()
159/128:
X = dataset.iloc[:, 0:42]
y = dataset.iloc[:, 43]
159/129:
X = dataset.iloc[:, 0:42]
y = dataset.iloc[:, 42]
159/130: x
159/131:
X = dataset.iloc[:, 0:42]
y = dataset.iloc[:, 42]
159/132: x
159/133: X
159/134: Y
159/135: y
159/136:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
159/137:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
159/138:
import tensorflow
print(tensorflow.__version__)
159/139:
# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
159/140:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
159/141: classifier.summary()
159/142: X_train.shape
159/143:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/144:
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,
                        validation_data=VALIDATION_SET)
159/145:
import tensorflow as tf
print(tensorflow.__version__)
159/146:
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (X_valid, y_valid)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,
                        validation_data=VALIDATION_SET)
159/147:
LAYERS = [tf.keras.layers.Flatten(input_shape=[28, 28], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (y_test,X_test)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)
159/148:
LAYERS = [tf.keras.layers.Flatten(input_shape=[74, 4], name="inputLayer"),
          tf.keras.layers.Dense(130, activation="relu", name="hiddenLayer71"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer102"),
          tf.keras.layers.Dense(100, activation="relu", name="hiddenLayer312"),
          tf.keras.layers.Dense(50, activation="softmax", name="outputLayer")]

model_clf = tf.keras.models.Sequential(LAYERS)
LOSS_FUNCTION = "sparse_categorical_crossentropy" # use => tf.losses.sparse_categorical_crossentropy
OPTIMIZER = 'adam' # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)
METRICS = ["accuracy"]

model_clf.compile(loss=LOSS_FUNCTION,optimizer=OPTIMIZER , metrics=METRICS)

EPOCHS = 80

VALIDATION_SET = (y_test,X_test)

history = model_clf.fit(X_train, y_train, epochs=EPOCHS,validation_data=VALIDATION_SET)
159/149:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = ))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
159/150:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
159/151: classifier.summary()
159/152: X_train.shape
159/153:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/154:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu'))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
159/155: classifier.summary()
159/156:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
159/157: classifier.summary()
159/158: X_train.shape
159/159:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/160:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss ='rmsprop', metrics = ['accuracy'])
159/161:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/162:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =None, metrics = ['accuracy'])
159/163:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/164:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss ='Mean Absolute Error', metrics = ['accuracy'])
159/165:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
160/1:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = "keras.losses.MeanSquaredError", metrics = ['accuracy'])
160/2:
# Part 2 - Now let's make the ANN!

# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
160/3:
# Initialising the ANN
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 11))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss = "keras.losses.MeanSquaredError", metrics = ['accuracy'])
160/4:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/166:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =classifier.compile(optimizer = 'adam', loss = "keras.losses.MeanSquaredError", metrics = ['accuracy'])', metrics = ['accuracy'])
159/167:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =classifier.compile(optimizer = 'adam', loss = "MeanSquaredError", metrics = ['accuracy'])', metrics = ['accuracy'])
159/168:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =classifier.compile(optimizer = 'adam', metrics = ['accuracy'])', metrics = ['accuracy'])
                   
keras.losses.MeanSquaredError()
159/169:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =classifier.compile(optimizer = 'adam', metrics = ['accuracy'])', metrics = ['accuracy'])
                   
loss = keras.losses.MeanSquaredError()
159/170:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =classifier.compile(optimizer = 'adam', metrics = ['accuracy']),loss =tf.keras.losses.MeanSquaredError(),metrics = ['accuracy'])
                   
loss = keras.losses.MeanSquaredError()
159/171:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =classifier.compile(optimizer = 'adam', metrics = ['accuracy']),loss =tf.keras.losses.MeanSquaredError(),metrics = ['accuracy'])
159/172:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss =classifier.compile(optimizer = 'adam', metrics = ['accuracy']),tf.keras.losses.MeanSquaredError(),metrics = ['accuracy'])
159/173:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,kernel_initializer='he_uniform',activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10, kernel_initializer = 'he_uniform',activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, kernel_initializer = 'glorot_uniform', activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
159/174: classifier.summary()
159/175:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/176:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10,activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, activation = 'sigmoid'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
159/177: classifier.summary()
159/178:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100,)
159/179:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100)
159/180:
classifier = Sequential()

# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,activation='relu',input_dim = 42))

# Adding the second hidden layer
classifier.add(Dense(units = 10,activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, activation = 'softmax'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
159/181:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.33, batch_size = 100,epochs = 100)
159/182:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.55, batch_size = 100,epochs = 100)
165/1:
# Import all needed libraries and sublibraries

import tensorflow as tf

from keras.models import Sequential
from keras.layers import Dense
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
165/2:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tf.keras.models import Sequential
from tf.keras.layers import Dense
from tf.keras.optimizers import Adam
from tf.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
165/3:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow .keras.models import Sequential
from tensorflow .keras.layers import Dense
from tensorflow .keras.optimizers import Adam
from tensorflow .keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
165/4:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer.csv')
165/5:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
165/6:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
165/7:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(3,)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
165/8:
# Runs and plots the performance of a model with the same parameters from before (and a learning rate of 10000), 
# but now with an activation function (Relu)

model = Sequential()
model.add(Dense(1, input_shape=(3,), activation = 'relu'))
model.compile(Adam(lr=10000), 'mean_squared_error')
history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)

history_dict=history.history
loss_values = history_dict['loss']
val_loss_values=history_dict['val_loss']
plt.plot(loss_values,'bo',label='training loss')
plt.plot(val_loss_values,'r',label='training loss val')
165/9:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(43,)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
165/10:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(3,)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
166/1:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(23,23)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
166/2:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow .keras.models import Sequential
from tensorflow .keras.layers import Dense
from tensorflow .keras.optimizers import Adam
from tensorflow .keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
166/3:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer.csv')
166/4:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
166/5:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
166/6:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(23,23)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
166/7:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(23,43)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
166/8:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(,43)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
168/1:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow .keras.models import Sequential
from tensorflow .keras.layers import Dense
from tensorflow .keras.optimizers import Adam
from tensorflow .keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
168/2:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer.csv')
168/3:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
168/4:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
168/5:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(0,43)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
168/6:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(None,43)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
169/1:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(None,43)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
169/2:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow .keras.models import Sequential
from tensorflow .keras.layers import Dense
from tensorflow .keras.optimizers import Adam
from tensorflow .keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
169/3:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer.csv')
169/4:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
169/5:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
169/6:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(None,43)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
169/7:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)

X_train.shape()
169/8:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)

df1.shape()
169/9:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyery.csv')
169/10:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyery.csv')
169/11:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyery.csv')
169/12:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyery.csv')
169/13:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyery.csv')
169/14:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyery1.csv')
169/15:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyery1.csv')
169/16:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer1.csv')
169/17:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\Regression-Model-YT-Video-master\buyer1.csv')
170/1:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\Regression-Model-YT-Video-master\buyer1.csv')
170/2:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow .keras.models import Sequential
from tensorflow .keras.layers import Dense
from tensorflow .keras.optimizers import Adam
from tensorflow .keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
170/3:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\Regression-Model-YT-Video-master\buyer1.csv')
170/4:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer1.csv')
170/5:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer11.csv')
170/6:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)

df1.shape()
170/7:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)

df1.shape()
170/8:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
170/9:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)

df1.shape()
170/10:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
170/11:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(None,43)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
170/12:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(None,42)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
171/1:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(1, input_shape=(1,42)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
171/2:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow .keras.models import Sequential
from tensorflow .keras.layers import Dense
from tensorflow .keras.optimizers import Adam
from tensorflow .keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
171/3:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer11.csv')
171/4:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
171/5:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
171/6:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Dense(10, input_shape=(1,42)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
172/1:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow .keras.models import Sequential
from tensorflow .keras.layers import Dense
from tensorflow .keras.layers import flatten
from tensorflow .keras.optimizers import Adam
from tensorflow .keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
172/2:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(flatten(1))
    model.add(Dense(10, input_shape=(1,42)))
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
172/3:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
172/4:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
172/5:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.models import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
172/6:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.models import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
172/7:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
172/8:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layer import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
172/9:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
173/1:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
173/2:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
173/3:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer11.csv')
173/4:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
173/5:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
173/6:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer"))
    model.add(Dense(10)
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
173/7:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=[1, 41], name="inputLayer"))
    model.add(Dense(10)
    
    #Compiles model
    model.compile(Adam(lr=i), 'mean_squared_error')
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
173/8:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer"))
    model.add(Dense(10)
    
    #Compiles model
    model.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
173/9:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
173/10:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer11.csv')
173/11:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
173/12:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
173/13:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer"))
    model.add(Dense(10)
    
    #Compiles model
    model.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
173/14:
# Plots the results of a learning rate of 100, 1000, and 10000 respectively, with all other parameters constant

LR = [100,1000,10000]

for i in LR:
    #Defines linear regression model and its structure
    model = Sequential()
    model.add(Flatten(input_shape=[1, 42], name="inputLayer"))
    model.add(Dense(10)
    
    #Compiles model
    model.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
    
    #Fits model
    history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)
    history_dict=history.history
    
    #Plots model's training cost/loss and model's validation split cost/loss
    loss_values = history_dict['loss']
    val_loss_values=history_dict['val_loss']
    plt.figure()
    plt.plot(loss_values,'bo',label='training loss')
    plt.plot(val_loss_values,'r',label='val training loss')
174/1:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
cf.go_offline()
174/2: dataset = pd.read_csv('buyer.csv')
174/3:
X = dataset.iloc[:, 0:42]
y = dataset.iloc[:, 42]
174/4:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)
174/5:
# Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
174/6:
import tensorflow as tf
print(tensorflow.__version__)
174/7:
# Importing the Keras libraries and packages
import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
174/8:
import tensorflow as tf
print(tensorflow.__version__)
174/9:
classifier = Sequential()

classifier.add(Flatten(input_shape=[1, 42], name="inputLayer")) 
# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,activation='relu'))

# Adding the second hidden layer
classifier.add(Dense(units = 10,activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, activation = 'softmax'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
174/10:
classifier = Sequential()

classifier.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,activation='relu'))

# Adding the second hidden layer
classifier.add(Dense(units = 10,activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, activation = 'softmax'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
174/11: classifier.summary()
174/12: X_train.shape
174/13:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.55, batch_size = 100,epochs = 100)
174/14:
classifier = Sequential()

classifier.add(tf.keras.layers.Flatten(input_shape=[1, 44], name="inputLayer")) 
# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,activation='relu'))

# Adding the second hidden layer
classifier.add(Dense(units = 10,activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, activation = 'softmax'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
174/15:
classifier = Sequential()

classifier.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 10,activation='relu'))

# Adding the second hidden layer
classifier.add(Dense(units = 10,activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 1, activation = 'softmax'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
174/16:
classifier = Sequential()

classifier.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
# Adding the input layer and the first hidden layer
classifier.add(Dense(units = 200,activation='relu'))

# Adding the second hidden layer
classifier.add(Dense(units = 100,activation='relu'))
# Adding the output layer
classifier.add(Dense(units = 10, activation = 'relu'))

# Compiling the ANN
classifier.compile(optimizer = 'adam', loss=tf.keras.losses.MeanSquaredError(), metrics = ['accuracy'])
174/17:
# Fitting the ANN to the Training set
model_history=classifier.fit(X_train, y_train,validation_split=0.55, batch_size = 100,epochs = 100)
173/15:
# Runs and plots the performance of a model with the same parameters from before (and a learning rate of 10000), 
# but now with an activation function (Relu)

model = Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
model.add(Dense(1, input_shape=(3,), activation = 'relu'))
model.compile(Adam(lr=10000), 'mean_squared_error')
history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)

history_dict=history.history
loss_values = history_dict['loss']
val_loss_values=history_dict['val_loss']
plt.plot(loss_values,'bo',label='training loss')
plt.plot(val_loss_values,'r',label='training loss val')
173/16:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
173/17:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer11.csv')
173/18:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
173/19:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
173/20:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
173/21:
# Runs and plots the performance of a model with the same parameters from before (and a learning rate of 10000), 
# but now with an activation function (Relu)

model = Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
model.add(Dense(1, input_shape=(3,), activation = 'relu'))
model.compile(Adam(lr=10000), 'mean_squared_error')
history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)

history_dict=history.history
loss_values = history_dict['loss']
val_loss_values=history_dict['val_loss']
plt.plot(loss_values,'bo',label='training loss')
plt.plot(val_loss_values,'r',label='training loss val')
173/22:
# Import all needed libraries and sublibraries

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import dense
import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
173/23:
# Import all needed libraries and sublibraries

import tensorflow as tf

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
173/24:
# Runs and plots the performance of a model with the same parameters from before (and a learning rate of 10000), 
# but now with an activation function (Relu)

model = Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
model.add(Dense(1, input_shape=(3,), activation = 'relu'))
model.compile(Adam(lr=10000), 'mean_squared_error')
history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)

history_dict=history.history
loss_values = history_dict['loss']
val_loss_values=history_dict['val_loss']
plt.plot(loss_values,'bo',label='training loss')
plt.plot(val_loss_values,'r',label='training loss val')
175/1:
# Runs and plots the performance of a model with the same parameters from before (and a learning rate of 10000), 
# but now with an activation function (Relu)

model = Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
model.add(Dense(1,activation = 'relu'))
model.compile(Adam(lr=10000), 'mean_squared_error')
history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)

history_dict=history.history
loss_values = history_dict['loss']
val_loss_values=history_dict['val_loss']
plt.plot(loss_values,'bo',label='training loss')
plt.plot(val_loss_values,'r',label='training loss val')
175/2:
# Import all needed libraries and sublibraries

import tensorflow as tf

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
176/1:
# Import all needed libraries and sublibraries

import tensorflow as tf

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
176/2:
# Import input (x) and output (y) data, and asign these to df1 and df1

df1 = pd.read_csv('buyer.csv')

df2 = pd.read_csv('buyer11.csv')
176/3:
# Split the data into input (x) training and testing data, and ouput (y) training and testing data, 
# with training data being 80% of the data, and testing data being the remaining 20% of the data

X_train, X_test, y_train, y_test = train_test_split(df1, df2, test_size=0.2)
176/4:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
176/5:
# Scale both training and testing input data

X_train = preprocessing.scale(X_train)

X_test = preprocessing.scale(X_test)
176/6:
# Runs and plots the performance of a model with the same parameters from before (and a learning rate of 10000), 
# but now with an activation function (Relu)

model = Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
model.add(Dense(1,activation = 'relu'))
model.compile(Adam(lr=10000), 'mean_squared_error')
history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)

history_dict=history.history
loss_values = history_dict['loss']
val_loss_values=history_dict['val_loss']
plt.plot(loss_values,'bo',label='training loss')
plt.plot(val_loss_values,'r',label='training loss val')
176/7:
# Import all needed libraries and sublibraries

import tensorflow as tf

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras import Adam
import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
176/8:
# Import all needed libraries and sublibraries

import tensorflow as tf

import tensorflow.keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout
from tensorflow.keras.optimizers import Adam
import pandas as pd

import sklearn
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from matplotlib import pyplot as plt
%matplotlib inline
%config InlineBackend.figure_format='retina'
176/9:
# Runs and plots the performance of a model with the same parameters from before (and a learning rate of 10000), 
# but now with an activation function (Relu)

model = Sequential()
model.add(tf.keras.layers.Flatten(input_shape=[1, 42], name="inputLayer")) 
model.add(Dense(1,activation = 'relu'))
model.compile(Adam(lr=10000), 'mean_squared_error')
history = model.fit(X_train, y_train, epochs = 500, validation_split = 0.1,verbose = 0)

history_dict=history.history
loss_values = history_dict['loss']
val_loss_values=history_dict['val_loss']
plt.plot(loss_values,'bo',label='training loss')
plt.plot(val_loss_values,'r',label='training loss val')
179/1:
import pandas as pd
import numpy as np
import seaborn as sns
import cufflins as cf
cf.go_offline()
179/2:
import pandas as pd
import numpy as np
import seaborn as sns
import cufflinks as cf
cf.go_offline()
179/3: dataset = sns.load_data("iris")
179/4: dataset = sns("iris")
179/5: dataset = sns.load_dataset("iris")
179/6: dataset = cf.load_dataset("iris")
179/7: dataset = sns.load_dataset("iris")
179/8:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
179/9: dataset.iplot()
179/10: dataset.iplot(kind = "bar")
179/11: dataset.iplot(kind = "box")
179/12: dataset.iplot(kind = "box",hue="species")
179/13: dataset.iplot(kind = "box")
179/14: dataset.iplot(kind = "box",x='Species',y='PetalLengthCm')
179/15: dataset.iplot(kind = "box",x='Species',y='PetalLengthCm')
179/16: dataset.head()
179/17: dataset.iplot(kind = "box",x='species',y='PetalLengthCm')
179/18: dataset.iplot(kind ="box",x='species',y='PetalLengthCm')
179/19: dataset.iplot(kind ="box",x='species',y='petal_length')
179/20: dataset.iplot(kind ="box",x='species',y='petal_length')
179/21: dataset.iplot(x='species',y='petal_length')
179/22: dataset.iplot(x='species',y='petal_length',kind ="box")
179/23: dataset.iplot(x='species',kind ="box")
179/24: dataset.iplot(x='species',kind ="box")
179/25: dataset.iplot(x='species',kind ="box")
179/26: help(dataset.iplot(x='species',kind ="box"))
179/27: help(iplot)
179/28: help(.iplot)
179/29: help(dataset.iplot)
179/30: dataset.iplot(x='species',kind ="box")
179/31: dataset.iplot(x='species',kind ="box",mode='overlay')
179/32: dataset.iplot(x='species',kind ="box",mode='overlay')
179/33: dataset.iplot(x='species',y='sepal_length',kind ="box",mode='overlay')
179/34: dataset[["species"]].iplot(x='species',y='sepal_length',kind ="box",mode='overlay',)
179/35: dataset[["species"]].iplot(x='species',y='sepal_length',kind ="box",mode='overlay',)
179/36: dataset.values_count
179/37: dataset.values_count()
179/38: dataset.values_count()
179/39: dataset.value_counts()
179/40: dataset[["species"]].iplot(x='setosa',y='sepal_length',kind ="box",mode='overlay',)
179/41: dataset.iplot(x='setosa',y='sepal_length',kind ="box",mode='overlay',)
179/42: dataset[["setosa"]].iplot(x='setosa',y='sepal_length',kind ="box",mode='overlay',)
179/43: dataset[["species"]].iplot(x='setosa',y='sepal_length',kind ="box",mode='overlay',)
179/44: dataset[["species"]].iplot(kind ="box",mode='overlay',)
179/45: dataset[["species"]].iplot((y='sepal_length'kind ="box",mode='overlay',)
179/46: dataset[["species"]].iplot((y='sepal_length'kind ="box", mode='overlay')
179/47: dataset[["species"]].iplot((y='sepal_length',kind ="box", mode='overlay')
179/48: dataset[["species"]].iplot((y='sepal_length',kind ="box")
179/49: dataset[["species"]].iplot(y='sepal_length',kind ="box")
179/50: dataset.iplot(y='sepal_length',kind ="box")
179/51: dataset.iplot(y='sepal_width',kind ="box")
179/52: dataset.iplot(y='sepal_width',kind ="box")
179/53: dataset.iplot(y='sepal_width',kind ="box",values= 'virginica' )
179/54:
dataset.iplot(y='sepal_width',kind ="box",values=
              'virginica' )
179/55: dataset.iplot(y='sepal_width',kind ="box",values='virginica' )
179/56:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='Species',y='PetalLengthCm',data=iris,order=['Iris-virginica','Iris-versicolor','Iris-setosa'],linewidth=2.5,orient='v',dodge=False)
179/57: dataset.iplot(y='sepal_width',kind ="box",values='virginica',order=['Iris-virginica','Iris-versicolor','Iris-setosa'], )
179/58: dataset.iplot(y='sepal_width',kind ="box",values='virginica',order=['Iris-virginica','Iris-versicolor','Iris-setosa'])
179/59:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='Species',y='PetalLengthCm',data=dataset,order=['Iris-virginica','Iris-versicolor','Iris-setosa'],linewidth=2.5,orient='v',dodge=False)
179/60:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='Species',y='PetalLengthCm',data=dataset,order=['dataset-virginica','Iris-versicolor','Iris-setosa'],linewidth=2.5,orient='v',dodge=False)
179/61:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='Species',y='PetalLengthCm',data=dataset,order=['dataset-virginica','dataset-versicolor','dataset-setosa'],linewidth=2.5,orient='v',dodge=False)
179/62:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='species',y='PetalLengthCm',data=dataset,order=['dataset-virginica','dataset-versicolor','dataset-setosa'],linewidth=2.5,orient='v',dodge=False)
179/63:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='species',y='petal_length',data=dataset,order=['dataset-virginica','dataset-versicolor','dataset-setosa'],linewidth=2.5,orient='v',dodge=False)
179/64:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='species',y='petal_length',data=dataset,order=['iris-virginica','dataset-versicolor','dataset-setosa'],linewidth=2.5,orient='v',dodge=False)
179/65:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='species',y='petal_length',data=dataset,order=['iris-virginica','iris-versicolor','iris-setosa'],linewidth=2.5,orient='v',dodge=False)
179/66:
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.boxplot(x='species',y='petal_length',data=dataset,linewidth=2.5,orient='v',dodge=False)
179/67: dataset.iplot(y='sepal_width',kind ="box",values='virginica',dodge=False)
179/68: dataset.iplot(x='species',y='petal_length',kind ="box",values='virginica',dodge=False)
179/69: dataset.iplot(x='species',y='petal_length',kind ="box",linewidth=2.5,orient='v')
179/70: dataset.iplot(x='species',y='petal_length',kind ="box",linewidth=2.5)
179/71: dataset.iplot(x='species',y='petal_length',kind ="box",categories='myGroup')
179/72:
box_age = dataset[['species', 'sepal_length ']]
box_age.pivot(columns='species', values='sepal_length').iplot(kind='box')
179/73:
box_age = dataset[['species', 'sepal_length']]
box_age.pivot(columns='species', values='sepal_length').iplot(kind='box')
179/74:
box_age = dataset[['species', 'sepal_length']]
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
179/75: dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
179/76: dataset.pivot(columns='species').iplot(kind='box')
179/77: dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
179/78: dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',axis=0)
179/79: dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True)
179/80:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='sepal_width  ').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='petal_length  ').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='petal_width  ').iplot(kind='box',subplots=True)
179/81:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',subplots=True)
179/82:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',subplots=True)
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',subplots=True)
179/83:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/84:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True, shape : (2,2))
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/85:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True, shape = (2,2))
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/86:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True, shape=(2,2))
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/87:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',subplots=True, shape=(2,2))
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',subplots=True, shape=(2,2))
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/88:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/89:
fig = subplots()
fig , ax = fig(2,2)
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/90:
fig = Subplots()
fig , ax = fig(2,2)
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/91:

fig , ax = fig(2,2)
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/92:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/93:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=1)
179/94:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)

fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.plot(fig, filename='subplots-shared-xaxes')
179/95:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.plot(fig, filename='subplots-shared-xaxes')
179/96:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
from plotly import tools 
import plotly.plotly as py
import plotly.graph_objs as go
179/97:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
from plotly import tools
179/98:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.plot(fig, filename='subplots-shared-xaxes')
179/99:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = plotly.subplots.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.plot(fig, filename='subplots-shared-xaxes')
179/100:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.plot(fig, filename='subplots-shared-xaxes')
179/101:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.iplot(fig, filename='subplots-shared-xaxes')
179/102:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
from plotly import tools 
import plotly.plotly as py
import plotly.graph_objs as go
179/103:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.iplot(fig, filename='subplots-shared-xaxes')
179/104:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
from plotly import tools 
import plotly as py
import plotly.graph_objs as go
179/105:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.iplot(fig, filename='subplots-shared-xaxes')
179/106:
a= dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
b=dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
c=dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
d=dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
fig = tools.make_subplots(rows=2, cols=2)
fig.add_traces(a)
fig.add_traces(b)
fig.add_traces(c)
fig.add_traces(d)
fig['layout'].update(height=600, width=600, title='PARTICLES CORRELATION')
py.plot(fig, filename='subplots-shared-xaxes')
179/107:
cf.subplots([dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')],shape=(2,1)).iplot()
179/108:
cf.subplots(dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box'),shape=(2,1)).iplot()
179/109: dataset.value_counts()
179/110:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box')
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/111:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',ytitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/112:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',y_title="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/113:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',Ytitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/114:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box')
dataset.pivot(columns='species', values='petal_length').iplot(kind='box')
dataset.pivot(columns='species', values='petal_width').iplot(kind='box')
179/115:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
179/116:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
179/117:
dataset.pivot(columns='species', values='sepal_length','sepal_width').iplot(kind='box',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
179/118:
dataset.pivot(columns='species', values='sepal_length''sepal_width').iplot(kind='box',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
179/119:
dataset.pivot(columns='species', values=['sepal_length','sepal_width']).iplot(kind='box',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
179/120:
dataset.pivot(columns='species', values='sepal_length','sepal_width').iplot(kind='box',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
179/121:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='box',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
183/1:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from collections import Counter
from IPython.core.display import display, HTML
sns.set_style('darkgrid')
183/2:
from sklearn.datasets import load_boston
boston_dataset = load_boston()
dataset = pd.DataFrame(boston_dataset.data, columns = boston_dataset.feature_names)
183/3: dataset.head()
183/4: dataset['MEDV'] = boston_dataset.target
183/5: dataset.head()
183/6: dataset.isnull().sum()
183/7: a = pd.read_csv('housing.csv')
183/8: a = pd.read_csv('HousingData.csv)
183/9: a = pd.read_csv('HousingData.csv')
183/10: a = pd.read_csv(r'C:\Users\Admin\Desktop\mtech ml\boston housing\HousingData.csv')
183/11: a.isnull().sum()
183/12: dataset.iplot()
183/13:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import cross_val_score
from collections import Counter
from IPython.core.display import display, HTML
sns.set_style('darkgrid')
import cufflinks as cf
cf.go_offline()
183/14: dataset.iplot()
183/15: dataset.iplot(kind='box')
183/16: dataset.describe()
184/1:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='scatter',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/2:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
184/3: dataset = sns.load_dataset("iris")
184/4:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='scatter',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/5:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='line',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/6:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='spread',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/7:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='bar',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/8:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='ratio',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/9:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='heatmap',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/10:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='pie',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/11:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='histogram',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='box',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='box',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='box',yTitle="petal_width")
184/12:
dataset.pivot(columns='species', values='sepal_length').iplot(kind='histogram',yTitle="sepal_length")
dataset.pivot(columns='species', values='sepal_width').iplot(kind='histogram',yTitle="sepal_width")
dataset.pivot(columns='species', values='petal_length').iplot(kind='histogram',yTitle="petal_length")
dataset.pivot(columns='species', values='petal_width').iplot(kind='histogram',yTitle="petal_width")
183/17: dataset.corr()
183/18: dataset.corr().iplot()
183/19: dataset.corr().iplot(kind="heatmap")
186/1:
def impute_cabin(cols):
    Deck=cols[0]
    fare=cols[1]
    if pd.isnull(Deck):
        if fare == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return cabin
186/2:
sns.boxplot(x="Deck",y='fare',data=cabin_only, palette ='winter')
Deck.isnull()
186/3:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
186/4:
#Import necessary Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

import seaborn as sns

%matplotlib inline
186/5: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\titanic ml\train.csv")
186/6: tnc = pd.read_csv(r"C:\Users\Admin\Desktop\mtech ml\titanic ml\train.csv")
187/1:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
187/2:
#Data 
book = open('Deathnote-script.txt', 'r',encoding='utf-8').read()

#Remove Stop Words
book_stopwords = set(STOPWORDS)
187/3:
# instantiate a word cloud object
book_wc = WordCloud(background_color='white', max_words=5000, stopwords=book_stopwords)

# generate the word cloud
book_wc.generate(book)
187/4:
# display the word cloud

plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/5:
# Resize the cloud

fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height

# display the cloud
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/6:
# Update Stopwords
stp_words=['man','said','one','will']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/7:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/8:
# Update Stopwords
stp_words=['man','said','one','will','name']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/9:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/10:
# Update Stopwords
stp_words=['man','said','one','will','name','want']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/11:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/12:
# Update Stopwords
stp_words=['man','said','one','will','name','want','going']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/13:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/14:
# Update Stopwords
stp_words=['man','said','one','will','name','want','going','know']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/15:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/16:
# Update Stopwords
stp_words=['man','said','one','will','name','want','going','know','people']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/17:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/18:
# Update Stopwords
stp_words=['man','said','one','will','name','want','going','know','people','see']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/19:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/20:
# Update Stopwords
stp_words=['man','said','one','will','name','want','going','know','people','see','even']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/21:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/22:
# Update Stopwords
stp_words=['man','said','one','will','name','want','going','know','people','see','even','yeah']
[book_stopwords.add(n) for n in stp_words]   # add the words said to stopwords
187/23:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
187/24:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
import cufflinks as cf
cf.go_offline()
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
187/25:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
book_wc.iplot()
187/26:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
import cufflinks as cf
cf.go_offline()
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
187/27:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
import cufflinks as cf
cf.go_offline()
import seaborn
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
187/28:
#Importing required librarires

import numpy as np  # useful for many scientific computing in Python
import pandas as pd # primary data structure library
from PIL import Image # converting images into arrays
import matplotlib.pyplot as plt # for visualizing the data
from wordcloud import WordCloud, STOPWORDS
import cufflinks as cf
cf.go_offline()
import seaborn as sns
# save mask for book
book_mask = np.array(Image.open('sherlock_mask.jpg'))
187/29:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
book_wc.sns()
187/30:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
book.sns()
187/31:
# re-generate the word cloud
book_wc.generate(book)


# display the cloud
fig = plt.figure()
fig.set_figwidth(14) # set width
fig.set_figheight(18) # set height
plt.imshow(book_wc, interpolation='bilinear')
plt.axis('off')
plt.show()
book.iplot()
189/1:
# load dataset
x = pd.read_csv('../input/train.csv')

x.head()
191/1: tnc = pd.read_csv("train.csv")
191/2:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
191/3: tnc = pd.read_csv("train.csv")
191/4: tnc.head()
191/5: tnc.drop("Name")
191/6: tnc.drop("Name",axis=1)
191/7: tnc
191/8: tnc = tnc.drop("Name",axis=1)
191/9: tnc
191/10: tnc = tnc.drop("Name","Ticket",axis=1)
191/11: tnc = tnc.drop("Name""Ticket",axis=1)
191/12: tnc = tnc.drop( columns="Name","Ticket",axis=1)
191/13: tnc = tnc.drop(["Name","Ticket"],axis=1)
191/14: tnc = tnc.drop(["Ticket"],axis=1)
191/15: tnc
191/16: tnc.head()
191/17: tnc
191/18: tnc = pd.read_csv("train.csv")
191/19: tnc.head()
191/20: tnc.corr()
191/21: tnc = tnc.drop(["Name","Ticket"],axis=1)
191/22: tnc.iplot()
191/23: tnc.iplot(kind="scatter",modde='markers')
191/24: tnc.iplot(kind="scatter",mode='markers')
191/25: tnc.iplot(kind="scatter",mode=' lines+markers+text')
191/26: tnc.iplot(kind="scatter",mode='markers+text')
191/27: tnc = tnc.drop(["Name","Ticket",'PassengerId'],axis=1)
191/28: tnc = pd.read_csv("train.csv")
191/29: tnc.head()
191/30: tnc.corr()
191/31: tnc = tnc.drop(["Name","Ticket",'PassengerId'],axis=1)
191/32: tnc
191/33: tnc.isnull().sum()
191/34: tnc.iplot(kind="box",mode='markers+text')
191/35: tnc.iplot(kind="scatter",mode=' markers+text')
191/36: tnc.iplot(x='Survived',y='Age',"kind="box",mode=' markers+text')
191/37: tnc.iplot(x='Survived',y='Age',kind="box",mode=' markers+text')
191/38: tnc.iplot(x='Survived',y='Age',kind="box")
191/39: tnc.iplot(x='Survived',y='Age',kind="bar")
191/40: tnc.iplot(x='Survived',y='Age',kind="bar",barmode="group")
191/41: tnc.iplot(x='Survived',y='Age',kind="bar",barmode="stack")
191/42: tnc.iplot(x='Survived',y='Age',kind="bar",barmode="overlay")
191/43: tnc.iplot(x='Survived',y='Fare',kind="bar",barmode="overlay")
191/44: tnc.iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/45: tnc.iplot(x='Pclass',y='age',kind="bar",barmode="overlay")
191/46: tnc.iplot(x='Pclass',y='Age',kind="bar",barmode="overlay")
191/47: tnc.iplot(x='Pclass',y='Age',kind="box",barmode="overlay")
191/48: tnc.iplot(x='Pclass',y='Age',kind="bar",barmode="overlay")
191/49: tnc.iplot(x='Pclass',y='Age',kind="box")
191/50: tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="outliers")
191/51: tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="all")
191/52: tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
191/53: tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="False")
191/54: tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
191/55: tnc.iplot(x='Pclass',y='Age',kind="box",barmode="overlay")
191/56: tnc.iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/57: tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
191/58: tnc.isnull().sum().iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/59: tnc.isnull().iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/60: tnc.isnull().iplot()
191/61: tnc.isnull().sum().iplot()
191/62: tnc.isnull().sum().iplot(kind="markers")
191/63: tnc.isnull().sum().iplot(kind="bar")
191/64: tnc.isnull().sum().iplot(kind="hist")
191/65: tnc.isnull().sum().iplot(kind="line")
191/66: tnc.isnull().sum().iplot(kind="box")
191/67: tnc.isnull().sum().iplot(kind="line",mode="markers")
191/68: tnc.isnull().sum().iplot(kind="line",mode="markers+text")
191/69: tnc.isnull().sum().iplot(kind="line",mode="markers")
191/70: tnc.isnull().sum().iplot(kind="line",mode="markers+line")
191/71: tnc.isnull().sum().iplot(kind="line",mode="line+markers")
191/72: tnc.isnull().sum().iplot(kind="line",mode="lines+markers")
191/73: tnc.iplot(x='Survived',y='Pclass',kind="hist",barmode="overlay")
191/74: tnc.iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/75: tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
191/76:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/77: tnc.shape()
191/78: tnc = pd.read_csv("train.csv")
191/79: tnc.shape()
191/80: tnc.shape
191/81: tnc.describe()
191/82: tnc.info()
191/83: tnc.head()
191/84: tnc.describe()
191/85: tnc.isnull()
191/86: tnc.isnull().sum()
191/87: train["survived"].values_count()
191/88: tnc["survived"].values_count()
191/89: tnc["survived"].values_counts()
191/90: tnc["Survived"].values_counts()
191/91: tnc["Survived"].value_counts()
191/92:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Survived',y='Sex',kind="bar",barmode="overlay")
191/93:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/94:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Survived',y='Sex',kind="bar",barmode="overlay")
191/95:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Survived',y='Pclass',kind="bar",barmode="overlay")
191/96:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Pclass',y='Age',kind="bar",barmode="overlay")
191/97: tnc['survived'][tnc['sex']==male].value_counts()
191/98: tnc['Survived'][tnc['sex']==male].value_counts()
191/99: tnc['Survived'][tnc['Sex']==male].value_counts()
191/100: tnc['Survived'][tnc['Sex']== 'male'].value_counts()
191/101: tnc['Survived'][tnc['Sex']== 'male'].value_counts().iplot(kind="line",mode="lines+markers")
191/102: tnc['Survived'][tnc['Sex']== 'male'].value_counts().iplot()
191/103: tnc['Survived'][tnc['Sex']== 'male'].value_counts().iplot(kind="bar")
191/104: tnc['Survived'][tnc['Sex']== 'male'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='male count')
191/105: tnc['Survived'][tnc['Sex']== 'male'].value_counts()
191/106: tnc['Survived'][tnc['Sex']=="female"]
191/107: tnc['Survived'][tnc['Sex']=="female"].value.counts(normalize=True)
191/108: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True)
191/109: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='male count')
191/110: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
191/111: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="scatter",mode='markers',xTitle='survied',yTitle='female percent')
191/112: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="box",,xTitle='survied',yTitle='female percent')
191/113: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="box",xTitle='survied',yTitle='female percent')
191/114: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="hist",xTitle='survied',yTitle='female percent')
191/115: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="pie",xTitle='survied',yTitle='female percent')
191/116: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
193/1: f = f.open("new",w+)
193/2: f = open("new","w+"")
193/3: f = open("new","w+")
193/4: f="anans"
193/5: f.read()
193/6: f = open("new","w+")
193/7: f.write("hello world")
193/8: f.read()
193/9: f = open("new","r")
193/10: f.read()
193/11: f.write("hello world /n",'2nd line')
193/12: f.write("hello world /n 2nd line")
193/13: f = open("new","w+")
193/14: f.write("hello world /n 2nd line")
193/15: f = open("new","r")
193/16: f.read()
193/17: f.write("hello world \n 2nd line")
193/18: f = open("new","w+")
193/19: f.write("hello world \n 2nd line")
193/20: f = open("new","r")
193/21: f.read()
193/22:
f.write("hello world \n 2nd line")
f.write("hello world \n 2nd line")
193/23: f = open("new","w+")
193/24:
f.write("hello world \n 2nd line")
f.write("hello world \n 2nd line")
193/25: f = open("new","r")
193/26: f.read()
193/27: print(f.readline(1))
193/28: print(f.readline(0))
193/29: f = open("new","r")
193/30: f.read()
193/31: print(f.readline(0))
193/32: f = open("new","w")
193/33:
f.write("hello world\n 2nd line")
f.write("hello world\n 2nd line")
193/34: f = open("new","r")
193/35: f.read()
193/36: print(f.readline(0))
193/37: print(f.readline())
193/38: s =(f.readline())
193/39:
s =(f.readline())
s
193/40: f = open("new","r")
193/41:
s =(f.readline())
s
193/42:
f.read()
s =(f.readline())
print(s[len(s)-1])
193/43:
f.read()
s =(f.readline())
print(s[len(s)-1])
193/44:
f.read()
s =(f.readline())
print(s[len(s)-1])
f.close()
193/45:
f.read()
s =f.readline()
print(s[len(s)-1])
f.close()
193/46: f = open("new","r")
193/47:
f.read()
s =f.readline()
print(s[len(s)-1])
f.close()
193/48:
f = open("new","w")
f.write("hello world\n 2nd line")
file.append("Test")
f.write("hello world\n 2nd line")
193/49:
f = open("new","w")
f.write("hello world\n 2nd line")
f.append("Test")
f.write("hello world\n 2nd line")
193/50: import os
193/51:
f = open("new","w")
f.write("hello world\n 2nd line")
f.append("Test")
f.write("hello world\n 2nd line")
193/52:
f = open("new","w")
f.write("hello world\n 2nd line")
f.append("new")
f.write("hello world\n 2nd line")
193/53:
f = open("new","w")
f.write("hello world\n 2nd line")
f.append("new")
f.write("hello world\n 2nd line")
193/54:
f = open("new","w")
f.write("hello world\n 2nd line")
f.write("hello world\n 2nd line")
193/55: f = open("new","r")
193/56:
f.read()
s =f.readline()
print(s[len(s)-1])
f.close()
193/57:
f = open("new","w+")
f.write("hello world\n 2nd line")
f.write("hello world\n 2nd line")
193/58:
f = open("new","r")
f.read()
s =f.readline()
print(s[len(s)-1])
f.close()
193/59:
f = open("new","w+")
f.write("""hello world\n 2nd line
        zjcnckbcbj
        mcxmnmxnx""")
f.write("hello world\n 2nd line")
193/60:
f = open("new","r")
f.read()
s =f.readline()
print(s[len(s)-1])
f.close()
193/61:
f = open("new","w+")
f.write("""hello world\n 2nd line
        zjcnckbcbj
        mcxmnmxnx""")
193/62:
f = open("new","r")
f.read()
s =f.readline()
print(s[len(s)-1])
f.close()
193/63:
f = open("new","r")
f.read()
s =f.readline()
print(s[len(s)-1])
f.close()
193/64:
f = open("new","r")
f.read()
s =f.readline()
print(s[0])
f.close()
193/65:
f = open("new","r")
f.read()
s =f.readlines()
print(s[0])
f.close()
193/66:
f = open("new","r")
f.read()
s =f.readlines()
print(s[0])
f.close()
193/67:
f = open("new","r")
f.read()
s =f.readlines()
print(s[1])
f.close()
193/68:
f = open("new","r")
f.read()
s =f.readlines()
f.close()
193/69:
f = open("new","r")
f.read()
s =f.readlines()
s
f.close()
193/70:
f = open("new","r")
f.read()
s =f.readlines()
print(s)
f.close()
193/71: f.close()
193/72:
f = open("new","r")
f.read()
s =f.readlines()
print(s)
193/73:
f = open("new","w+")
f.write("""hello world\n 2nd line
        zjcnckbcbj
        mcxmnmxnx""")
193/74:
f = open("new","r")
f.read()
s =f.readlines()
print(s)
194/1:
import cv2

cv2.namedWindow("preview")
vc = cv2.VideoCapture(0)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
194/2:
import cv2

cv2.namedWindow("preview")
vc = cv2.VideoCapture(0)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
194/3:
import cv2

cv2.namedWindow("preview")
vc = cv2.VideoCapture(192.168.1.10:554)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
194/4:
import cv2

cv2.namedWindow("preview")
url=http://192.168.1.10:554/
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
194/5:
import cv2

cv2.namedWindow("preview")
url="http://192.168.1.10:554/"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
195/1:
import cv2

cv2.namedWindow("preview")
url="rtsp://mahesh:shivaraj@123@192.168.1.10/554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
195/2:
import cv2

cv2.namedWindow("preview")
url="rtsp://mahesh:shivaraj@123@192.168.1.10/554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
195/3:
import cv2

cv2.namedWindow("preview")
url="rtsp://mahesh:shivaraj@123@192.168.1.10/554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
196/1:
import cv2

cv2.namedWindow("preview")
url="rtsp://mahesh:shivaraj@123@192.168.1.10/554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
196/2:
import cv2

cv2.namedWindow("preview")
url="rtsp://mahesh:shivaraj@123@192.168.1.10/554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
196/3:
import cv2

cv2.namedWindow("preview")
url="rtsp://mahesh:shivaraj@123@192.168.1.10/554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
196/4:
import cv2

cv2.namedWindow("preview")
url="rtsp://shivatajmahesh11@gmail.com:shivaraj@123@123.134.213.115:554/cam/realmonitor?channel=1&subtype=0"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
196/5:
import cv2

cv2.namedWindow("preview")
url="rtsp://shivarajmahesh11@gmail.com:[shivaraj@123]@123.134.213.115:554/cam/realmonitor?channel=1&subtype=0"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
198/1:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://shivarajmahesh11@gmail.com:[shivaraj@123]@123.134.213.115:554/cam/realmonitor?channel=1&subtype=0"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
198/2:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://123.134.213.115:554/cam/realmonitor?channel=1&subtype=0"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
198/3:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://shivarajmahesh11@gmail.com:shivaraj@123@192.168.1.10:554/cam/realmonitor?channel=1&subtype=1"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
198/4:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://shivarajmahesh11@gmail.com:shivaraj@123@192.168.1.10:554/cam/realmonitor?channel=1&subtype=1"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
198/5:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://192.168.1.10:554/cam/realmonitor?channel=1&subtype=1"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
198/6:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://192.168.1.10:554/cam/realmonitor?channel=1&subtype=1"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
198/7: !pip install vlc
198/8: !conda install vlc
198/9: !pip install python-vlc
198/10:
import vlc
player=vlc.MediaPlayer('rtsp://shivarajmahesh11@gmail.com:shivaraj@123@192.168.1.10:554/cam/realmonitor?channel=1&subtype=1')
player.play()
198/11:
import vlc
player=vlc.MediaPlayer('rtsp://shivarajmahesh11@gmail.com:shivaraj@123@192.168.1.10:554/cam/realmonitor?channel=1&subtype=1')
player.play()
199/1: len(rtsp://shivarajmahesh11@gmail.com:shivaraj@123@192.168.1.10:554/cam/realmonitor?channel=1&subtype=1)
199/2: len('rtsp://shivarajmahesh11@gmail.com:shivaraj@123@192.168.1.10:554/cam/realmonitor?channel=1&subtype=1')
199/3:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://admin:@192.168.1.10:554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
199/4:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="ffplay rtsp://admin:@192.168.1.10:554"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
199/5:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://shivarajmahesh11@gmail.com:Dataframe_123@192.168.1.10/cam/realmonitor?channel=1&subtype=00&authbasic=YWRtaW46YWRtaW4="
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
199/6:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://shivarajmahesh11@gmail.com:Dataframe_123@192.168.1.10:544/cam/realmonitor?channel=1&subtype=00&authbasic=YWRtaW46YWRtaW4="
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
199/7:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://shivarajmahesh11@gmail.com:Dataframe_123@192.168.1.10:544/cam/realmonitor?channel=1&subtype=1 "
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
199/8:
import vlc
player=vlc.MediaPlayer('rtsp://shivarajmahesh11@gmail.com:Dataframe_123@192.168.1.10:544/cam/realmonitor?channel=1&subtype=1')
player.play()
200/1:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
200/2: !nvidia-smi
200/3: !nvidia-smi
200/4:
import tensorflow as tf
tf.config.gpu.set_per_process_memory_fraction(0.75)
tf.config.gpu.set_per_process_memory_growth(True)
200/5:
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
201/1: tnc['Pclass']tnc['Age'].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/2: tnc['Pclass'][tnc['Age']].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/3:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
201/4: tnc = pd.read_csv("train.csv")
201/5: tnc.shape
201/6: tnc.head()
201/7: tnc.info()
201/8: tnc.describe()
201/9: tnc.corr()
201/10: tnc = tnc.drop(["Name","Ticket",'PassengerId'],axis=1)
201/11: tnc
201/12: tnc.iplot(kind="scatter",mode=' markers+text')
201/13:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Pclass',y='Age',kind="bar",barmode="overlay")
201/14: tnc.isnull().sum()
201/15: tnc.isnull().sum().iplot(kind="line",mode="lines+markers")
201/16: tnc["Survived"].value_counts()
201/17: tnc['Survived'][tnc['Sex']== 'male'].value_counts()
201/18: tnc['Survived'][tnc['Sex']== 'male'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='male count')
201/19: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True)
201/20: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/21: tnc['Pclass'][tnc['Age']].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/22: tnc['Pclass']tnc['Age'].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/23: tnc['Pclass']tnc['Age'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/24: tnc['Pclass'][tnc['Age']].value_counts().iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/25: tnc['Pclass','Age'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/26: tnc['Pclass','Age'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/27: tnc['Pclass','Age'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/28: tnc['Pclass','Age'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/29: tnc['Pclass','Age'].iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/30: tnc.pivot('Pclass':Age).iplot(kind='box')
201/31: tnc.pivot('Pclass':'Age').iplot(kind='box')
201/32: tnc.pivot(coloumns='Pclass':'Age').iplot(kind='box')
201/33: tnc.pivot(coloumns='Pclass',values='Age').iplot(kind='box')
201/34: tnc.pivot(columns='Pclass',values='Age').iplot(kind='box')
201/35:
def impute_age(cols):
    age = cols[0]
    pclass =cols[1]
    if tnc.isnull('Age'):
        if Pclass==1:
            return 37
        if Pclass==2:
            return 29
        if Pcass==3:
            return 24
        else:
return Age
201/36:
def impute_age(cols):
    age = cols[0]
    pclass =cols[1]
    if tnc.isnull('Age'):
        if Pclass==1:
            return 37
        if Pclass==2:
            return 29
        if Pcass==3:
            return 24
        else:
            return Age
201/37: tnc.isnull('Age'
201/38: tnc.isnull('Age')
201/39: pd.isnull('Age')
201/40: pd.isnull(Age)
201/41:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
201/42: tnc['Age']=tnc[["Age",'Pclass']].apply(impute_age,axis=1)
201/43: tnc.isnull().iplot()
201/44: tnc.isnull()
201/45: tnc.isnull().values_count()
201/46: tnc.isnull().value_counts()
201/47: tnc.isnull().value_counts().iplot()
201/48: tnc.isnull().value_counts().iplot(kind="heatmap")
201/49: tnc.isnull().value_counts().iplot(kind="Heatmap")
201/50: tnc.isnull().iplot(kind="Heatmap")
201/51:
tnc.isnull().iplot(kind="Heatmap")
tnc.iplot()
201/52: tnc.isnull().iplot(kind="Heatmap")
201/53: tnc.isnull().iplot(kind="heatmap")
201/54: tnc.isnull().iplot(kind="heatmap",data=[Heatmap(z=z,x=x,y=y,zmin=zmin,zmax=zmax,colorscale=colorscale)])
201/55: tnc.isnull().iplot(kind="heatmap",data=[tnc(z=z,x=x,y=y,zmin=zmin,zmax=zmax,colorscale=colorscale)])
201/56: tnc.isnull().iplot(kind="heatmap",data=tnc
201/57: tnc.isnull().iplot(kind="heatmap",data=tnc)
201/58: tnc['Age'].isnull().iplot(kind="heatmap",data=tnc)
201/59: tnc['Age'].isnull().iplot(kind="heatmap")
201/60:
survived_sex = df[df['Survived']==1]['Sex'].value_counts()
dead_sex = df[df['Survived']==0]['Sex'].value_counts()
df1 = pd.DataFrame([survived_sex,dead_sex])
df1.index = ['Survived','Dead']
df1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
201/61:
survived_sex = tnc[tnc['Survived']==1]['Sex'].value_counts()
dead_sex = tnc[tnc['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
201/62: tnc["age"].isnull()
201/63: tnc["Age"].isnull()
201/64: tnc["Age"].isnull().value_counts()
201/65: tnc["Age"].isnull().sum()
201/66: Age=pd.getdummies(tnc['Age'],axis=1.drop=True)
201/67: Age=pd.getdummies(tnc['Age'],axis=1.drop_first=True)
201/68: Age=pd.get_dummies(tnc['Age'],axis=1.drop_first=True)
201/69: Age=pd.get_dummies(data=tnc['Age'],axis=1.drop_first=True)
201/70: Age=pd.get_dummies(data=tnc['Age'],axis=1,drop_first=True)
201/71: Age=pd.get_dummies(data=tnc['Age'],drop_first=True)
201/72: Age
201/73: Sex=pd.get_dummies(data=tnc['Sex'],drop_first=True)
201/74: Sex
201/75: tnc=tnc[Sex]
201/76: tnc.head()
201/77: tnc
201/78: tnc=tnc['Sex']
201/79: tnc
201/80: Sex
201/81: tnc = pd.read_csv("train.csv")
201/82: tnc.shape
201/83: tnc.head()
201/84: tnc = tnc.drop(["Name","Ticket",'PassengerId'],axis=1)
201/85:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
201/86: tnc['Age']=tnc[["Age",'Pclass']].apply(impute_age,axis=1)
201/87: Sex=pd.get_dummies(data=tnc['Sex'],drop_first=True)
201/88: Sex
201/89: tnc
201/90: pd.drop(tnc['sex'])
201/91: tnc=tnc.drop(['sex'])
201/92: tnc=tnc.drop('sex')
201/93: tnc=tnc.drop('sex',axis=1)
201/94: tnc=tnc.drop('Sex',axis=1)
201/95: tnc
201/96: tnc=tnc.concat(Sex)
201/97: tnc=pd.concat(Sex,axis=1)
201/98: tnc=pd.concat([Sex],axis=1)
201/99: tnc
201/100: tnc=pd.concat([tnc,Sex],axis=1)
201/101: tnc
201/102:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
201/103: tnc = pd.read_csv("train.csv")
201/104: tnc.shape
201/105: tnc.head()
201/106: tnc.info()
201/107: tnc.describe()
201/108: tnc.corr()
201/109: tnc = tnc.drop(["Name","Ticket",'PassengerId'],axis=1)
201/110: tnc
201/111: tnc.iplot(kind="scatter",mode=' markers+text')
201/112:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Pclass',y='Age',kind="bar",barmode="overlay")
201/113: tnc.isnull().sum()
201/114: tnc.isnull().sum().iplot(kind="line",mode="lines+markers")
201/115: tnc["Survived"].value_counts()
201/116: tnc['Survived'][tnc['Sex']== 'male'].value_counts()
201/117: tnc['Survived'][tnc['Sex']== 'male'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='male count')
201/118: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True)
201/119: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/120:
survived_sex = tnc[tnc['Survived']==1]['Sex'].value_counts()
dead_sex = tnc[tnc['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
201/121: tnc.pivot(columns='Pclass',values='Age').iplot(kind='box')
201/122:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
201/123: tnc['Age']=tnc[["Age",'Pclass']].apply(impute_age,axis=1)
201/124: tnc["Age"].isnull().sum()
201/125: Sex=pd.get_dummies(data=tnc['Sex'],drop_first=True)
201/126: Sex
201/127: tnc=tnc.drop('Sex',axis=1)
201/128: tnc
201/129: tnc=pd.concat([tnc,Sex],axis=1)
201/130: tnc
201/131:
tnc = tnc.dropna(subset=['Embarked'])

embarked_one_hot = pd.get_dummies(train['Embarked'], prefix='Embarked')
tnc = pd.concat([train, embarked_one_hot], axis=1)
tnc.head()
201/132:
tnc = tnc.dropna(subset=['Embarked'])
code = pd.get_dummies(train['Embarked'], prefix='Embarked')
tnc = pd.concat([tnc, code], axis=1)
tnc.head()
201/133:
tnc = tnc.dropna(subset=['Embarked'])
code = pd.get_dummies(tnc['Embarked'], prefix='Embarked')
tnc = pd.concat([tnc, code], axis=1)
tnc.head()
201/134:
tnc['Cabin'] = tnc['Cabin'].apply(lambda x: x[0])
code = pd.get_dummies(tnc['Cabin'], prefix='Cabin')
tnc = pd.concat([train, code], axis=1)
tnc.columns
201/135:
tnc['Cabin'] = tnc['Cabin'].fillna('U')
tnc['Cabin'] = tnc['Cabin'].apply(lambda x: x[0])
code = pd.get_dummies(tnc['Cabin'], prefix='Cabin')
tnc = pd.concat([train, code], axis=1)
tnc.columns
201/136:
tnc['Cabin'] = tnc['Cabin'].fillna('U')
tnc['Cabin'] = tnc['Cabin'].apply(lambda x: x[0])
code = pd.get_dummies(tnc['Cabin'], prefix='Cabin')
tnc = pd.concat([tnc, code], axis=1)
tnc.columns
201/137:
tnc['Cabin'] = tnc['Cabin'].fillna('U')
tnc['Cabin'] = tnc['Cabin'].apply(lambda x: x[0])
code = pd.get_dummies(tnc['Cabin'], prefix='Cabin')
tnc = pd.concat([tnc, code], axis=1)
tnc.head()
201/138: tnc = tnc.drop(subset=['Cabin'])
201/139: tnc = tnc.drop(['Cabin'])
201/140: tnc = tnc.drop(['Cabin'],axis=1)
201/141: tnc.head()
201/142: tnc = tnc.drop(['Embarked'],axis=1)
201/143: tnc.head()
201/144:
survived_sex = tnc[tnc['Survived']==1]['Sex']['Age'].value_counts()
dead_sex = tnc[tnc['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
201/145:
survived_sex = tnc[tnc['Survived']==1]['Sex'].value_counts()
dead_sex = tnc[tnc['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
201/146:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
201/147: tnc = pd.read_csv("train.csv")
201/148: tnc.shape
201/149: tnc.head()
201/150: tnc.info()
201/151: tnc.describe()
201/152: tnc.corr()
201/153: tnc = tnc.drop(["Name","Ticket",'PassengerId'],axis=1)
201/154: tnc
201/155: tnc.iplot(kind="scatter",mode=' markers+text')
201/156:
tnc.iplot(x='Pclass',y='Age',kind="box",boxpoints="suspectedoutliers")
tnc.iplot(x='Pclass',y='Age',kind="bar",barmode="overlay")
201/157: tnc.isnull().sum()
201/158: tnc.isnull().sum().iplot(kind="line",mode="lines+markers")
201/159: tnc["Survived"].value_counts()
201/160: tnc['Survived'][tnc['Sex']== 'male'].value_counts()
201/161: tnc['Survived'][tnc['Sex']== 'male'].value_counts().iplot(kind="bar",xTitle='survied',yTitle='male count')
201/162: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True)
201/163: tnc['Survived'][tnc['Sex']=="female"].value_counts(normalize=True).iplot(kind="bar",xTitle='survied',yTitle='female percent')
201/164:
survived_sex = tnc[tnc['Survived']==1]['Sex'].value_counts()
dead_sex = tnc[tnc['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
201/165: tnc.pivot(columns='Pclass',values='Age').iplot(kind='box')
201/166:
def impute_age(cols):
    Age=cols[0]
    Pclass=cols[1]
    if pd.isnull(Age):
        if Pclass == 1:
            return 37
        elif Pclass == 2:
            return 29
        else:
            return 24
    else:
        return Age
201/167: tnc['Age']=tnc[["Age",'Pclass']].apply(impute_age,axis=1)
201/168: tnc["Age"].isnull().sum()
201/169: Sex=pd.get_dummies(data=tnc['Sex'],drop_first=True)
201/170: Sex
201/171: tnc=tnc.drop('Sex',axis=1)
201/172: tnc
201/173: tnc=pd.concat([tnc,Sex],axis=1)
201/174: tnc
201/175:
tnc = tnc.dropna(subset=['Embarked'])
code = pd.get_dummies(tnc['Embarked'], prefix='Embarked')
tnc = pd.concat([tnc, code], axis=1)
tnc.head()
201/176:
tnc['Cabin'] = tnc['Cabin'].fillna('U')
tnc['Cabin'] = tnc['Cabin'].apply(lambda x: x[0])
code = pd.get_dummies(tnc['Cabin'], prefix='Cabin')
tnc = pd.concat([tnc, code], axis=1)
tnc.head()
201/177: tnc = tnc.drop(['Cabin'],axis=1)
201/178: tnc = tnc.drop(['Embarked'],axis=1)
201/179: tnc.head()
205/1: http://localhost:8888/edit/Desktop/mtech%20ml/new%20house%20prediction/House_Price_Data.csv
205/2:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
import seaborn as sns
205/3: hp=pd.read_csv(http://localhost:8888/edit/Desktop/mtech%20ml/new%20house%20prediction/House_Price_Data.csv)
205/4: hp=pd.read_csv('http://localhost:8888/edit/Desktop/mtech%20ml/new%20house%20prediction/House_Price_Data.csv')
205/5: hp=pd.read_csv(r'http://localhost:8888/edit/Desktop/mtech%20ml/new%20house%20prediction/House_Price_Data.csv')
205/6: hp=pd.read_csv(House_Price_Data.csv)
205/7: hp=pd.read_csv('House_Price_Data.csv')
205/8: hp.head()
205/9: hp.shape()
205/10: hp.shape
205/11: hp.coloums()
205/12: hp.colums()
205/13: hp.coloums()
205/14: hp.columns()
205/15: hp.columns
205/16: hp.corr()
205/17: hp.iplot()
205/18: read=pd.read_clipboard(House_Price_Data_Description.txt)
205/19: read=pd.read_clipboard('House_Price_Data_Description.txt')
205/20:
read=pd.read_clipboard('House_Price_Data_Description.txt')
read.head()
205/21:
read=pd.read_clipboard('House_Price_Data_Description.txt')
read
205/22:
read=pd.read_clipboard('House_Price_Data_Description.txt')
read
205/23:
read=pd.read_excel('House_Price_Data_Description.txt')
read
205/24:
read=pd.read_html('House_Price_Data_Description.txt')
read
205/25:
read=pd.read_fwf('House_Price_Data_Description.txt')
read
205/26:
read=pd.read_fwf('House_Price_Data_Description.txt')
read
pd.set_option(display.max_rows)
205/27:
read=pd.read_fwf('House_Price_Data_Description.txt')
read
pd.set_option(display.max_rows,True)
205/28:
read=pd.read_fwf('House_Price_Data_Description.txt')
read
pd.set_option(display.[max_rows],True)
205/29:
read=pd.read_fwf('House_Price_Data_Description.txt')
read
pd.set_option(display.max_rows,True)
205/30:
read=pd.read_fwf('House_Price_Data_Description.txt')
read
205/31: hp.isnull()
205/32: hp.isnull().sum()
205/33:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
205/34:

hp.isnull().sum()
205/35:

hp.isnull().sum()
205/36:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 9999;
205/37:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 999;
205/38: pd.set_option('display.max_rows',100)
205/39:

hp.isnull().sum()
205/40: hp.isnull().sum().ascending=True
205/41: hp.isnull().sum().ascending(True)
205/42: hp.isnull().sum().ascending()
205/43: hp.isnull().sum().asc()
205/44: hp.isnull().sum().sort()
205/45:
k=hp.isnull().sum()
k.sort()
205/46:
k=hp.isnull().sum()
k.Sort()
205/47:
k=hp.isnull().sum()
k.sorted()
205/48:
k=hp.isnull().sum()
k.sorted()
205/49:
k=hp.isnull().sum()
k.sort_values()
205/50:
k=hp.isnull().sum()
k.sort_values(ascending=True)
205/51:
k=hp.isnull().sum()
k.sort_values(ascending=False)
205/52: k=hp.isnull().sum().sort_values(ascending=False)
205/53: hp.isnull().sum().sort_values(ascending=False)
205/54: hp.isnull().sum().sort_values(ascending=False).iplot()
205/55: hp.isnull().sum().sort_values(ascending=False).iplot(kind='heatmap')
205/56: hp.isnull().sum().sort_values(ascending=False).iplot(kind='bar')
205/57: hp.corr().iplot()
205/58: hp.corr().iplot(kind='heatmap')
205/59: hp.info()
205/60: hp.describe()
205/61: hp.head)()
205/62: hp.head()
205/63:
pd.set_option('display.max_rows',100)
pd.set_option('display.max_coloumns',100)
205/64:
pd.set_option('display.max_rows',100)
pd.set_option('display.max_coloumn',100)
205/65:
pd.set_option('display.max_rows',100)
pd.set_option('display.max_column',100)
205/66: hp.head()
205/67:
pd.set_option('display.max_rows',200)
pd.set_option('display.max_column',200)
211/1:
import pandas as pd
import numpy as np
import seaborn as sns
from dataprep.eda import plot
from dataprep.eda import plot_correlation
211/2:
!pip install dataprep
import pandas as pd
import numpy as np
import seaborn as sns
from dataprep.eda import plot
from dataprep.eda import plot_correlation
211/3:
#!pip install dataprep
import pandas as pd
import numpy as np
import seaborn as sns
from dataprep.eda import plot
from dataprep.eda import plot_correlation
211/4: !pip install dataprep
213/1:
#!pip install dataprep
import pandas as pd
import numpy as np
import seaborn as sns
from dataprep.eda import plot
from dataprep.eda import plot_correlation
213/2: data=pd.read_csv("House_Price_Data.csv")
213/3: plot_correlation(data, "SalePrice")
213/4: plot_correlation(data, "OverallQual", "SalePrice")
213/5: plot(data, "OverallQual", "SalePrice")
213/6: plot(data, "Fireplaces", "SalePrice")
212/1:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
import seaborn as sns
import pandas as pd
import numpy as np
import seaborn as sns
from dataprep.eda import plot
from dataprep.eda import plot_correlation
212/2: plot_correlation(data, "SalePrice")
212/3: plot_correlation(hp, "SalePrice")
212/4:
read=pd.read_fwf('House_Price_Data_Description.txt')
read
212/5:
%%javascript
IPython.OutputArea.auto_scroll_threshold = 999;
212/6:
pd.set_option('display.max_rows',200)
pd.set_option('display.max_column',200)
212/7:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
import seaborn as sns
import pandas as pd
import numpy as np
import seaborn as sns
from dataprep.eda import plot
from dataprep.eda import plot_correlation
212/8: hp=pd.read_csv('House_Price_Data.csv')
212/9: hp.shape
212/10: hp.info()
212/11: hp.describe()
212/12: hp.columns
212/13: hp.corr()
212/14: hp.head()
212/15: hp.iplot()
212/16: hp.isnull().sum().sort_values(ascending=False)
212/17: hp.isnull().sum().sort_values(ascending=False).iplot(kind='bar')
212/18: hp.corr().iplot(kind='heatmap')
212/19: plot_correlation(hp, "SalePrice")
212/20: plot(hp, "Fireplaces", "SalePrice")
212/21: plot_corelation(hp, "Fireplaces", "SalePrice")
212/22: plot_correlation(hp, "Fireplaces", "SalePrice")
212/23:
plot(data, "TotalBsmtSF", "SalePrice")
plot(data, "GrLivArea", "SalePrice")
212/24:
plot(hp, "TotalBsmtSF", "SalePrice")
plot(hp, "GrLivArea", "SalePrice")
213/7: plot(data, "GrLivArea", "SalePrice")
213/8: plot(data, "TotalBsmtSF", "SalePrice")
212/25: plot(hp, "GrLivArea", "SalePrice")
212/26: plot_correlation(hp, "KitchenQual", "SalePrice")
212/27: plot_correlation(hp, "KitchenAbvGr", "SalePrice")
212/28: hp.drop(['Alley','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)
212/29: data.shape
212/30: hp.shape
213/9:
#no missing values
sns.heatmap(data.isnull(),yticklabels=False,cbar=False,cmap='coolwarm')
212/31: hp['BsmtExposure']=hp['BsmtExposure'].fillna(data['BsmtExposure'].mode()[0])
212/32: hp['BsmtExposure']=hp['BsmtExposure'].fillna(hp['BsmtExposure'].mode()[0])
212/33: hp['BsmtFinType2']=hp['BsmtFinType2'].fillna(hp['BsmtFinType2'].mode()[0])
212/34: hp['GarageQual']=hp['GarageQual'].fillna(hp['GarageQual'].mode()[0])
212/35: hp['GarageCond']=hp['GarageCond'].fillna(hp['GarageCond'].mode()[0])
212/36: hp['BsmtQual']=hp['BsmtQual'].fillna(hp['BsmtQual'].mode()[0])
212/37: hp['FireplaceQu']=hp['FireplaceQu'].fillna(hp['FireplaceQu'].mode()[0])
212/38: hp['GarageType']=hp['GarageType'].fillna(hp['GarageType'].mode()[0])
212/39: hp['GarageFinish']=hp['GarageFinish'].fillna(hp['GarageFinish'].mode()[0])
212/40: hp.isnull().sort_values(ascending=False)
212/41: hp.isnull().sum().sort_values(ascending=False)
212/42: hp['LotFrontage']=hp['LotFrontage'].fillna(hp['LotFrontage'].mean())
212/43: hp['LotFrontage']=hp['LotFrontage'].fillna(hp['LotFrontage'].mean())
212/44: hp.isnull().sum().sort_values(ascending=False)
212/45: hp['GarageYrBlt']=hp['GarageYrBlt'].fillna(hp['GarageYrBlt'].mean())
212/46: hp['BsmtCond']=hp['BsmtCond'].fillna(hp['BsmtCond'].mean())
212/47: hp['BsmtFinType1']=hp['BsmtFinType1'].fillna(hp['BsmtFinType1'].mean())
212/48: hp = hp.dropna(subset=['MasVnrType'])
212/49: hp = hp.dropna(subset=['MasVnrArea'])
212/50: hp = hp.dropna(subset=['Electrical'])
212/51: hp.dropna(subset=['BsmtCond'])
212/52: hp=hp.dropna(subset=['BsmtCond'])
212/53: hp = hp.dropna(subset=['BsmtFinType1'])
212/54: hp.isnull().sum().sort_values(ascending=False)
212/55:
def category_onehot_multcols(multcolumns):
    df_final=final_df
    i=0
    for fields in multcolumns:
        
        print(fields)
        df1=pd.get_dummies(final_df[fields],drop_first=True)
        
        final_df.drop([fields],axis=1,inplace=True)
        if i==0:
            df_final=df1.copy()
        else:
            
            df_final=pd.concat([df_final,df1],axis=1)
        i=i+1
       
        
    df_final=pd.concat([final_df,df_final],axis=1)
        
    return df_final
212/56:
columns=['MSZoning','Street','LotShape','LandContour','Utilities','LotConfig','LandSlope','Neighborhood',
         'Condition2','BldgType','Condition1','HouseStyle','SaleType',
        'SaleCondition','ExterCond',
         'ExterQual','Foundation','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',
        'RoofStyle','RoofMatl','Exterior1st','Exterior2nd','MasVnrType','Heating','HeatingQC',
         'CentralAir',
         'Electrical','KitchenQual','Functional',
         'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PavedDrive']
212/57: len(columns)
212/58: main_df=data.copy()
212/59: main_df=hp.copy()
212/60: final_df=pd.concat([data],axis=0)
212/61: final_df=pd.concat([hp],axis=0)
212/62: final_df
212/63: final_df=category_onehot_multcols(columns)
212/64: final_df.shape
212/65: final_df =final_df.loc[:,~final_df.columns.duplicated()]
212/66: final_df
215/1:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import dataprep from dataprep import plot
import dataprep from dataprep import plot_correlation
215/2:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import dataprep 
from dataprep import plot
import dataprep from dataprep import plot_correlation
215/3:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import dataprep 
from dataprep import plot
import dataprep from dataprep import plot_correlation
215/4:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import dataprep 
from dataprep import plot
from dataprep import plot_correlation
215/5:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import dataprep 
from dataprep.eda import plot
from dataprep.eda import plot_correlation
215/6:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import matplotlib
import dataprep 
from dataprep.eda import plot
from dataprep.eda import plot_correlation
cf.go_offline()
215/7: sp=pd.read_csv('Train.csv')
215/8: sp=pd.read_csv('train.csv')
215/9: sp=pd.read_csv('Stock_price_Train.csv')
215/10: sp.head()
215/11: sp.describe()
215/12: sp.isnull().sum()
215/13: sp.iplot()
215/14: sp,corr()
215/15: sp.corr()
215/16: sp.iplot(kind='box')
215/17: sp.iplot(kind='box',boxpoints='suspectedoutliers')
215/18: sp.iplot(kind='box',boxpoints='suspectedoutliers')
216/1:
income = 45000
taxPayable = 0
print("Given income", income)

if income <= 10000:
    taxPayable = 0
elif income <= 20000:
    taxPayable = (income - 10000) * 10 / 100
else:
    # first 10,000
    taxPayable = 0

    # next 10,000
    taxPayable = 10000 * 10 / 100

    # remaining
    taxPayable += (income - 20000) * 20 / 100

print("Total tax to pay is", taxPayable)
216/2:
income = 45000
taxPayable = 0
print("Given income", income)

if income <= 10000:
    taxPayable = 0
elif income <= 20000:
    taxPayable = (income - 10000) * 10 / 100
else:
    taxPayable = 0

    taxPayable = 10000 * 10 / 100

    taxPayable += (income - 20000) * 20 / 100

print("Total tax to pay is", taxPayable)
216/3: print('My', 'Name', 'Is', 'James', sep='**')
216/4:
floatNumbers = []
n = int(input("Enter the list size : "))

print("\n")
for i in range(0, n):
    print("Enter number at location", i, ":")
    item = float(input())
        floatNumbers.append(item)
    
print("User List is ", floatNumbers)
216/5:
floatNumbers = []
n = int(input("Enter the list size : "))

print("\n")
for i in range(0, n):
    print("Enter number at location", i, ":")
    item = float(input())
    floatNumbers.append(item)
    
print("User List is ", floatNumbers)
216/6:
list1 = [12, 15, 32, 42, 55, 75, 122, 132, 150, 180, 200]
for item in list1:
    if (item > 150):
        break
    if(item % 5 == 0):
        print(item)
216/7:
def outerFun(a, b):
    square = a**2
    def innerFun(a,b):
        return a+b
    add = innerFun(a, b)
    return add+5

result = outerFun(5, 10)
print(result)
216/8:
str1 = "Emma is a data scientist who knows Python. Emma works at google."
print("Original String is:", str1)

index = str1.rfind("Emma")
print("Last occurrence of Emma starts at", index)
216/9:
str1 = "Emma is a data scientist who knows Python. Emma works at google."
print("Original String is:", str1)

index = str1.rfind("Emma")
print("Last occurrence of Emma starts at", index)
216/10:
str1 = "Emma25 is Data scientist50 and AI Expert"
print("The original string is : " + str1)
# Words with both alphabets and numbers
# isdigit() for numbers + isalpha() for alphabets
# use any() to check each character
res = []
temp = str1.split()
for item in temp:
    if any(char.isalpha() for char in item) and any(char.isdigit() for char in item):
        res.append(item)
print("Displaying words with alphabets and numbers")
for i in res:
    print(i)
216/11:
str1 = "Emma25 is Data scientist50 and AI Expert"
print("The original string is : " + str1)
# Words with both alphabets and numbers
# isdigit() for numbers + isalpha() for alphabets
# use any() to check each character
res = []
temp = str1.split()
for item in temp:
    if any(char.isalpha() for char in item) and any(char.isdigit() for char in item):
        res.append(item)
print("Displaying words with alphabets and numbers")
for i in res:
    print(i)
216/12:
firstSet  = {57, 83, 29}
secondSet = {57, 83, 29, 67, 73, 43, 48}

print("First Set ", firstSet)
print("Second Set ", secondSet)

print("First set is subset of second set -", firstSet.issubset(secondSet))
print("Second set is subset of First set - ", secondSet.issubset(firstSet))

print("First set is Super set of second set - ", firstSet.issuperset(secondSet))
print("Second set is Super set of First set - ", secondSet.issuperset(firstSet))

if(firstSet.issubset(secondSet)):
  firstSet.clear()
  
if(secondSet.issubset(firstSet)):
  secondSet.clear()

print("First Set ", firstSet)
print("Second Set ", secondSet)
216/13:
firstSet  = {57, 83, 29}
secondSet = {57, 83, 29, 67, 73, 43, 48}

print("First Set ", firstSet)
print("Second Set ", secondSet)

print("First set is subset of second set -", firstSet.issubset(secondSet))
print("Second set is subset of First set - ", secondSet.issubset(firstSet))

print("First set is Super set of second set - ", firstSet.issuperset(secondSet))
print("Second set is Super set of First set - ", secondSet.issuperset(firstSet))

if(firstSet.issubset(secondSet)):
  firstSet.clear()
  
if(secondSet.issubset(firstSet)):
  secondSet.clear()

print("First Set ", firstSet)
print("Second Set ", secondSet)
216/14:
list1 = ["a", "b", ["c", ["d", "e", ["f", "g"], "k"], "l"], "m", "n"]
Sub List to be added = ["h", "i", "j"]

Expected output:
['a', 'b', ['c', ['d', 'e', ['f', 'g', 'h', 'i', 'j'], 'k'], 'l'], 'm', 'n']

Solution:
list1 = ["a", "b", ["c", ["d", "e", ["f", "g"], "k"], "l"], "m", "n"]
subList = ["h", "i", "j"]
list1[2][1][2].extend(subList)
print(list1)
216/15:

list1 = ["a", "b", ["c", ["d", "e", ["f", "g"], "k"], "l"], "m", "n"]
subList = ["h", "i", "j"]
list1[2][1][2].extend(subList)
print(list1)
216/16:

list1 = ["a", "b", ["c", ["d", "e", ["f", "g"], "k"], "l"], "m", "n"]
subList = ["h", "i", "j"]
list1[2][1][2].extend(subList)
print(list1)
216/17:
 "name": "Kelly",
  "age":25,
  "salary": 8000,
  "city": "New york"
}
Expected output:

{
  "name": "Kelly",
  "age":25,
  "salary": 8000,
  "location": "New york"
}
216/18:



{
  "name": "Kelly",
  "age":25,
  "salary": 8000,
  "location": "New york"
}
216/19:
sampleDict = {
  "name": "Kelly",
  "age":25,
  "salary": 8000,
  "city": "New york"
}
sampleDict['location'] = sampleDict.pop('city')
print(sampleDict)
216/20:
sampleDict = {
  "name": "Kelly",
  "age":25,
  "salary": 8000,
  "city": "New york"
}
sampleDict['location'] = sampleDict.pop('city')
print(sampleDict)
216/21:

set1 = {10, 20, 30, 40, 50}
set2 = {60, 70, 80, 90, 10}

if set1.isdisjoint(set2):
  print("Two sets have no items in common")
else:
  print("Two sets have items in common")
  print(set1.intersection(set2))
216/22:

set1 = {10, 20, 30, 40, 50}
set2 = {60, 70, 80, 90, 10}

if set1.isdisjoint(set2):
  print("Two sets have no items in common")
else:
  print("Two sets have items in common")
  print(set1.intersection(set2))
216/23:
tuple1 = (('a', 23),('b', 37),('c', 11), ('d',29))
tuple1 = tuple(sorted(list(tuple1), key=lambda x: x[1]))
print(tuple1)
216/24:
tuple1 = (('a', 23),('b', 37),('c', 11), ('d',29))
tuple1 = tuple(sorted(list(tuple1), key=lambda x: x[1]))
print(tuple1)
216/25:
tuple1 = (('a', 23),('b', 37),('c', 11), ('d',29))
tuple1 = tuple(sorted(list(tuple1), key=lambda x: x[1]))
print(tuple1)
216/26:

class py_solution:
 def threeSum(self, nums):
        nums, result, i = sorted(nums), [], 0
        while i < len(nums) - 2:
            j, k = i + 1, len(nums) - 1
            while j < k:
                if nums[i] + nums[j] + nums[k] < 0:
                    j += 1
                elif nums[i] + nums[j] + nums[k] > 0:
                    k -= 1
                else:
                    result.append([nums[i], nums[j], nums[k]])
                    j, k = j + 1, k - 1
                    while j < k and nums[j] == nums[j - 1]:
                        j += 1
                    while j < k and nums[k] == nums[k + 1]:
                        k -= 1
            i += 1
            while i < len(nums) - 2 and nums[i] == nums[i - 1]:
                i += 1
        return result

print(py_solution().threeSum([-25, -10, -7, -3, 2, 4, 8, 10]))
216/27:
tuple1 = (('a', 23),('b', 37),('c', 11), ('d',29))
tuple1 = tuple(sorted(list(tuple1), key=lambda x: x[1]))
print(tuple1)
216/28:

class py_solution:
 def threeSum(self, nums):
        nums, result, i = sorted(nums), [], 0
        while i < len(nums) - 2:
            j, k = i + 1, len(nums) - 1
            while j < k:
                if nums[i] + nums[j] + nums[k] < 0:
                    j += 1
                elif nums[i] + nums[j] + nums[k] > 0:
                    k -= 1
                else:
                    result.append([nums[i], nums[j], nums[k]])
                    j, k = j + 1, k - 1
                    while j < k and nums[j] == nums[j - 1]:
                        j += 1
                    while j < k and nums[k] == nums[k + 1]:
                        k -= 1
            i += 1
            while i < len(nums) - 2 and nums[i] == nums[i - 1]:
                i += 1
        return result

print(py_solution().threeSum([-25, -10, -7, -3, 2, 4, 8, 10]))
217/1: !watch nvidia-smi
217/2: watch -n 0.5 nvidia-smi
217/3: !watch -n 0.5 nvidia-smi
217/4:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
217/5:
from tensorflow as tf 
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
217/6:
import tensorflow as tf 
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
217/7: !watch nvidia-smi
217/8:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
221/1:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
221/2:
import tensorflow as tf 
config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
222/1:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
222/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
222/3:  nvidia-smi
222/4: !nvidia-smi
222/5: !watch nvidia-smi
222/6: tf.test.gpu_device_name()
222/7:
from tensorflow.compact.v1 import configproto
from tensorflow.compact.v1 import Interactivesession

config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
222/8:
from tensorflow.compat.v1 import configproto
from tensorflow.compat.v1 import Interactivesession

config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
222/9:
from tensorflow.compat.v1 import configProto
from tensorflow.compat.v1 import Interactivesession

config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
222/10:
from tensorflow.compat.v1 import configproto
from tensorflow.compat.v1 import Interactivesession

config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
222/11:
from tensorflow.compat.v1 import Configproto
from tensorflow.compat.v1 import Interactivesession

config = tf.ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.8
config.gpu_options.allow_growth = True
set_session(tf.Session(config=config))
222/12:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
222/13:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
222/14:
# import the libraries as shown below

from keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
222/15:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
222/16:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
222/17:
import tensorflow as tf
print(tf.__version__)
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
222/18:

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
222/19:

import tensorflow 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
224/1:

import tensorflow 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/1:

import tensorflow 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/2: import tensorflow
227/3:
import tensorflow 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/4:
import tensorflow 
from keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/5:
import tensorflow
from tensorflow import keras
227/6:
import tensorflow 
from keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/7:
from tf.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
227/8:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)
227/9:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
226/1:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.9
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras

from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization

df = pd.read_csv(r'C:\Users\Admin\Desktop\cassava\train.csv')
df.dtypes
df['label'] = df['label'].astype('str')
df.dtypes


print(f"There are {df.shape[0]} images in train data")
with open(r'C:\Users\Admin\Desktop\cassava\label_num_to_disease_map.json') as filename:
    labels = json.load(filename)
    
    # defining some variables which will be useful later
TRAIN_PATH = r'C:\Users\Admin\Desktop\cassava\train_images'
IMAGE_WIDTH = 224
IMAGE_HEIGHT = 224
EPOCHS = 40
BATCH_SIZE = 32
df.image_id[0]

plt.figure(figsize=(16, 12))
df_sample = df.sample(12).reset_index(drop=True)
for i in range(9):
    plt.subplot(3, 3, i+1)
    img = cv2.imread(os.path.join(TRAIN_PATH, df_sample.image_id[i]))
    img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    plt.axis('off')
    plt.imshow(img)
    plt.title(labels.get(df_sample.label[i]))
plt.tight_layout()
plt.show()

plt.figure(figsize = (10,10))
plt.title('Bar distribution of labels')
sns.countplot(df.label.values)
plt.show()

train_datagen = keras.preprocessing.image.ImageDataGenerator( horizontal_flip=True,
    vertical_flip=True,
    rotation_range=20,
    shear_range=20,
    zoom_range=0.2,
    height_shift_range=0.1,
    width_shift_range=0.1,
    validation_split=0.2)
    
train_imagegen = train_datagen.flow_from_dataframe(
    df,
    directory= r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='training',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

valid_datagen = keras.preprocessing.image.ImageDataGenerator(
    validation_split=0.2
)

valid_imagegen = valid_datagen.flow_from_dataframe(
    df,
    directory=r'C:\Users\Admin\Desktop\cassava\train_images',
    x_col='image_id',
    y_col='label',
    subset='validation',
    target_size=(IMAGE_HEIGHT, IMAGE_WIDTH),
    class_mode='categorical',
    batch_size=BATCH_SIZE
)

base_model = keras.applications.inception_v3.InceptionV3(include_top=False, weights='imagenet',pooling='avg', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH ,3))
print(base_model.summary())

# Freeze all the layers
for layer in base_model.layers[:]:
    layer.trainable = False

# Check the trainable status of the individual layers
for layer in base_model.layers:
    print(layer, layer.trainable)
    
    #Adding custom Layers
add_model = Sequential()

add_model.add(Flatten())
add_model.add(Dense(1248, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(248, activation='relu',input_shape=base_model.output_shape))
add_model.add(Dense(128, activation='relu',input_shape=base_model.output_shape))             
add_model.add(Dropout(0.20))
add_model.add(Dense(5, activation='softmax'))

# creating the final model
model = Model(inputs=base_model.input, outputs=add_model(base_model.output))

model_checkpoint = keras.callbacks.ModelCheckpoint(
    './best_weights.h5',
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode="min"
)

early_stopping = keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.001,
    patience=5,
    verbose=1,
    mode="min",
    restore_best_weights=True,
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.3,
    patience=2,
    verbose=1,
    mode="min",
    min_delta=0.001,
)

# compile the model
opt = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)

model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics=['accuracy'])
print(model.summary())

history = model.fit_generator(
    train_imagegen,
    epochs=EPOCHS,
    steps_per_epoch=(len(df)*0.8) // BATCH_SIZE,
    validation_data=valid_imagegen,
    validation_steps=(len(df)*0.2) // BATCH_SIZE,
    callbacks = [model_checkpoint, early_stopping, reduce_lr]
)

model.save("inception.h5")

model.load_weights(r"C:\Users\Admin\Desktop\cassava\best_weights.h5")
plt.figure(figsize=(15, 5))
plt.plot(history.history['accuracy'], 'b*-', label="train_acc")
plt.plot(history.history['val_accuracy'], 'r*-', label="val_acc")
plt.grid()
plt.title("train_acc vs val_acc")
plt.ylabel("Accuracy")
plt.xlabel("Epochs")
plt.legend()
plt.show()
227/10:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
227/11: from tensorflow import keras
227/12:
# import the libraries as shown below

from keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
227/13:
import tensorflow 
from keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/14:
import tensorflow 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/15:
import tensorflow 
from tf.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/16:
import tensorflow 
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
import cv2
from tensorflow import keras
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics



from tensorflow.keras import optimizers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.models import Model, load_model
from tensorflow.keras import applications
from tensorflow.keras.callbacks import ReduceLROnPlateau

from tensorflow.keras.layers import BatchNormalization
227/17:
# import the libraries as shown below

from keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
227/18:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
227/19: from tensorflow import keras
227/20:
import tensorflow as tf
print(f"Tensorflow Version: {tf.__version__}")
print(f"Keras Version: {tf.keras.__version__}")
228/1:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
228/2:
# import the libraries as shown below

from keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
228/3:
# import the libraries as shown below

from tf.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
228/4:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
228/5: from tensorflow import keras.layers
228/6: from tensorflow import keras
228/7: from tensorflow import keras
228/8:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
228/9:
from tensorflow import keras
from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
228/10:
from tensorflow import keras
from keras.layers import Input, Lambda, Dense, Flatten
228/11:
from tensorflow import keras
from tf.keras.layers import Input, Lambda, Dense, Flatten
228/12:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
231/1:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
231/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
231/3: tf.test.gpu_device_name()
231/4:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
231/5:
# import the libraries as shown below

from tf.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
231/6: import tensorflow as tf
231/7:
# import the libraries as shown below

from tf.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
233/1:
import tensorflow as tf
print(tf.__version__)
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import json
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os
from sklearn import model_selection
from sklearn import metrics
from tensorflow.keras.layers import Conv2D
import cv2 as cv2
from tensorflow import keras
234/1:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
234/2: tf.test.gpu_device_name()
234/3: !nvidia-smi
234/4:
# import the libraries as shown below

from tf.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
234/5:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
234/6: import tensorflow as tf
234/7:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
236/1:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
236/2:
from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
237/1:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
237/2:
from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
237/3:
from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
237/4:
# This is needed to display the images.
%matplotlib inline
237/5:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
237/6:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')

from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util\
237/7:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile


from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util\


from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
240/1:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
240/2:
# This is needed to display the images.
%matplotlib inline
240/3:
from utils import label_map_util

from utils import visualization_utils as vis_util
240/4:
from object_detection utils import label_map_util

from utils import visualization_utils as vis_util
240/5:
from object_detection.utils import label_map_util

from utils import visualization_utils as vis_util
240/6:
from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
240/7:
# What model to download.
MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('object_detection', 'mscoco_label_map.pbtxt')
240/8:
opener = urllib.request.URLopener()
opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)
tar_file = tarfile.open(MODEL_FILE)
for file in tar_file.getmembers():
  file_name = os.path.basename(file.name)
  if 'frozen_inference_graph.pb' in file_name:
    tar_file.extract(file, os.getcwd())
240/9:
detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')
240/10: category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)
240/11:
def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)
240/12:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
240/13:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
240/14:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
240/15:
# What model to download.
MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('object_detection/data', 'mscoco_label_map.pbtxt')
240/16:
# What model to download.
MODEL_NAME = 'ssd_mobilenet_v1_coco_2017_11_17'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('object_detection/data', 'mscoco_label_map.pbtxt')
240/17:
opener = urllib.request.URLopener()
opener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)
tar_file = tarfile.open(MODEL_FILE)
for file in tar_file.getmembers():
  file_name = os.path.basename(file.name)
  if 'frozen_inference_graph.pb' in file_name:
    tar_file.extract(file, os.getcwd())
240/18:
detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')
240/19: category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)
240/20:
def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)
240/21:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'object_detection/test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 3) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
240/22:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
240/23:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
240/24:
%matplotlib inline 
plt.figure(figsize=(50,50)) 
plt.imshow(image_np)
240/25:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'object_detection/test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 5) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
240/26:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
240/27:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
240/28:
%matplotlib inline 
plt.figure(figsize=(50,50)) 
plt.imshow(image_np)
240/29:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
240/30:
%matplotlib inline 
plt.figure(figsize=(50,50)) 
plt.imshow(image_np)
240/31:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'object_detection/test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 6) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
240/32:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
240/33:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
240/34:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'object_detection/test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 8) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
240/35:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
240/36:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
244/1: !pip install insta-scrape
244/2: from instascrape import *
244/3:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/google/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
245/1:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/mahesh_s11/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
245/2: from instascrape import *
245/3:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/mahesh_s11/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
245/4:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/le_mechatronics/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
245/5:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.strftime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
245/6:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.strftime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
245/7: dir
245/8:
from instascrape import Profile 
chris = Profile('mahesh_s11')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.strftime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
245/9:
from instascrape import Profile 
chris = Profile('mahesh_s11')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post
    post.download(f"{fname}.png")
245/10:
from instascrape import *
import pandas 
import numpy
245/11:
from instascrape import Profile 
chris = Profile('mahesh_s11')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.strftime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
245/12:
from instascrape import Profile 
chris = Profile('mahesh_s11')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
245/13:
from instascrape import Profile 
chris = Profile('mahesh_s11')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
245/14:
from instascrape import Profile 
chris = Profile('mahesh_s11')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
245/15:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
246/1:
from instascrape import *
import pandas 
import numpy
246/2:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/le_mechatronics/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
246/3:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/google/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
246/4:
from instascrape import *
import pandas 
import numpy
246/5:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
247/1:
print('PyDev console: using IPython 7.19.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\Admin\\PycharmProjects\\pythonProject', 'C:/Users/Admin/PycharmProjects/pythonProject'])
248/1:
!pip install vader
!pip install bs4
!pip install requests
!pip install url
!pip install json
251/1:
!pip install vader
!pip install bs4
!pip install requests
!pip install url
!pip install json
251/2:
import json
import requests
from bs4 import BeautifulSoup
import sys
import re
import vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer 
  
# function to print sentiments 
# of the sentence. 
def sentiment_scores(sentence): 
  
    # Create a SentimentIntensityAnalyzer object. 
    sid_obj = SentimentIntensityAnalyzer() 
  
    # polarity_scores method of SentimentIntensityAnalyzer 
    # oject gives a sentiment dictionary. 
    # which contains pos, neg, neu, and compound scores. 
    sentiment_dict = sid_obj.polarity_scores(sentence) 
      
    print("Overall sentiment dictionary is : ", sentiment_dict) 
    print("sentence was rated as ", sentiment_dict['neg']*100, "% Negative") 
    print("sentence was rated as ", sentiment_dict['neu']*100, "% Neutral") 
    print("sentence was rated as ", sentiment_dict['pos']*100, "% Positive") 
  
    print("Sentence Overall Rated As", end = " ") 
  
    # decide sentiment as positive, negative and neutral 
    if sentiment_dict['compound'] >= 0.05 : 
        print("Positive\n")
        return 1
  
    elif sentiment_dict['compound'] <= - 0.05 : 
        print("Negative\n")
        return 0
  
    else : 
        print("Neutral\n")
  
if __name__=="__main__":

    try:
        print("Enter the users instagram username(should be public)")
        s=str(input())
        input1='https://www.instagram.com/' + s + '/'
        r = requests.get(input1)
        soup = BeautifulSoup(r.text, 'html.parser')
        t=[]
        p=[]
        caption=[]
        script = soup.find('script', text=lambda t: t.startswith('window._sharedData'))
        page_json = script.text.split(' = ', 1)[1].rstrip(';')
        data = json.loads(page_json)


        for post in data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['edges']:
            image_src = post['node']['edge_media_to_caption']
            for k,val in image_src.items():
                val=str(val)
                t=val.split(":")
                x=str(t[2])
                cap=str(x[2:-4])
                cap=cap.replace("\\n"," ")
                nestr = re.sub(r'[^a-zA-Z0-9 ]',r'',cap)
                p.append(nestr)
            
        b=[]
        for i in range(len(p)):
            print(p[i])
            b.append(sentiment_scores(p[i]))
        #print(b)

        if not b:
            print("Account private")
        else:   
            if(b.count(1)>b.count(0)):
                print("User is on the positive side")
            else:
                print("User is on the negative side")
    except:
        print("Account doesn't exist or is not reachable at the moment")
251/3:
import json
import requests
from bs4 import BeautifulSoup
import sys
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer 
  
# function to print sentiments 
# of the sentence. 
def sentiment_scores(sentence): 
  
    # Create a SentimentIntensityAnalyzer object. 
    sid_obj = SentimentIntensityAnalyzer() 
  
    # polarity_scores method of SentimentIntensityAnalyzer 
    # oject gives a sentiment dictionary. 
    # which contains pos, neg, neu, and compound scores. 
    sentiment_dict = sid_obj.polarity_scores(sentence) 
      
    print("Overall sentiment dictionary is : ", sentiment_dict) 
    print("sentence was rated as ", sentiment_dict['neg']*100, "% Negative") 
    print("sentence was rated as ", sentiment_dict['neu']*100, "% Neutral") 
    print("sentence was rated as ", sentiment_dict['pos']*100, "% Positive") 
  
    print("Sentence Overall Rated As", end = " ") 
  
    # decide sentiment as positive, negative and neutral 
    if sentiment_dict['compound'] >= 0.05 : 
        print("Positive\n")
        return 1
  
    elif sentiment_dict['compound'] <= - 0.05 : 
        print("Negative\n")
        return 0
  
    else : 
        print("Neutral\n")
  
if __name__=="__main__":

    try:
        print("Enter the users instagram username(should be public)")
        s=str(input())
        input1='https://www.instagram.com/' + s + '/'
        r = requests.get(input1)
        soup = BeautifulSoup(r.text, 'html.parser')
        t=[]
        p=[]
        caption=[]
        script = soup.find('script', text=lambda t: t.startswith('window._sharedData'))
        page_json = script.text.split(' = ', 1)[1].rstrip(';')
        data = json.loads(page_json)


        for post in data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['edges']:
            image_src = post['node']['edge_media_to_caption']
            for k,val in image_src.items():
                val=str(val)
                t=val.split(":")
                x=str(t[2])
                cap=str(x[2:-4])
                cap=cap.replace("\\n"," ")
                nestr = re.sub(r'[^a-zA-Z0-9 ]',r'',cap)
                p.append(nestr)
            
        b=[]
        for i in range(len(p)):
            print(p[i])
            b.append(sentiment_scores(p[i]))
        #print(b)

        if not b:
            print("Account private")
        else:   
            if(b.count(1)>b.count(0)):
                print("User is on the positive side")
            else:
                print("User is on the negative side")
    except:
        print("Account doesn't exist or is not reachable at the moment")
251/4:
import json
import requests
from bs4 import BeautifulSoup
import sys
import re
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer 
  
# function to print sentiments 
# of the sentence. 
def sentiment_scores(sentence): 
  
    # Create a SentimentIntensityAnalyzer object. 
    sid_obj = SentimentIntensityAnalyzer() 
  
    # polarity_scores method of SentimentIntensityAnalyzer 
    # oject gives a sentiment dictionary. 
    # which contains pos, neg, neu, and compound scores. 
    sentiment_dict = sid_obj.polarity_scores(sentence) 
      
    print("Overall sentiment dictionary is : ", sentiment_dict) 
    print("sentence was rated as ", sentiment_dict['neg']*100, "% Negative") 
    print("sentence was rated as ", sentiment_dict['neu']*100, "% Neutral") 
    print("sentence was rated as ", sentiment_dict['pos']*100, "% Positive") 
  
    print("Sentence Overall Rated As", end = " ") 
  
    # decide sentiment as positive, negative and neutral 
    if sentiment_dict['compound'] >= 0.05 : 
        print("Positive\n")
        return 1
  
    elif sentiment_dict['compound'] <= - 0.05 : 
        print("Negative\n")
        return 0
  
    else : 
        print("Neutral\n")
  
if __name__=="__main__":

    try:
        print("Enter the users instagram username(should be public)")
        s=str(input())
        input1='https://www.instagram.com/' + s + '/'
        r = requests.get(input1)
        soup = BeautifulSoup(r.text, 'html.parser')
        t=[]
        p=[]
        caption=[]
        script = soup.find('script', text=lambda t: t.startswith('window._sharedData'))
        page_json = script.text.split(' = ', 1)[1].rstrip(';')
        data = json.loads(page_json)


        for post in data['entry_data']['ProfilePage'][0]['graphql']['user']['edge_owner_to_timeline_media']['edges']:
            image_src = post['node']['edge_media_to_caption']
            for k,val in image_src.items():
                val=str(val)
                t=val.split(":")
                x=str(t[2])
                cap=str(x[2:-4])
                cap=cap.replace("\\n"," ")
                nestr = re.sub(r'[^a-zA-Z0-9 ]',r'',cap)
                p.append(nestr)
            
        b=[]
        for i in range(len(p)):
            print(p[i])
            b.append(sentiment_scores(p[i]))
        #print(b)

        if not b:
            print("Account private")
        else:   
            if(b.count(1)>b.count(0)):
                print("User is on the positive side")
            else:
                print("User is on the negative side")
    except:
        print("Account doesn't exist or is not reachable at the moment")
253/1:
from instascrape import *
import pandas 
import numpy
253/2:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
253/3:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
253/4:
from instascrape import *
import pandas 
import numpy
253/5:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
253/6:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/le_mechatronics/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
253/7:
from instascrape import Profile 
chris = Profile('le_mechatronics')
chris.scrape()

recents = chris.get_recent_posts()
chris_photos = [post for post in recents if not post.is_video]

for post in chris_photos: 
    fname = post.upload_date.datetime("%Y-%m-%d %Hh%Mm")
    post.download(f"{fname}.png")
253/8:
# Instantiate the scraper objects 
google = Profile('https://www.instagram.com/le_mechatronics/')
google_post = Post('https://www.instagram.com/p/CG0UU3ylXnv/')
google_hashtag = Hashtag('https://www.instagram.com/explore/tags/google/')

# Scrape their respective data 
google.scrape()
google_post.scrape()
google_hashtag.scrape()

print(google.followers)
print(google_post['hashtags'])
print(google_hashtag.amount_of_posts)
256/1:
from instascrape import *
import pandas 
import numpy
256/2:
var myRequest = new Request('flowers.jpg');
var myHeaders = myRequest.headers; // Headers {}
256/3:
var myRequest = new Request('.jpg');
var myHeaders = myRequest.headers; // Headers {}
256/4:
var myRequest = new Request('download.jpg');
var myHeaders = myRequest.headers; // Headers {}
256/5:
from instascrape import *
import pandas 
import numpy
import requests
256/6:
var myRequest = new Request('download.jpg');
var myHeaders = myRequest.headers; //Headers {}
256/7:
var myRequest = new Request('download.jpg');
var myHeaders = myRequest.headers;
256/8:
import requests
r=requests.get("http://www.example.com/", headers={"content-type":"text"})
256/9:
import requests
r=requests.get("http://www.google.com/", headers={"content-type":"text"})
256/10:
import requests 

from instascrape import Profile 

headers = {
    "user-agent":"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Mobile Safari/537.36 Edg/87.0.664.66", 
    "cookie":'YAlFHQALAAGmPuT8v3vxm9WLJE43'
}

resp = requests.get("https://www.instagram.com/google/", headers=headers)

google = Profile(resp.text).scrape()
256/11:
import requests 

from instascrape import Profile 

headers = {
    "user-agent":"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Mobile Safari/537.36 Edg/87.0.664.66", 
    "cookie":'7883813360%3AYnpbiOzvHH4hGk%3A9'
}

resp = requests.get("https://www.instagram.com/google/", headers=headers)

google = Profile(resp.text).scrape()
256/12:
import requests 

from instascrape import Profile 

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.74 Safari/537.36 Edg/79.0.309.43", 
    "cookie":'7883813360%3AYnpbiOzvHH4hGk%3A9'
}

resp = requests.get("https://www.instagram.com/google/", headers=headers)

google = Profile(resp.text).scrape()
258/1: import tensorflow as tf
258/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
258/3:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
258/4:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'Datasets/train'
valid_path = 'Datasets/test'
258/5:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
258/6:
# don't train existing weights
for layer in vgg16.layers:
    layer.trainable = False
258/7:
  # useful for getting number of output classes
folders = glob('Datasets/train/*')
258/8: import tensorflow as tf
258/9:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
258/10: !nvidia-smi
258/11: tf.test.gpu_device_name()
258/12:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
258/13:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'Datasets/train'
valid_path = 'Datasets/test'
258/14:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
258/15:
# don't train existing weights
for layer in vgg16.layers:
    layer.trainable = False
258/16:
  # useful for getting number of output classes
folders = glob('Datasets/train/*')
258/17:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
258/18:
prediction = Dense(len(folders), activation='softmax')(x)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
258/19:

# view the structure of the model
model.summary()
258/20:
# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
258/21:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
258/22:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory('Datasets/train',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
263/1:
print('PyDev console: using IPython 7.19.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:/Users/Admin/Desktop/h2/shredder-machine-hosur'])
265/1:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile
%matplotlib inline

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
265/2:
# This is needed to display the images.
%matplotlib inline
print(tf.__version__)
from tensorflow.python.client import device_lib
print(device_lib.list_local_devices())
265/3:


from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
265/4:
# What model to download.
MODEL_NAME = 'inference_graph'
MODEL_FILE = MODEL_NAME + '.tar.gz'
DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'

# Path to frozen detection graph. This is the actual model that is used for the object detection.
PATH_TO_FROZEN_GRAPH = MODEL_NAME + '/frozen_inference_graph.pb'

# List of the strings that is used to add correct label for each box.
PATH_TO_LABELS = os.path.join('training', 'labelmap.pbtxt')
265/5:
detection_graph = tf.Graph()
with detection_graph.as_default():
  od_graph_def = tf.GraphDef()
  with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:
    serialized_graph = fid.read()
    od_graph_def.ParseFromString(serialized_graph)
    tf.import_graph_def(od_graph_def, name='')
265/6:
def load_image_into_numpy_array(image):
  (im_width, im_height) = image.size
  return np.array(image.getdata()).reshape(
      (im_height, im_width, 3)).astype(np.uint8)
265/7:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'image{}.jpg'.format(i)) for i in range(1, 4) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
265/8:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
265/9:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/10:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/11:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'cam_image{}.jpg'.format(i)) for i in range(1, 4) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
265/12:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
265/13:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/14:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/15:
%matplotlib inline
plt.imshow(image_np)
265/16:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/17:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
265/18:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/19: category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)
265/20:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/21:
%matplotlib inline
plt.imshow(image_np)
265/22:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'cam_image{}.jpg'.format(i)) for i in range(1, 2) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
265/23:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
265/24:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
265/25:
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'cam_image{}.jpg'.format(i)) for i in range(1, 3) ]

# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)
265/26:
def run_inference_for_single_image(image, graph):
  with graph.as_default():
    with tf.Session() as sess:
      # Get handles to input and output tensors
      ops = tf.get_default_graph().get_operations()
      all_tensor_names = {output.name for op in ops for output in op.outputs}
      tensor_dict = {}
      for key in [
          'num_detections', 'detection_boxes', 'detection_scores',
          'detection_classes', 'detection_masks'
      ]:
        tensor_name = key + ':0'
        if tensor_name in all_tensor_names:
          tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(
              tensor_name)
      if 'detection_masks' in tensor_dict:
        # The following processing is only for single image
        detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])
        detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])
        # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.
        real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)
        detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])
        detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])
        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(
            detection_masks, detection_boxes, image.shape[0], image.shape[1])
        detection_masks_reframed = tf.cast(
            tf.greater(detection_masks_reframed, 0.5), tf.uint8)
        # Follow the convention by adding back the batch dimension
        tensor_dict['detection_masks'] = tf.expand_dims(
            detection_masks_reframed, 0)
      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')

      # Run inference
      output_dict = sess.run(tensor_dict,
                             feed_dict={image_tensor: np.expand_dims(image, 0)})

      # all outputs are float32 numpy arrays, so convert types as appropriate
      output_dict['num_detections'] = int(output_dict['num_detections'][0])
      output_dict['detection_classes'] = output_dict[
          'detection_classes'][0].astype(np.uint8)
      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]
      output_dict['detection_scores'] = output_dict['detection_scores'][0]
      if 'detection_masks' in output_dict:
        output_dict['detection_masks'] = output_dict['detection_masks'][0]
  return output_dict
265/27:
for image_path in TEST_IMAGE_PATHS:
  image = Image.open(image_path)
  # the array based representation of the image will be used later in order to prepare the
  # result image with boxes and labels on it.
  image_np = load_image_into_numpy_array(image)
  # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
  image_np_expanded = np.expand_dims(image_np, axis=0)
  # Actual detection.
  output_dict = run_inference_for_single_image(image_np, detection_graph)
  # Visualization of the results of a detection.
  vis_util.visualize_boxes_and_labels_on_image_array(
      image_np,
      output_dict['detection_boxes'],
      output_dict['detection_classes'],
      output_dict['detection_scores'],
      category_index,
      instance_masks=output_dict.get('detection_masks'),
      use_normalized_coordinates=True,
      line_thickness=8)
  plt.figure(figsize=IMAGE_SIZE)
  plt.imshow(image_np)
267/1:
import vlc
player=vlc.MediaPlayer('rtsp://admin:L27BC57D@192.168.1.10:544/cam/realmonitor?channel=1&subtype=1')
player.play()
267/2:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://admin:L27BC57D@192.168.1.10:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
267/3:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
url="rtsp://admin:L27BC57D@192.168.1.10:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
267/4:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
cv2.resizeWindow('image', 900, 900) 
url="rtsp://admin:L27BC57D@192.168.1.10:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
267/5:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
cv2.resizeWindow('image', 400, 400) 
url="rtsp://admin:L27BC57D@192.168.1.10:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
267/6:
import pandas as pd
import numpy as np
import cv2

cv2.namedWindow("preview")
cv2.resizeWindow('preview', 400, 400) 
url="rtsp://admin:L27BC57D@192.168.1.10:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
267/7:
import pandas as pd
import numpy as np
import cv2


cv2.namedWindow("preview", cv2.WINDOW_NORMAL)
url="rtsp://admin:L27BC57D@192.168.1.10:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
267/8:
import pandas as pd
import numpy as np
import cv2


cv2.namedWindow("preview", cv2.WINDOW_NORMAL)
url="rtsp://admin:L27BC57D@192.168.1.10:37777/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
268/1:
import pandas as pd
import numpy as np
import cv2


cv2.namedWindow("preview", cv2.WINDOW_NORMAL)
url="rtsp://admin:L27BC57D@192.168.1.10:554/cam/realmonitor?channel=1&subtype=0&unicast=true&proto=Onvif"
vc = cv2.VideoCapture(url)

if vc.isOpened(): # try to get the first frame
    rval, frame = vc.read()
else:
    rval = False

while rval:
    cv2.imshow("preview", frame)
    rval, frame = vc.read()
    key = cv2.waitKey(20)
    if key == 27: # exit on ESC
        break
cv2.destroyWindow("preview")
270/1:
treepersqkm={"usa":40000,"britain":10000,"india":30000,"brazil":20000} 
lst = []
def moretress(a):
    for i in treepersqkm:
        if treepersqkm[i]> 20000 :
            lst.append(i)
            print(i)
moretress(treepersqkm)
270/2:
name=str(input("Pls enter name:"))
if( name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning ", name)
270/3:
name=str(input("Pls enter name:"))
if( name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning " name)
270/4:
name=str(input("Pls enter name:"))
if( name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning " name)
270/5:
name=(input("Pls enter name:"))
if( name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning " name)
270/6:
name=input("Pls enter name:")
if( name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning " name)
270/7:
name=input("Pls enter name:")
if(name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning " name)
270/8:
name=str()
name=input("Pls enter name:")
if(name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning " name)
270/9:
name=str(input("Pls enter name:"))
if( name== 'Bond'):
print("Welcome on board 007")
else:
print("Good morning ", name)
270/10:
uname=input('Enter Your Name : ') 
            if uname=='Bond':
        print('Welcome on Board 007 ') 
            else : 
                print('Good Morning ',uname)
270/12:
name=str(input("Pls enter name:"))
if( name== 'Bond'):
    print("Welcome on board 007")
    else:
        print("Good morning ", name)
270/13:
uname=input('Enter Your Name : ') 
            if uname=='Bond':
        print('Welcome on Board 007 ') 
            else : 
                print('Good Morning ',uname)
270/15:
uname=input('Enter Your Name : ') 
if uname=='Bond':
    print('Welcome on Board 007 ') 
else : 
    print('Good Morning ',uname)
270/16:
name=str(input("Pls enter name:"))
if( name== 'Bond'):
    print("Welcome on board 007")
else:
    print("Good morning ", name)
270/17:
uname=input('Enter Your Name : ') 
if name=='Bond':
    print('Welcome on Board 007 ') 
else : 
    print('Good Morning ',name)
270/18:
name=input('Enter Your Name : ') 
if name=='Bond':
    print('Welcome on Board 007 ') 
else : 
    print('Good Morning ',name)
270/19:
name=str(input("Pls enter name:"))
if name== 'Bond':
    print("Welcome on board 007")
else:
    print("Good morning ", name)
270/20:

a=[1,2,3,2]
b=[]
foriin a:
b.append(i*i)
print(b)
270/21:

a=[1,2,3,2]
b=[]
foriin a:
    b.append(i*i)
print(b)
270/22:

a=[1,2,3,2]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/23:

a=[1,2,3,5]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/24:
a=[1,2,3,5]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/25:
a= [1,2,3,5]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/26:
a = [1,2,3,5]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/27: a = [1,2,3,5]
270/28:
list = [1,2,3,5]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/29: a = []
270/30: a = []
270/31:
list = [1,2,3,5]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/32:
list = [1,2,3]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/33:
a = []
b=[]
270/34:
list = [1,2,3]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/35:
list = [1,2,3]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/36:
a = [1,2,3]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/37:
a = [1,2,3]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/38:
a = []
b=[]
270/39:
a = [1,2,3]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/40:
a = [1,2,3]
b=[]
fori a:
    b.append(i*i)
    print(b)
270/41:
a = [1,2,3]
b=[]
foriin a:
    b.append(i*i)
    print(b)
270/42:
a = [1,2,3]
b=[]
for i in a:
    b.append(i*i)
    print(b)
270/43:
a = [1,2,3]
b=[]
for i in a:
    b.append(i*i)
    print(b)
270/44:
a = [1,2,3]
b=[]
for i in a:
    b.append(i*i)
    print(b)
270/45:
a = [1,2,3]
b=[]
for i in a:
    b.append(i*i)
    print(b)
270/46:
a = [1,2,3]
b=[]
for i in a:
    b.append(i*i)
    print(b)
270/47:
a = [1,2,3]
b=[]
for i in a:
    b.append(i*i)
print(b)
270/48:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
 if(list[i]) == 100:
print("There is a 100 at index no: ",i)
 break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/49:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
 if(list[i]) == 100:
print("There is a 100 at index no: ",i)
 break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/50:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
 if(list[i]) == 100:
print("There is a 100 at index no: ",i)
 break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/51:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
 if(list[i]) == 100:
print("There is a 100 at index no: ",i)
 break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/52:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
print("There is a 100 at index no: ",i)
 break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/53:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
 break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/54:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
        break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/55:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
    break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/56:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
    break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/57:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
        break
 i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/58:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
        break
        i+=1
 if(len(list) == i):
 print("No 100 exists.")
270/59:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
        break
        i+=1
        if(len(list) == i):
 print("No 100 exists.")
270/60:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
        break
        i+=1
        if(len(list) == i):
            print("No 100 exists.")
270/61:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
        break
        i+=1
        if(len(list) == i):
            print("No 100 exists.")
270/62:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
    break
        i+=1
        if(len(list) == i):
            print("No 100 exists.")
270/63:

list=[1,2,29,90,100,53,90]
i=0
while(i<len(list)):
    if(list[i]) == 100:
        print("There is a 100 at index no: ",i)
        break
    i+=1
    if(len(list) == i):
        print("No 100 exists.")
270/64:
for i in range(0,6)
print('*'*i)
270/65:
for i in range(0,6)
print('*',*i)
270/66:
for i in range(0,6):
    print('*',*i)
270/67:
for i in range(0,6):
    print('*'*i)
270/68:
for i in range(6,0,-1):
    print('*'*i)
270/69:
def avg(n1,n2,n3):
    avg(n1,n2,n3)
avg(1,2,3)
270/70:
def avg(n1,n2,n3):
    avg(n1,n2,n3)
a =avg(1,2,3)
270/71:
def avg1(n1,n2,n3):
    avg(n1,n2,n3)
a =avg(1,2,3)
270/72:
def avg1(n1,n2,n3):
    avg(n1,n2,n3)
a =avg1(1,2,3)
270/73:
def avg1(n1,n2,n3):
    avg =(n1,n2,n3)/len(avg1)
a =avg1(1,2,3)
270/74:
def avg1(n1,n2,n3):
    avg =(n1,n2,n3)/3
a =avg1(1,2,3)
270/75:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
a =avg1(1,2,3)
270/76:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
avg1(1,2,3)
270/77:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
print(avg1(1,2,3))
270/78:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
avg1(1,2,3)
print(avg)
270/79:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
a =avg1(1,2,3)
print(a)
270/80:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
a =avg1(1,2,3)
print(avg)
270/81:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
a =avg1(1,2,3)
print(list(avg)
270/82:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
a =avg1(1,2,3)
print(list(avg))
270/83:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
    print(avg)
a =avg1(1,2,3)
270/84:
def avg1(n1,n2,n3):
    avg =(n1+n2+n3)/3
    print("average is:",avg)
a =avg1(1,2,3)
270/85:
fruit = input("enter the fruit name and weight: [(this format)]")
fruit.sort(key=lambda x:x[1], reverse= True)
fruit.sort(key=lambda x:x[1], reverse= False)
270/86:
fruit = input("enter the fruit name and weight: [(this format)]")
fruit.sorted(key=lambda x:x[1], reverse= True)
fruit.sorted(key=lambda x:x[1], reverse= False)
270/87:
Write a Python program to calculate the average value of the numbers in a given tuple of tuples. 
Original Tuple:
((10, 10, 10, 12), (30, 45, 56, 45), (81, 80, 39, 32), (1, 2, 3, 4))
Average value of the numbers of the said tuple of tuples:
[30.5, 34.25, 27.0, 23.25]
Original Tuple:
((1, 1, -5), (30, -15, 56), (81, -60, -39), (-10, 2, 3))
Average value of the numbers of the said tuple of tuples:
[25.5, -18.0, 3.75]
270/88:
class Queue: 
    def __init__(self): 
        self.items = [] 
    def is_empty(self): 
        return self.items == [] 
    def enqueue(self, data): 
        self.items.append(data) 
    def dequeue(self): 
        return self.items.pop(0) 
q = Queue() 
while True: 
    print('enqueue <value>') 
    print('dequeue') 
    print('quit') 
    do = input('What would you like to do? ').split() 
    operation = do[0].strip().lower() 
    if operation == 'enqueue': 
        q.enqueue(int(do[1])) 
    elif operation == 'dequeue': 
        if q.is_empty(): 
            print('Queue is empty.') 
        else: 
            print('Dequeued value: ', q.dequeue()) 
    elif operation == 'quit': 
        break
270/89:
classVehicle:
def __init__(self,name,mileage,capacity):
self.name=name
self.mileage=mileage
self.capacity=capacity
def fare(self):
return self.capacity*100
class Bus(Vehicle):
pass
School_bus=Bus("SchoolVolvo",12,50)
print("TotalBusfareis:",School_bus.fare())
270/90:
classVehicle:
def __init__(self,name,mileage,capacity):
self.name=name
self.mileage=mileage
self.capacity=capacity
def fare(self):
return self.capacity*100
class Bus(Vehicle):
pass
School_bus=Bus("SchoolVolvo",12,50)
print("TotalBusfareis:",School_bus.fare())
270/91:
class Vehicle: 
    def __init__(self, name, mileage, capacity): 
        self.name = name 
        self.mileage = mileage 
        self.capacity = capacity 
    def fare(self): 
        return self.capacity * 100 
class Bus(Vehicle): 
    pass 
School_bus = Bus("School Volvo", 12, 50) 
print("Total Bus fare is:", School_bus.fare())
270/92:
class Vehicle: 
    def __init__(self, name, mileage, capacity): 
        self.name = name 
        self.mileage = mileage 
        self.capacity = capacity 
    def fare(self): 
        return self.capacity * 100 
class Bus(Vehicle): 
    pass 
School_bus = Bus("School Volvo", 13, 50) 
print("Total Bus fare is:", School_bus.fare())
270/93:
class Vehicle: 
    def __init__(self, name, mileage, capacity): 
        self.name = name 
        self.mileage = mileage 
        self.capacity = capacity 
    def fare(self): 
        return self.capacity * 100 
class Bus(Vehicle): 
    pass 
School_bus = Bus("School Volvo", 13, 50) 
print("Total Bus fare is:", School_bus.fare())
270/94:
class Vehicle: 
    def __init__(self, name, mileage, capacity): 
        self.name = name 
        self.mileage = mileage 
        self.capacity = capacity 
    def fare(self): 
        return self.capacity * 100 
class Bus(Vehicle): 
    pass 
School_bus = Bus("School Volvo", 14, 50) 
print("Total Bus fare is:", School_bus.fare())
270/95:
class Vehicle: 
    def __init__(self, name, mileage, capacity): 
        self.name = name 
        self.mileage = mileage 
        self.capacity = capacity 
    def fare(self): 
        return self.capacity * 100 
class Bus(Vehicle): 
    pass 
School_bus = Bus("School Volvo", 14, 60) 
print("Total Bus fare is:", School_bus.fare())
270/96:
income=float(input ('Enter Your Income : ')) 
taxPayable = 0 
if income<=10000 : 
  taxPayable=0 
elif income<=20000 : 
    taxPayable=(income-10000)*10/100 
else: 
      taxPayable= (income-20000)*20/100 
print(' Tax to be Paid is : ',taxPayable)
270/97:
input_string = input("Enter a list elements separated by space ") 
print("\n") 
userList = input_string.split() 
print("user list is ", userList) 
sum1 = 0 
for num in userList: 
    sum1 += float(num) 
print("Sum = ", sum1)
270/98:
list1=[12,15,32,42,55,75,122,132,150,180,200]
foriteminlist1:
if(item>150):
break
if(item%5==0):
print(item)
270/99:
list1=[12,15,32,42,55,75,122,132,150,180,200]
foriteminlist1:
if(item>150):
break
if(item%5==0):
print(item)
270/100:
list1=[12,15,32,42,55,75,122,132,150,180,200]
foriteminlist1:
if(item>150):
break
if(item%5==0):
print(item)
270/101:
list1=[12,15,32,42,55,75,122,132,150,180,200]
foriteminlist1:
if(item>150):
break
if(item%5==0):
print(item)
270/102:
list1=[12,15,32,42,55,75,122,132,150,180,200]
foriteminlist1:
if(item>150):
break
if(item%5==0):
print(item)
270/103:
list1=[12,15,32,42,55,75,122,132,150,180,200]
foriteminlist1:
if(item>150):
break
if(item%5==0):
print(item)
270/104:
list1 = [12, 15, 32, 42, 55, 75, 122, 132, 150, 180, 200] 
for item in list1: 
    if (item > 150): 
        break 
    if(item % 5 == 0): 
        print(item)
270/105:
list1 = [12, 16, 32, 42, 57, 75, 12, 132, 150, 160, 200,250] 
for item in list1: 
    if (item > 150): 
        break 
    if(item % 5 == 0): 
        print(item)
270/106:
list1 = [12, 16, 32, 42, 57, 75, 12, 132, 150, 160, 200,250,300] 
for item in list1: 
    if (item > 150): 
        break 
    if(item % 5 == 0): 
        print(item)
270/107:
square=a**2
def innerFun(a,b):
returna+b
add=innerFun(a,b)
returnadd+5
result=outerFun(5,10)
print(result)
270/108:
 square=a**2 
  def innerFun(a,b): 
    return a+b  add = innerFun(a,b) 
  return add+5 
result=outerFun(5,10)
270/109:
square=a**2 
  def innerFun(a,b): 
    return a+b  add = innerFun(a,b) 
  return add+5 
result=outerFun(5,10)
270/110:
square=a**2 
def innerFun(a,b):
    return a+b
add = innerFun(a,b) 
  return add+5 
result=outerFun(5,10)
270/111:
square=a**2 
def innerFun(a,b):
    return a+b
add = innerFun(a,b) 
return add+5 
result=outerFun(5,10)
270/112:
square=a**2 
def innerFun(a,b):
    return a+b
add = innerFun(a,b) 
    return add+5 
result=outerFun(5,10)
270/113:
square=a**2 
def innerFun(a,b):
    return a+b
    add = innerFun(a,b) 
    return add+5 

result=outerFun(5,10)
270/114:
square=a**2 
def innerFun(a,b):
    return a+b
    add = innerFun(a,b) 
    return add+5 

result=outerFun(5,10)
270/115:
def outerFun(a,b): 
  square=a**2 
  def innerFun(a,b): 
    return a+b  add = innerFun(a,b) 
  return add+5 
result=outerFun(5,10) 
print(result)
270/116:
def outerFun(a,b): 
  square=a**2 
  def innerFun(a,b): 
    return a+b  
add = innerFun(a,b) 
  return add+5 
result=outerFun(5,10) 
print(result)
270/117:
def outerFun(a,b):
    square=a**2 
def innerFun(a,b):
    return a+b  
add = innerFun(a,b) 
  return add+5 
result=outerFun(5,10) 
print(result)
270/118:
def outerFun(a,b):
    square=a**2 
def innerFun(a,b):
    return a+b  
add = innerFun(a,b) 
    return add+5 
result=outerFun(5,10) 
print(result)
270/119:
def outerFun(a,b):
    square=a**2 
def innerFun(a,b):
    return a+b  
    add = innerFun(a,b) 
     return add+5 
result=outerFun(5,10) 
print(result)
270/120:
def outerFun(a,b):
    square=a**2 
def innerFun(a,b):
    return a+b  
    add = innerFun(a,b) 
    return add+5 
result=outerFun(5,10) 
print(result)
270/121:
def outerFun(a,b):
    square=a**2 
def innerFun(a,b):
    return a+b  
    add = innerFun(a,b) 
    return add+5 
result=outerFun(5,10) 
print(result)
270/122: !pip install mortgage
270/123: loan = Loan(principal=200000, interest=.06, term=30)
270/124:
from mortgage import Loan
loan = Loan(principal=200000, interest=.06, term=30)
270/125: loan.summarize()
270/126: loan.summarize
270/127:
from mortgage import Loan
loan = Loan(principal=200000, interest=6, term=30)
270/128:
from mortgage import Loan
loan = Loan(principal=200000, interest=0.7, term=30)
270/129: loan.summarize
270/130:
from mortgage import Loan
loan = Loan(principal=200000, interest=0.7, term=20)
270/131: loan.summarize
270/132:
import math

def calc_mortgage(principal, interest, years):
    '''
    given mortgage loan principal, interest(%) and years to pay
    calculate and return monthly payment amount
    '''
    # monthly rate from annual percentage rate
    interest_rate = interest/(100 * 12)
    # total number of payments
    payment_num = years * 12
    # calculate monthly payment
    payment = principal * \
        (interest_rate/(1-math.pow((1+interest_rate), (-payment_num))))
    return payment


# mortgage loan principal
principal = 100000
# percent annual interest
interest = 7.5
# years to pay off mortgage
years = 30
# calculate monthly payment amount
monthly_payment = calc_mortgage(principal, interest, years)
# calculate total amount paid
total_amount = monthly_payment * years * 12

# show result ...
# {:,} uses the comma as a thousands separator
sf = '''\
For a {} year mortgage loan of ${:,}
at an annual interest rate of {:.2f}%
you pay ${:.2f} monthly'''
print(sf.format(years, principal, interest, monthly_payment))
print('-'*40)
print("Total amount paid will be ${:,.2f}".format(total_amount))

''' result ...
For a 30 year mortgage loan of $100,000
at an annual interest rate of 7.50%
you pay $699.21 monthly
----------------------------------------
Total amount paid will be $251,717.22
'''
270/133:
import math

def calc_mortgage(principal, interest, years):
    '''
    given mortgage loan principal, interest(%) and years to pay
    calculate and return monthly payment amount
    '''
    # monthly rate from annual percentage rate
    interest_rate = interest/(100 * 12)
    # total number of payments
    payment_num = years * 12
    # calculate monthly payment
    payment = principal * \
        (interest_rate/(1-math.pow((1+interest_rate), (-payment_num))))
    return payment


# mortgage loan principal
principal = 200000
# percent annual interest
interest = 7.5
# years to pay off mortgage
years = 30
# calculate monthly payment amount
monthly_payment = calc_mortgage(principal, interest, years)
# calculate total amount paid
total_amount = monthly_payment * years * 12

# show result ...
# {:,} uses the comma as a thousands separator
sf = '''\
For a {} year mortgage loan of ${:,}
at an annual interest rate of {:.2f}%
you pay ${:.2f} monthly'''
print(sf.format(years, principal, interest, monthly_payment))
print('-'*40)
print("Total amount paid will be ${:,.2f}".format(total_amount))
270/134:
import math

def calc_mortgage(principal, interest, years):
    '''
    given mortgage loan principal, interest(%) and years to pay
    calculate and return monthly payment amount
    '''
    # monthly rate from annual percentage rate
    interest_rate = interest/(100 * 12)
    # total number of payments
    payment_num = years * 12
    # calculate monthly payment
    payment = principal * \
        (interest_rate/(1-math.pow((1+interest_rate), (-payment_num))))
    return payment


# mortgage loan principal
principal = 200000
# percent annual interest
interest = 6.5
# years to pay off mortgage
years = 30
# calculate monthly payment amount
monthly_payment = calc_mortgage(principal, interest, years)
# calculate total amount paid
total_amount = monthly_payment * years * 12

# show result ...
# {:,} uses the comma as a thousands separator
sf = '''\
For a {} year mortgage loan of ${:,}
at an annual interest rate of {:.2f}%
you pay ${:.2f} monthly'''
print(sf.format(years, principal, interest, monthly_payment))
print('-'*40)
print("Total amount paid will be ${:,.2f}".format(total_amount))
271/1: import tensorflow as tf
271/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
271/3:
from tensorflow.compat.v1 import configProto
from tensorflow.compat.v1 import Interactivesession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
271/4:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import Interactivesession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
271/5:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import Interactivesession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
271/6:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
271/7: !nvidia-smi
271/8:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
271/9: tf.test.gpu_device_name()
271/10:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob
271/11: from tf.keras.layers import Input
271/12: import tensorflow as tf
271/13: from tf.keras.layers import Input
271/14: from tensorflow.keras.layers import Input
271/15:
from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
271/16:
#layers ,applications,models,preprocessing(lamp)
from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
from glob import glob
271/17:
import tensorflow as tf 
__tf__.version
271/18:
import tensorflow as tf 
__tensorflow__.version
271/19:
import tensorflow as tf 
__tensorflow.version__
271/20:
import tensorflow as tf 
__tf.version__
271/21:
import tensorflow as tf 
tf.__version__
272/1:
import tensorflow as tf 
tf.__version__
271/22: !nvidia-smi
271/23:
# import the libraries as shown below

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob,__all__
271/24:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'Datasets/test'
271/25:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
271/26:
# import the libraries as shown below

#layers ,applications,models,preprocessing(lamp)
from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
from glob import glob,__all__
271/27:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
271/28:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
271/29: vgg16.summary()
271/30:
  # useful for getting number of output classes
folders = glob('training_set/*')
271/31:
  # useful for getting number of output classes
folders = glob('training_set/*')
271/32: folders
271/33: folders
271/34:
  # useful for getting number of output classes
folders = glob('training_set/*')
271/35: folders
271/36:
  # useful for getting number of output classes
folders = __all__('training_set/*')
271/37:
  # useful for getting number of output classes
folders = glob('training_set/*')
271/38: folders
271/39: IMAGE_SIZE
271/40: IMAGE_SIZE + 3
271/41: IMAGE_SIZE + [3]
271/42:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
271/43:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights
# +3 because of rcb 3 channel, +1 for grey scale img
#include_top=False because the output has 1000 class we are removing it as we need only 2
#imagenet is competition in which vgg16 gave good result

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
271/44: vgg16.summary()
271/45:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
271/46:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
271/47:
prediction = Dense(len(folders), activation='softmax')(x)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/48:
prediction = Dense(len(folders), activation='softmax')(x)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/49:

# view the structure of the model
model.summary()
271/50:
y = Dense(300, activation='softmax')(x)
z = Dense(200, activation='softmax')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/51:

# view the structure of the model
model.summary()
271/52:
y = Dense(300, activation='relu')(x)
z = Dense(200, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/53:

# view the structure of the model
model.summary()
271/54:
y = Dense(200, activation='relu')(x)
z = Dense(50, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/55:

# view the structure of the model
model.summary()
271/56:
y = Dense(100, activation='relu')(x)
z = Dense(50, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/57:

# view the structure of the model
model.summary()
271/58:
y = Dense(80, activation='relu')(x)
z = Dense(50, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/59:

# view the structure of the model
model.summary()
271/60:
y = Dense(20, activation='relu')(x)
z = Dense(5, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
271/61:

# view the structure of the model
model.summary()
271/62:
# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
271/63:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
271/64:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory('traning_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
271/65:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
271/66:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory('traning_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
271/67:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
271/68:
test_set = test_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'categorical')
271/69:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=50,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
271/70:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit(
  training_set,
  validation_data=test_set,
  epochs=50,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
271/71:
# import the libraries as shown below
#layers ,applications,models,preprocessing(lamp)

from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
import pandas
from glob import glob,__all__
271/72:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=50,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
274/1:
import tensorflow as tf 
tf.__version__
274/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
274/3:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
274/4: !nvidia-smi
274/5: tf.test.gpu_device_name()
274/6:
# import the libraries as shown below
#layers ,applications,models,preprocessing(lamp)

from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
import pandas
from glob import glob,__all__
274/7:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
274/8: #IMAGE_SIZE + [3]
274/9:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights
# +3 because of rcb 3 channel, +1 for grey scale img
#include_top=False because the output has 1000 class we are removing it as we need only 2
#imagenet is competition in which vgg16 gave good result

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
274/10: vgg16.summary()
274/11:
# don't train existing weights
for layer in vgg16.layers:
    layer.trainable = False
274/12:
  # useful for getting number of output classes
folders = glob('training_set/*')
274/13: folders
274/14:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
274/15:
y = Dense(20, activation='relu')(x)
z = Dense(5, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
274/16:

# view the structure of the model
model.summary()
274/17:
# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
274/18:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
274/19:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
274/20:
test_set = test_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'categorical')
274/21:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=5,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
276/1:
import tensorflow as tf 
tf.__version__
276/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
276/3:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
276/4: !nvidia-smi
276/5: tf.test.gpu_device_name()
276/6:
# import the libraries as shown below
#layers ,applications,models,preprocessing(lamp)

from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
import pandas
from glob import glob,__all__
276/7:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
276/8: #IMAGE_SIZE + [3]
276/9:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights
# +3 because of rcb 3 channel, +1 for grey scale img
#include_top=False because the output has 1000 class we are removing it as we need only 2
#imagenet is competition in which vgg16 gave good result

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
276/10: vgg16.summary()
276/11:
# don't train existing weights
for layer in vgg16.layers:
    layer.trainable = False
276/12:
  # useful for getting number of output classes
folders = glob('training_set/*')
276/13: folders
276/14:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
276/15:
y = Dense(20, activation='relu')(x)
z = Dense(5, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
276/16:

# view the structure of the model
model.summary()
276/17:
# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
276/18:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
276/19:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
276/20:
test_set = test_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'categorical')
276/21:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=5,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
278/1:
import tensorflow as tf 
tf.__version__
278/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
278/3:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
278/4: !nvidia-smi
278/5: tf.test.gpu_device_name()
278/6:
# import the libraries as shown below
#layers ,applications,models,preprocessing(lamp)

from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
import pandas
from glob import glob,__all__
278/7:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
278/8: #IMAGE_SIZE + [3]
278/9:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights
# +3 because of rcb 3 channel, +1 for grey scale img
#include_top=False because the output has 1000 class we are removing it as we need only 2
#imagenet is competition in which vgg16 gave good result

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
278/10: vgg16.summary()
278/11:
# don't train existing weights
for layer in vgg16.layers:
    layer.trainable = False
278/12:
  # useful for getting number of output classes
folders = glob('training_set/*')
278/13: folders
278/14:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
278/15:
y = Dense(20, activation='relu')(x)
z = Dense(5, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
278/16:

# view the structure of the model
model.summary()
278/17:
# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
278/18:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
278/19:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
278/20:
test_set = test_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'categorical')
278/21:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=5,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
278/22:
# plot the loss
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='train loss')
plt.plot(r.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(r.history['accuracy'], label='train acc')
plt.plot(r.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc')
278/23:
# save it as a h5 file


from tensorflow.keras.models import load_model

model.save('model_vgg16.h5')
278/24:

y_pred = model.predict(test_set)
278/25: y_pred
278/26:
import numpy as np
y_pred = np.argmax(y_pred, axis=1)
278/27: y_pred
278/28:
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
278/29: model=load_model('model_resnet50.h5')
278/30:
# save it as a h5 file


from tensorflow.keras.models import load_model

model.save('model_vgg16.h5')
278/31:

y_pred = model.predict(test_set)
278/32: y_pred
278/33:
import numpy as np
y_pred = np.argmax(y_pred, axis=1)
278/34: y_pred
278/35:
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
278/36: img_data
278/37: img=image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\dogs',target_size=(224,224))
278/38: img=image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\dogs',target_size=(224,224))
278/39: img=image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\dogs',target_size=(224,224))
278/40: img=image.load_img(r'test_set\dogs',target_size=(224,224))
278/41:
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
278/42: img=image.load_img(r'test_set\dogs',target_size=(224,224))
278/43: img=image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',target_size=(224,224))
278/44: img=image.load_img(r'C:\Users\Admin\Desktop\tf2\cnn\test_set\dog.4017.jpg',target_size=(224,224))
278/45: img=image.load_img(r'test_set\dog.4017.jpg',target_size=(224,224))
278/46: img=image.load_img('dog.4059.jpg',target_size=(224,224))
278/47:
test_image=image.img_to_array(img)
test_image=test_image/255
test_image=np.expand_dims(x,axis=0)
result=model.predict(test_image)
278/48:
test_image=image.img_to_array(x)
test_image=test_image/255
test_image=np.expand_dims(x,axis=0)
result=model.predict(test_image)
278/49: x=image.load_img('dog.4059.jpg',target_size=(224,224))
278/50:
test_image=image.img_to_array(x)
test_image=test_image/255
test_image=np.expand_dims(x,axis=0)
result=model.predict(test_image)
278/51: x.shape
278/52: result
278/53:
#x=np.expand_dims(x,axis=0)
#img_data=preprocess_input(x)
test_image.shape
278/54: model.predict(test_image)
278/55: a=np.argmax(model.predict(test_image), axis=1)
278/56: a==1
278/57: if a==0:
278/58:
if a==0:
    print("cat")
else:
    print('dog')
278/59:
y = Dense(10, activation='relu')(x)
z = Dense(5, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
278/60:

# view the structure of the model
model.summary()
278/61:

# view the structure of the model
model.summary()
278/62:
import tensorflow as tf 
tf.__version__
278/63:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
278/64:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
278/65: !nvidia-smi
278/66: tf.test.gpu_device_name()
278/67:
# import the libraries as shown below
#layers ,applications,models,preprocessing(lamp)

from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
import pandas
from glob import glob,__all__
278/68:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
278/69: #IMAGE_SIZE + [3]
278/70:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights
# +3 because of rcb 3 channel, +1 for grey scale img
#include_top=False because the output has 1000 class we are removing it as we need only 2
#imagenet is competition in which vgg16 gave good result

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
278/71: vgg16.summary()
278/72:
# don't train existing weights
for layer in vgg16.layers:
    layer.trainable = False
278/73:
  # useful for getting number of output classes
folders = glob('training_set/*')
278/74: folders
278/75:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
278/76:
y = Dense(10, activation='relu')(x)
z = Dense(5, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
278/77:

# view the structure of the model
model.summary()
278/78:
# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
278/79:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
278/80:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
278/81:
test_set = test_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'categorical')
278/82:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=30,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
278/83:
# plot the loss
import matplotlib.pyplot as plt
plt.plot(r.history['loss'], label='train loss')
plt.plot(r.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(r.history['accuracy'], label='train acc')
plt.plot(r.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc')
278/84:
# save it as a h5 file


from tensorflow.keras.models import load_model

model.save('model_vgg16.h5')
278/85:

y_pred = model.predict(test_set)
278/86: y_pred
278/87:
import numpy as np
y_pred = np.argmax(y_pred, axis=1)
278/88: y_pred
278/89:
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
278/90: #model=load_model('model_resnet50.h5')
278/91: #img_data
278/92: x=image.load_img('dog.4059.jpg',target_size=(224,224))
278/93:
test_image=image.img_to_array(x)
test_image=test_image/255
test_image=np.expand_dims(x,axis=0)
result=model.predict(test_image)
278/94: result
278/95:
#x=np.expand_dims(x,axis=0)
#img_data=preprocess_input(x)
test_image.shape
278/96: a=np.argmax(model.predict(test_image), axis=1)
278/97:
if a==0:
    print("cat")
else:
    print('dog')
278/98:
if a==0:
    print("cat")
else:
    img.open('dog.4059.jpg')
    print('dog')
278/99:
if a==0:
    print("cat")
else:
    imgage.open('dog.4059.jpg')
    print('dog')
278/100:

if a==0:
    print("cat")
else:
    image.open('dog.4059.jpg')
    print('dog')
278/101:

if a==0:
    print("cat")
else:
    Image.open('dog.4059.jpg')
    print('dog')
278/102:
from PIL import *  
if a==0:
    print("cat")
else:
    Image.open('dog.4059.jpg')
    print('dog')
278/103:
from PIL import Image  
if a==0:
    print("cat")
else:
    Image.open('dog.4059.jpg')
    print('dog')
278/104:
from PIL import *
if a==0:
    print("cat")
else:
    im=Image.open('dog.4059.jpg')
    im.show()
    print('dog')
278/105:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open('dog.4059.jpg')
    im.show()
    print('dog')
278/106:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open('dog.4059.jpg')
    im.show()
    print('dog')
278/107:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    im.show()
    print('dog')
278/108:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(im1)
    plt.show()
    print('dog')
278/109:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(im)
    plt.show()
    print('dog')
278/110:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(im)
    
    print('dog')
278/111:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(im)
    
    print('&'*10,'dog')
278/112:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(im)
    
    print('&'*10,'&'*10,'dog')
278/113:
from PIL import Image
if a==0:
    print("cat")
else:
    im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(im)
    
    print('&'*10,'dog','&'*10,)
278/114:
x=image.load_img('dog.4059.jpg',target_size=(224,224))

plt.imshow(x)
278/115:
from PIL import Image

if a==0:
    im=Image.open(x)
    plt.imshow(im)
    print("cat")
else:
    im=Image.open(x)
    plt.imshow(im)
    print('&'*10,'dog','&'*10,)
278/116:
from PIL import Image

if a==0:
    #im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(im)
    print("cat")
else:
    #im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(x)
    print('&'*10,'dog','&'*10,)
278/117:
from PIL import Image

if a==0:
    #im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(x)
    #plt.imshow(im)
    print("cat")
else:
    #im=Image.open(r'C:\Users\Admin\Desktop\tf2\cnn\dog.4059.jpg')
    plt.imshow(x)
    print('&'*10,'dog','&'*10,)
282/1:
import cufflinks as cf
cf.go_offline()
282/2:
import pandas as pd
import cufflinks as cf
cf.go_offline()
282/3: import seaborn as sns
283/1:
import pandas as pd
import cufflinks as cf
cf.go_offline()
283/2: dataset.iplot()
283/3: s = pd.Dataframe(dataset)
283/4: s = pd.Data_frame(dataset)
283/5: s = pd.dataframe(dataset)
283/6: s = pd.DataFrame(dataset)
283/7:

dataset= [10,12,12, 13,12,11,14,13,15,10,10, 10, 100,12, 14,13, 12,10, 10,11,12,15,12,13,12,11,14,13,15,10, 15,12,10,14,13,15,10]
283/8: s = pd.DataFrame(dataset)
283/9: s.iplot()
283/10: s.iplot(kind=boxplot)
283/11: s.iplot(kind=box)
283/12: s.iplot(kind='box')
286/1: import pandas
286/2: import pandas as pd
286/3: u = pd.read_csv("Stock_price_Train.csv")
287/1: import pandas as pd
287/2: u = pd.read_csv("Stock_price_Train.csv")
287/3: u.isnull()
287/4: u.isnull().sum
287/5: u.isnull().sum()
287/6:
import pandas as pd
import cufflinks as cf
cf.go_offline()
287/7: u.iplot()
287/8: u.iplot(kind = "box")
287/9:
u.iplot(kind = "box",boxpoints="suspectedoutliers
")
287/10: u.iplot(kind = "box",boxpoints="suspectedoutliers")
288/1:
import tensorflow
tensorflow.__version__
288/2: !pip install nltk
288/3:
import os
import nltk
import nltk.corpus
288/4: print(os.list)
288/5: print(os.dir)
288/6: print(os.listdir)
288/7: print(os.listdir())
288/8: print(os.listdir(nltk.data.find("corpora")))
288/9: nltk.corpus.gutenberg.fields()
288/10:
import nltk
nltk.download()
288/11: print(os.listdir(nltk.data.find("corpora")))
288/12: print(os.listdir())
288/13: nltk.corpus.gutenberg.fields()
288/14: nltk.corpus.gutenberg.fileids()
288/15: nltk.corpus.gutenberg.words()
288/16: nltk.corpus.gutenberg.words(shakespeare-hamlet.txt)
288/17: nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')
288/18:
for word in hamlet:
    print(word,sep='',end="")
288/19: hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')
288/20:
for word in hamlet:
    print(word,sep='',end="")
288/21:
for word in hamlet:
    print(word,sep=' ',end="")
288/22:
for word in hamlet:
    print(word,sep=' ',end=" ")
288/23:
for word in hamlet:
    print(word,sep=' ',end="")
288/24:
for word in hamlet:
    print(word,sep='',end="")
288/25:
for word in hamlet:
    print(word,sep='',end=" ")
288/26: hamlet = nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')
288/27:  nltk.corpus.gutenberg.words('shakespeare-hamlet.txt')
288/28:
for word in hamlet:
    print(word,sep='',end=" ")
288/29:
sourceEncoding = "iso-8859-1"
targetEncoding = "utf-8"
source = open("textanalytics.txt")
target = open("target", "w")

target.write(unicode(source.read(), sourceEncoding).encode(targetEncoding))
289/1:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
289/2:
from object_detection.utils import label_map_util

from object_detection.utils import visualization_utils as vis_util
290/1:
import tensorflow as tf 
tf.__version__
290/2:
import tensorflow as tf 

if tf.test.gpu_device_name(): 
    

    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))

else:

    print("Please install GPU version of TF")
290/3:
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.allow_growth = True
config.gpu_options.per_process_gpu_memory_fraction = 0.8
session = InteractiveSession(config=config)
290/4: !nvidia-smi
290/5: tf.test.gpu_device_name()
290/6:
# import the libraries as shown below
#layers ,applications,models,preprocessing(lamp)

from tensorflow.keras.layers import Input,Lambda,Dense,Flatten
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
import numpy as np
import pandas
from glob import glob,__all__
290/7:
# re-size all the images to this
IMAGE_SIZE = [224, 224]

train_path = 'test_set'
valid_path = 'traning_set'
290/8: #IMAGE_SIZE + [3]
290/9:
# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights
# +3 because of rcb 3 channel, +1 for grey scale img
#include_top=False because the output has 1000 class we are removing it as we need only 2
#imagenet is competition in which vgg16 gave good result

vgg16 = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
290/10: vgg16.summary()
290/11:
# don't train existing weights
for layer in vgg16.layers:
    layer.trainable = False
290/12:
  # useful for getting number of output classes
folders = glob('training_set/*')
290/13: folders
290/14:
# our layers - you can add more if you want
x = Flatten()(vgg16.output)
290/15:
y = Dense(10, activation='relu')(x)
z = Dense(5, activation='relu')(y)
prediction = Dense(len(folders), activation='softmax')(z)

# create a model object
model = Model(inputs=vgg16.input, outputs=prediction)
290/16:

# view the structure of the model
model.summary()
290/17:
# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
290/18:
# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
290/19:
# Make sure you provide the same target size as initialied for the image size
training_set = train_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\training_set',
                                                 target_size = (224, 224),
                                                 batch_size = 32,
                                                 class_mode = 'categorical')
290/20:
test_set = test_datagen.flow_from_directory(r'C:\Users\Admin\Desktop\tf2\cnn\test_set',
                                            target_size = (224, 224),
                                            batch_size = 32,
                                            class_mode = 'categorical')
290/21:
# fit the model
# Run the cell. It will take some time to execute
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=30,
  steps_per_epoch=len(training_set),
  validation_steps=len(test_set)
)
291/1: !pip install modin[ray]
291/2: import modin.pandas as pd
291/3: a = pd.read_csv('C:\Users\Admin\Desktop\New folder\icml_face_data.csv')
291/4: a = pd.read_csv('C:\Users\Admin\Desktop\New folder\icml_face_data.csv',unicode='utf-8')
291/5: a = pd.read_csv('C:\Users\Admin\Desktop\New folder\icml_face_data.csv',unicode='utf-5889-1')
291/6: a = pd.read_csv(r'C:\Users\Admin\Desktop\New folder\icml_face_data.csv')
291/7: a
291/8: a.pixels.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/9: a.pixels.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/10: a.col2.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/11: a.Col2.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/12: a.pixels.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/13: a.pixels.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/14: a.'pixels'.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/15: a.col1.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/16: df = pd.read_csv(r'C:\Users\Admin\Desktop\New folder\icml_face_data.csv')
291/17: df
291/18: df.col1.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
291/19:
# make string version of original column, call it 'col'
df['col'] = df['col1'].astype(str)

# make the new columns using string indexing
df['col1'] = df['col'].str[0:2]
df['col2'] = df['col'].str[2:4]
df['col3'] = df['col'].str[4:6]

# get rid of the extra variable (if you want)
df.drop('col', axis=1, inplace=True)
291/20:
# make string version of original column, call it 'col'
df['col'] = df['pixels'].astype(str)

# make the new columns using string indexing
df['col1'] = df['col'].str[0:2]
df['col2'] = df['col'].str[2:4]
df['col3'] = df['col'].str[4:6]

# get rid of the extra variable (if you want)
df.drop('col', axis=1, inplace=True)
291/21:
# make string version of original column, call it 'col'
df['col'] = df['pixels'].astype(str)

# make the new columns using string indexing
df['col1'] = df['col'].str[0:2]
df['col2'] = df['col'].str[2:4]
df['col3'] = df['col'].str[4:6]

# get rid of the extra variable (if you want)
df.drop('col', axis=1, inplace=True)
291/22: pd.head()
291/23: df.head()
291/24: df.columns
291/25: df[pixels]
291/26: df['pixels']
292/1: import modin.pandas as pd
292/2: df = pd.read_csv(r'C:\Users\Admin\Desktop\New folder\icml_face_data.csv')
292/3: df
292/4: df.col1.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
292/5: df.pixels.astype(str).str.extractall("(?P<col1>\d{2})(?P<col2>\d{2})(?P<col3>\d{2})").reset_index(drop=True)
292/6: test = pd.DataFrame({'A':23,34,45},{{'B':20,30,40}})
292/7: test = pd.Data_Frame({'A':23,34,45},{{'B':20,30,40}})
292/8: test = pd.DataFrame({'A':[23,34,45]},{'B':[20,30,40]})
292/9: test = pd.DataFrame('A':[23,34,45],'B':[20,30,40])
292/10: test = pd.DataFrame({'A':[23,34,45],'B':[20,30,40]})
292/11: test[['A', 'B']] = test['AB'].str.split(' ', 1, expand=True)
292/12: test = pd.DataFrame({'AB':[23,34,45],'B':[20,30,40]})
292/13: test[['A', 'B']] = test['AB'].str.split(' ', 1, expand=True)
292/14: test['AB'].astype(str)
292/15: test[['A', 'B']] = test['AB'].str.split(' ', 1, expand=True)
292/16: test[['A', 'B']] = test['AB'].str.split(' ', 1, expand=True)
295/1:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
295/2: df=pd.read_csv("Train")
295/3:
import os
os.list(dir)
295/4:
import os
os.listdir()
295/5: df=pd.read_csv("train.csv")
295/6: df.iplot()
295/7: df.iplot(kind='pie')
295/8: df.head()
295/9: df.iplot(kind='pie', labels='Sex',values ='Survived')
295/10: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent')
295/11: df['male'].count()
295/12: df['male'].values_count()
295/13: df['male'].values_sum()
295/14: df['male'].value_counts()
295/15: df['Sex'].value_counts()
295/16: df['Sex'].value_counts().iplot()
295/17: df['Sex'].value_counts().iplot(kind='bar')
295/18: df['Sex']df[df['survived']==1].value_counts().iplot(kind='bar')
295/19: df['Sex'][df[df['survived']==1]].value_counts().iplot(kind='bar')
295/20: df['Sex'][df[df['Survived']==1]].value_counts().iplot(kind='bar')
295/21: df['Sex'][df['Survived']==1]].value_counts().iplot(kind='bar')
295/22: [df['Survived']==1]]
295/23: df[df['Survived']==1]]
295/24: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',sort='True')
295/25: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',sort=True)
295/26: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',sort=False)
295/27: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',sort=True)
295/28: df[df['Survived']=='1']]
295/29: df[df['Survived']=='1']
295/30: df[df['Survived']=='1'
295/31: df[df['Survived']=='1']
295/32: df[df['Survived']=='0']
295/33: df[df['Survived']=='1']
295/34: df['Sex']df[df['Survived']=='1'].value_counts().iplot(kind='bar')
295/35: df['Sex'][df['Survived']=='1'].value_counts().iplot(kind='bar')
295/36: [df['Survived']=='1']
295/37: [df['Survived']=='1'].value_counts()
295/38: [df['Survived']=='1'].sum()
295/39: [df['Survived']=='1'].count
295/40: [df['Survived']=='1'].count()
295/41: [df['Survived']=='1'].count(1)
295/42: [df['Survived']=='1'].count(1)
295/43: [df['Survived']=='1'].bool()
295/44: [df['Survived']=='1'].count(bool)
295/45: [df['Survived']=='1'].count(numeric_only =bool)
295/46: [df['Survived']=='1'].count(numeric_only =False)
295/47: [df['Survived']=='1'].count(numeric_only = False)
295/48: [df['Survived']=='1'].count(numeric_only = True)
295/49: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',theme='solar',sort=True)
295/50: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',theme='solar',sort=True, subplots=True, shape=(0,0))
295/51: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',theme='solar',sort=True, subplots=True, shape=(0,1))
295/52: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',theme='solar',sort=True, subplots=True, shape=(1,1))
295/53: cf.iplt()
295/54: cf.iplot()
295/55: cf.iplot(asImage=True)
295/56: 'cam_image1'.jpg.iplot(asImage=True)
295/57: 'cam_image1.jpg'.iplot(asImage=True)
295/58: a='cam_image1.jpg'
295/59: a.iplot(asImage=True)
295/60: a.iplot(asImage=True,In OFFLINE mode=a)
295/61: cf.iplot(asImage=True,In OFFLINE mode=a)
295/62: cf.iplot(asImage=True,In OFFLINE mode='cam_image1.jpg')
295/63: cf.help()
295/64: df.iplot(kind='pie', labels='Sex',values ='Survived', textinfo='label+percent',theme='solar')
295/65: df.iplot(kind='bar',x='Sex',y="survived")
295/66: df.iplot(kind='bar',x='Sex',y="Survived")
295/67: df.iplot(kind='bar',x='Sex',y="Survived", textinfo='label+percent')
295/68: df.iplot(kind='bar',x='Sex',y="Survived", textinfo='label+percent',barmode='group')
295/69: df.iplot(kind='bar',x='Sex',y="Survived", textinfo='label+percent',barmode='stack')
295/70: df.iplot(kind='bar',x='Sex',y="Survived", textinfo='label+percent',barmode='overlay')
295/71: df.iplot(kind='bar',x='Sex',y="Survived", textinfo='label+percent')
295/72: df.iplot(kind='bar',x='Sex',y="Survived".count(), textinfo='label+percent')
295/73: df.iplot(kind='bar',x='Sex',y="Survived".count_values(), textinfo='label+percent')
295/74: df.iplot(kind='bar',x='Sex',y=df["Survived"].count_values(), textinfo='label+percent')
295/75: df["Survived"].count_values()
295/76: df.count_values()
295/77: df.values_count()
295/78: df.value_counts()
295/79: df.value_counts().sum()
295/80: df["Survived"].value_counts().sum()
295/81: df["Survived"].value_counts()
295/82: df.iplot(kind='bar',x='Sex',y=df["Survived"].value_counts(), textinfo='label+percent')
295/83: df.iplot(kind='bar',x='Sex',y=df[(df["Survived"].value_counts()), textinfo='label+percent')
295/84: df.iplot(kind='bar',x='Sex',y=df[(df["Survived"].value_counts())], textinfo='label+percent')
295/85: y=df[(df["Survived"].value_counts())]
295/86: y=df["Survived"].value_counts()
295/87: y
295/88: df.iplot(kind='bar',x='Sex',y='y', textinfo='label+percent')
295/89: df.iplot(kind='bar',x='Sex',y=y, textinfo='label+percent')
295/90: df["Survived"]=="male".value_counts()
295/91: df[df["Survived"]=="male"].value_counts()
295/92: df[df["Survived"]=="Male"].value_counts()
295/93: df[df["Sex"]=="Male"].value_counts()
295/94: df[df["Sex"]=="male"].value_counts()
295/95: df["Sex"]=="male".value_counts()
295/96: [df["Sex"]=="male"].value_counts()
295/97: [tnc['Sex']== 'male'].value_counts()
295/98: [df['Sex']== 'male'].value_counts()
295/99: tnc['Survived'][df['Sex']== 'male'].value_counts()
295/100: df['Survived'][df['Sex']== 'male'].value_counts()
295/101: df['Survived'][df['Sex']== 'male'].value_counts().iplot(kind="bar")
295/102: df['Survived'][df['Sex']== 'male'].value_counts().iplot(kind="bar",x="Sex")
295/103: df['Survived'][df['Sex']== 'male'].value_counts().iplot(kind="bar",X="Sex")
295/104: df['Survived'][df['Sex']== 'male'].value_counts().iplot(kind="bar",x="Sex")
295/105: df['Survived'][df['Sex']== 'male'].value_counts().iplot(kind="bar")
295/106: df['Survived'][df['Sex']== 'female'].value_counts().iplot(kind="bar")
295/107: df.iplot(kind='bar', x='Pclass',y='SibSp')
295/108: df['Pclass'][df['SibSp']].value_counts().iplot(kind='bar', x='Pclass')
295/109: [df['SibSp']].value_counts()
295/110: [df['SibSp'].value_counts()
295/111: df['SibSp'].value_counts()
295/112: df['Pclass']df['SibSp'].value_counts().iplot(kind='bar', x='Pclass')
295/113: df['Pclass','SibSp'].value_counts().iplot(kind='bar', x='Pclass')
295/114: df['Pclass']['SibSp'].value_counts().iplot(kind='bar', x='Pclass')
295/115: df[df['Pclass']['SibSp']].value_counts().iplot(kind='bar', x='Pclass')
295/116: df[df['Pclass']['SibSp']].value_counts().iplot(kind='bar', x='Pclass')
295/117: df[df['Pclass']['SibSp'].value_counts().iplot(kind='bar', x='Pclass')
295/118: df.value_counts().iplot(kind='bar', x='Pclass')
295/119: df.iplot(kind='bar', x='Pclass')
295/120: df.iplot(kind='hist', x='Pclass')
295/121: df.iplot(kind='bar', x='Pclass')
295/122: df.iplot(kind='bar', x='Pclass',y='SibSp')
295/123: df.iplot(kind='hist', x='Pclass',y='SibSp')
295/124: df.iplot(kind='hist', x='Pclass',y=df['SibSp'].value_counts())
295/125: df.iplot(kind='bar', x='Pclass',y=df['SibSp'].value_counts())
295/126: a = df['SibSp'].value_counts()
295/127:
pd.pivot(df,a)
df.iplot(kind='bar', x='Pclass',y=df['SibSp'].value_counts())
295/128:
pd.pivot(df,a)
df.iplot(kind='bar', x='Pclass',barmode="overlay")
295/129:
pd.pivot(df,a)
df.iplot(kind='bar', x='Pclass',y='SibSp',barmode="overlay")
295/130:

df.iplot(kind='bar', x='Pclass',y='SibSp',barmode="overlay")
295/131: pd.pivot(df,coloumns='SibSp')
295/132: pd.pivot(df,columns='SibSp')
295/133: pd.pivot(df,columns='Pclass',values='SibSp')
295/134: a.iplot()
295/135: a.iplot(kind="bar")
295/136: a.iplot()
295/137: df['SibSp'].count()
295/138: df['SibSp'].count_values()
295/139: df['SibSp'].value_count()
295/140: df['SibSp'].value_counts()
295/141: pd.pivot(df,column='Pclass',value=a)
295/142: pd.pivot(df,columns='Pclass',value=a)
295/143: pd.pivot(df,columns='Pclass',values=a)
295/144: df['Survived'][df['Pclass']== '1'].value_counts()
295/145: df['SibSp'][df['Pclass']== '1'].value_counts()
295/146: df['SibSp'][df['Pclass']== 1].value_counts()
295/147: df['SibSp'][df['Pclass']== 1].value_counts().iplot(kind="bar")
295/148: df['SibSp'][df['Pclass']].value_counts().iplot(kind="bar")
295/149: df['SibSp'][df['Pclass']].value_counts().iplot(kind="bar",x='Pclass')
295/150: df['SibSp'].value_counts().iplot(kind="bar",x='Pclass')
295/151: df[df['SibSp']].value_counts().iplot(kind="bar",x='Pclass')
295/152:
import pandas as pd
import numpy as np
import cufflinks as cf
cf.go_offline()
import seaborn as sns
295/153: sns.scatterplot()
295/154: df.scatterplot()
295/155: sns.scatterplot(df)
295/156: pd.pivot(df,coloumns='Pclass',values="SibSp")
295/157: pd.pivot(df,columns='Pclass',values="SibSp")
295/158: a.iplot()
295/159: pd.pivot(df,index='Pclass',values="SibSp")
295/160: pd.pivot(df,index='Pclass',columns='Pclass',values="SibSp")
295/161: pd.pivot(df,index=[1,2,3],columns='Pclass',values="SibSp")
295/162: a=df['SibSp'].value_counts()
295/163: sns.scatterplot(df,x="Pclass",y="Sibsp")
295/164: sns.scatterplot(df,x="Pclass",y="SibSp")
295/165:
survived_sex = tnc[tnc['Survived']==1]['Sex']].value_counts()
dead_sex = tnc[tnc['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
295/166:
survived_sex = df[df['Survived']==1]['Sex']].value_counts()
dead_sex = df[df['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
295/167:
survived_sex = df[df['Survived']==1]['Sex'].value_counts()
dead_sex = df[df['Survived']==0]['Sex'].value_counts()
tnc1 = pd.DataFrame([survived_sex,dead_sex])
tnc1.index = ['Survived','Dead']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the Sex')
295/168: survived_sex = df[df['Pclass']==1]['SibSp'].value_counts()
295/169: df[df['Pclass']==1]['SibSp'].value_counts()
295/170: df[df['Pclass']]['SibSp'].value_counts()
295/171: df['Pclass']['SibSp'].value_counts()
295/172: df[df['Pclass']==1]['SibSp'].value_counts()
295/173:
class1 = df[df['Pclass']==1]['SibSp'].value_counts()
class2 = df[df['Pclass']==2]['SibSp'].value_counts()
class3 = df[df['Pclass']==3]['SibSp'].value_counts()
new = pd.DataFrame(class1,class2,class3)
295/174:
class1 = df[df['Pclass']==1]['SibSp'].value_counts()
class2 = df[df['Pclass']==2]['SibSp'].value_counts()
class3 = df[df['Pclass']==3]['SibSp'].value_counts()
new = pd.DataFrame(class1,class2,class3)
new.iplot()
295/175:
class1 = df[df['Pclass']==1]['SibSp'].value_counts()
class2 = df[df['Pclass']==2]['SibSp'].value_counts()
class3 = df[df['Pclass']==3]['SibSp'].value_counts()
new = pd.DataFrame(class1,class2,class3)
new.iplot(kind='bar')
295/176:
class1 = df[df['Pclass']==1]['SibSp'].value_counts().iplot()
class2 = df[df['Pclass']==2]['SibSp'].value_counts()
class3 = df[df['Pclass']==3]['SibSp'].value_counts()
295/177:
class1 = df[df['Pclass']==1]['SibSp'].value_counts().iplot(kind='Bar')
class2 = df[df['Pclass']==2]['SibSp'].value_counts()
class3 = df[df['Pclass']==3]['SibSp'].value_counts()
295/178:
df[df['Pclass']==1]['SibSp'].value_counts().iplot(kind='Bar')
class2 = df[df['Pclass']==2]['SibSp'].value_counts()
class3 = df[df['Pclass']==3]['SibSp'].value_counts()
295/179:
df[df['Pclass']==1]['SibSp'].value_counts().iplot(kind='Bar')
df[df['Pclass']==2]['SibSp'].value_counts()
df[df['Pclass']==3]['SibSp'].value_counts()
295/180:
df[df['Pclass']==1]['SibSp'].value_counts()
df[df['Pclass']==2]['SibSp'].value_counts()
df[df['Pclass']==3]['SibSp'].value_counts()
295/181:
sns.set_style('whitegrid')
sns.countplot(x='SibSp',hue='Pclass',data=df)
295/182:

df.iplot(kind='bar', x='SibSp',y='SibSp',barmode="overlay")
295/183:

df.iplot(kind='bar', x='SibSp',y='Pclass',barmode="overlay")
295/184:

df.iplot(kind='bar', x='SibSp',y='Pclass')
295/185:

df.iplot(kind='bar', x='SibSp')
295/186:

df.iplot(kind='bar', x='SibSp',keys=["hue"])
295/187:

df.iplot(kind='bar', x='SibSp',keys=["Pclass"])
295/188: df.iplot(kind='bar', x='SibSp',keys=["Pclass"])
295/189:
sns.set_style('whitegrid')
sns.countplot(x='SibSp',data=df)
295/190:
sns.set_style('whitegrid')
sns.countplot(x='SibSp',hue='Pclass',data=df)
295/191:
sns.set_style('whitegrid')
sns.scatterplot(x='SibSp',hue='Pclass',data=df)
295/192:
sns.set_style('whitegrid')
sns.barplot(x='SibSp',hue='Pclass',data=df)
295/193:
sns.set_style('whitegrid')
sns.countplot(x='SibSp',hue='Pclass',data=df)
295/194: df.iplot(kind='bar', x='SibSp',color='Pclass')
295/195: df.iplot(kind='bar',x='survived')
295/196: df.iplot(kind='bar',x='Survived')
295/197: df.iplot(kind='bar',x='Survived',keys=['Pclass,"Gender"'])
295/198: df.iplot(kind='bar',x='Survived',keys=['Pclass,"Sex"'])
295/199: df.iplot(kind='bar',x='Survived',keys=['Pclass,"Sex"'])
295/200: df.iplot(kind='bar',x='Survived',y=['Pclass,"Sex"'])
295/201: df.iplot(kind='bar',x='Survived',y=['Pclass',"Sex"'])
295/202: df.iplot(kind='bar',x='Survived',y=['Pclass',"Sex"])
295/203: df.iplot(kind='hist',x='Survived',y=['Pclass',"Sex"])
295/204: df.iplot(kind='hist',x='Survived',y='Pclass')
295/205: df.iplot(kind='bar',x='Survived',y=['Pclass',"Sex"])
295/206: df.iplot(kind='bar',y='Survived',x=['Pclass',"Sex"])
295/207: df.iplot(kind='bar',y='Survived',x=["Sex"])
295/208: df.iplot(kind='bar',y='Survived',x=["Pclass"])
295/209: df[df["Pclass"]]["Survival"].value_counts()
295/210: df[df["Pclass"]==1]["Survival"].value_counts()
295/211: df[df["Pclass"]==1]["Survived"].value_counts()
295/212:
a=df[df['Pclass']==1]['SibSp'].value_counts()
b=df[df['Pclass']==2]['SibSp'].value_counts()
c=df[df['Pclass']==3]['SibSp'].value_counts()
tnc1 = pd.DataFrame([a,b,c])
tnc1.index = ['1','2','3']
tnc1.iplot(kind='bar',barmode='stack', title='Survival by the class')
295/213:
a=df[df['Pclass']==1]['SibSp'].value_counts()
b=df[df['Pclass']==2]['SibSp'].value_counts()
c=df[df['Pclass']==3]['SibSp'].value_counts()
tnc1 = pd.DataFrame([a,b,c])
tnc1.index = ['1','2','3']
tnc1.iplot(kind='bar', title='Survival by the class')
295/214: df[df["Pclass"]]["Survived"].value_counts()
295/215: df[df["Pclass"]]["Survived"]}.value_counts()
295/216: df[df["Pclass"]]df["Survived"]}.value_counts()
295/217: df["Pclass"]]df["Survived"]}.value_counts()
295/218: df["Pclass"]df["Survived"]}.value_counts()
295/219: df["Pclass"]df["Survived"].value_counts()
295/220: df.value_counts(["Pclass", "Survived"])
295/221: df["Pclass","Survived"].value_counts()
295/222: df.value_counts(["Pclass", "Survived"]).iplot()
295/223: df.value_counts(["Pclass", "Survived"]).iplot(kind='bar')
295/224: !pip install dtale
295/225:
import dtale
import pandas as pd 
dtale.show(pd.read_csv("train.csv"))
295/226: !pip install sweetviz
295/227:
import pandas as pd
import sweetviz as sv

#EDA using Autoviz
sweet_report = sv.analyze(pd.read_csv("train.csv"))
295/228: sweet_report.show
295/229: sweet_report.show()
295/230: sweet_report
295/231: sweet_report.show_html('sweet_report.html')
295/232:
sns.set_style('whitegrid')
sns.countplot(x='Pclass',hue='SibSp',data=df)
295/233:
import dtale
import pandas as pd 
dtale.show(pd.read_csv("train.csv"))
295/234:
import dtale
import pandas as pd 
dtale.show(pd.read_csv("train.csv"))
295/235:

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Pclass'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Pclass']].count().reset_index()
chart_data = chart_data.dropna()

import plotly.graph_objs as go

charts = []
charts.append(go.Bar(
    x=chart_data['x'],
    y=chart_data['Pclass']
)
figure = go.Figure(data=charts, layout=go.Layout({
    'barmode': 'group',
    'legend': {'orientation': 'h'},
    'title': {'text': 'Pclass by Survived (Count)'},
    'xaxis': {'tickformat': '.0f', 'title': {'text': 'Survived'}},
    'yaxis': {'tickformat': '.0f', 'title': {'text': 'Pclass (Count)'}}
}))
295/236:
import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Pclass'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Pclass']].count().reset_index()
chart_data = chart_data.dropna()

import plotly.graph_objs as go

charts = []
charts.append(go.Bar(
    x=chart_data['x'],
    y=chart_data['Pclass']
)
figure = go.Figure(data=charts, layout=go.Layout({
    'barmode': 'group',
    'legend': {'orientation': 'h'},
    'title': {'text': 'Pclass by Survived (Count)'},
    'xaxis': {'tickformat': '.0f', 'title': {'text': 'Survived'}},
    'yaxis': {'tickformat': '.0f', 'title': {'text': 'Pclass (Count)'}}
}))
295/237:

# from plotly.offline import iplot, init_notebook_mode

init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
295/238:
import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Pclass'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Pclass']].count().reset_index()
chart_data = chart_data.dropna()

import plotly.graph_objs as go

charts = []
charts.append(go.Bar(
    x=chart_data['x'],
    y=chart_data['Pclass']
)
figure = go.Figure(data=charts, layout=go.Layout({
    'barmode': 'group',
    'legend': {'orientation': 'h'},
    'title': {'text': 'Pclass by Survived (Count)'},
    'xaxis': {'tickformat': '.0f', 'title': {'text': 'Survived'}},
    'yaxis': {'tickformat': '.0f', 'title': {'text': 'Pclass (Count)'}}
}))
295/239:

from plotly.offline import iplot, init_notebook_mode

init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
295/240:
import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Pclass'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Pclass']].count().reset_index()
chart_data = chart_data.dropna()

import plotly.graph_objs as go

charts = []
charts.append(go.Bar(
    x=chart_data['x'],
    y=chart_data['Pclass']
)
figure = go.Figure(data=charts, layout=go.Layout({
    'barmode': 'group',
    'legend': {'orientation': 'h'},
    'title': {'text': 'Pclass by Survived (Count)'},
    'xaxis': {'tickformat': '.0f', 'title': {'text': 'Survived'}},
    'yaxis': {'tickformat': '.0f', 'title': {'text': 'Pclass (Count)'}}
}))
295/241:
import pandas as pd
from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Pclass'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Pclass']].count().reset_index()
chart_data = chart_data.dropna()

import plotly.graph_objs as go

charts = []
charts.append(go.Bar(
    x=chart_data['x'],
    y=chart_data['Pclass']
)
figure = go.Figure(data=charts, layout=go.Layout({
    'barmode': 'group',
    'legend': {'orientation': 'h'},
    'title': {'text': 'Pclass by Survived (Count)'},
    'xaxis': {'tickformat': '.0f', 'title': {'text': 'Survived'}},
    'yaxis': {'tickformat': '.0f', 'title': {'text': 'Pclass (Count)'}}
}))
295/242:
import pandas as pd
from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Pclass'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Pclass']].count().reset_index()
chart_data = chart_data.dropna()

import plotly.graph_objs as go

charts = []
charts.append(go.Bar(
    x=chart_data['x'],
    y=chart_data['Pclass']
)
figure = go.Figure(data=charts, layout=go.Layout({
    'barmode': 'group',
    'legend': {'orientation': 'h'},
    'title': {'text': 'Pclass by Survived (Count)'},
    'xaxis': {'tickformat': '.0f', 'title': {'text': 'Survived'}},
    'yaxis': {'tickformat': '.0f', 'title': {'text': 'Pclass (Count)'}}
}))
295/243:
import pandas as pd
from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Pclass'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Pclass']].count().reset_index()
chart_data = chart_data.dropna()

import plotly.graph_objs as go

charts = []
charts.append(go.Bar(
    x=chart_data['x'],
    y=chart_data['Pclass']
)
figure = go.Figure(data=charts, layout=go.Layout({
    'barmode': 'group',
    'legend': {'orientation': 'h'},
    'title': {'text': 'Pclass by Survived (Count)'},
    'xaxis': {'tickformat': '.0f', 'title': {'text': 'Survived'}},
    'yaxis': {'tickformat': '.0f', 'title': {'text': 'Pclass (Count)'}}
}))
295/244:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go

chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

# from plotly.offline import iplot, init_notebook_mode

# init_notebook_mode(connected=True)
# chart.pop('id', None) # for some reason iplot does not like 'id'
# iplot(chart)
295/245:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go

chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

# from plotly.offline import iplot, init_notebook_mode

# init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
# iplot(chart)
295/246:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go

chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

from plotly.offline import iplot, init_notebook_mode

init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
295/247:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go
chart.pop('id', None)
chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

from plotly.offline import iplot, init_notebook_mode

init_notebook_mode(connected=True)
 # for some reason iplot does not like 'id'
iplot(chart)
295/248:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go

chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
chart.pop('id', None)
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

from plotly.offline import iplot, init_notebook_mode

init_notebook_mode(connected=True)
 # for some reason iplot does not like 'id'
iplot(chart)
295/249:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go

chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
295/250:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
295/251:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'
from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:
295/252:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

chart_data.iplot()
295/253:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

chart_data.iplot(kind='bar')
295/254:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

chart_data.iplot(kind='pie')
295/255:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go

chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
295/256:
# DISCLAIMER: 'df' refers to the data you passed in when calling 'dtale.show'

import pandas as pd

if isinstance(df, (pd.DatetimeIndex, pd.MultiIndex)):
    df = df.to_frame(index=False)

# remove any pre-existing indices for ease of use in the D-Tale code, but this is not required
df = df.reset_index().drop('index', axis=1, errors='ignore')
df.columns = [str(c) for c in df.columns]  # update columns to strings in case they are numbers

chart_data = pd.concat([
    df['Survived'],
    df['Sex'],
], axis=1)
chart_data = chart_data.sort_values(['Survived'])
chart_data = chart_data.rename(columns={'Survived': 'x'})
chart_data = chart_data.groupby(['x'])[['Sex']].count().reset_index()
chart_data = chart_data.dropna()
chart_data = chart_data[chart_data['Sex'] > 0]  # can't represent negatives in a pie

import plotly.graph_objs as go

chart = go.Pie(labels=chart_data['x'], y=chart_data['Sex'])
figure = go.Figure(data=[chart], layout=go.Layout({
    'legend': {'orientation': 'h'}, 'title': {'text': 'Sex by Survived (Count)'}
}))

# If you're having trouble viewing your chart in your notebook try passing your 'chart' into this snippet:

from plotly.offline import iplot, init_notebook_mode
init_notebook_mode(connected=True)
chart.pop('id', None) # for some reason iplot does not like 'id'
iplot(chart)
295/257:
import dtale
import pandas as pd 
dtale.show(pd.read_csv("train.csv"))
295/258:
import dtale
import pandas as pd 
dtale.show(pd.read_csv("train.csv"))
297/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
impoer seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
297/2:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
297/3: os.listdir()
297/4:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
297/5: os.listdir()
297/6: mst = pd.read_csv('mnist_train.csv')
297/7: mst
297/8: mst.describe()
297/9: dtale.show(pd.read_csv('mnist_train.csv'))
297/10: dtale.show(pd.read_csv('mnist_train.csv'))
297/11: dtale.show(pd.read_csv('mnist_train.csv'))
297/12: mst[row2]
297/13: mst[row=2]
297/14: mst[rows=2]
297/15: mst[0:1][0]
297/16: mst = pd.read_csv('mnist_train.csv'))
297/17: mst = pd.read_csv('mnist_train.csv')
297/18: mst[0:1][0]
297/19: mst[0:1][0:0]
297/20: mst[1:1][0:0]
297/21: mst[2:1][0:0]
297/22: mst[2:1][1:0]
297/23: mst[2:1][1:1]
297/24: mst.iloc[0]
297/25: a = mst.T
297/26: a
297/27: pd.set_option(display.[max_columns=100])
297/28: pd.set_option(display.['max_columns'=100])
297/29: pd.set_option('display.max_columns'=100])
297/30: pd.set_option('display.max_columns', 100)
297/31: pd.set_option('display.max_rows', 1000)
297/32: a
297/33: pd.set_option('display.max_columns', 1000)
297/34: pd.set_option('display.max_rows', 1000)
297/35: a
297/36: b = mst.iloc[0]
297/37:
# Python program to convert 
# numpy array to image 
  
# import required libraries 
import numpy as np 
from PIL import Image as im 
  
# define a main function 
def main(): 
  
    # create a numpy array from scratch 
    # using arange function. 
    # 1024x720 = 737280 is the amount  
    # of pixels. 
    # np.uint8 is a data type containing 
    # numbers ranging from 0 to 255  
    # and no non-negative integers 
    #array = np.arange(0, 737280, 1, np.uint8) 
      
    # check type of array 
    #print(type(array)) 
      
    # our array will be of width  
    # 737280 pixels That means it  
    # will be a long dark line 
    print(array.b) 
      
    # Reshape the array into a  
    # familiar resoluition 
    array = np.reshape(b, (1024, 720)) 
      
    # show the shape of the array 
    print(b.shape) 
  
    # show the array 
    print(b) 
      
    # creating image object of 
    # above array 
    data = im.fromarray(b) 
      
    # saving the final output  
    # as a PNG file 
    data.save('gfg_dummy_pic.png') 
  
# driver code 
if __name__ == "__main__": 
    
  # function call 
  main()
297/38:
# Python program to convert 
# numpy array to image 
  
# import required libraries 
import numpy as np 
from PIL import Image as im 
  
# define a main function 
def main(): 
  
    # create a numpy array from scratch 
    # using arange function. 
    # 1024x720 = 737280 is the amount  
    # of pixels. 
    # np.uint8 is a data type containing 
    # numbers ranging from 0 to 255  
    # and no non-negative integers 
    #array = np.arange(0, 737280, 1, np.uint8) 
      
    # check type of array 
    #print(type(array)) 
      
    # our array will be of width  
    # 737280 pixels That means it  
    # will be a long dark line 
    #print(array.b) 
      
    # Reshape the array into a  
    # familiar resoluition 
    array = np.reshape(b, (1024, 720)) 
      
    # show the shape of the array 
    print(b.shape) 
  
    # show the array 
    print(b) 
      
    # creating image object of 
    # above array 
    data = im.fromarray(b) 
      
    # saving the final output  
    # as a PNG file 
    data.save('gfg_dummy_pic.png') 
  
# driver code 
if __name__ == "__main__": 
    
  # function call 
  main()
297/39:
# Python program to convert 
# numpy array to image 
  
# import required libraries 
import numpy as np 
from PIL import Image as im 
  
# define a main function 
def main(): 
  
    # create a numpy array from scratch 
    # using arange function. 
    # 1024x720 = 737280 is the amount  
    # of pixels. 
    # np.uint8 is a data type containing 
    # numbers ranging from 0 to 255  
    # and no non-negative integers 
    #array = np.arange(0, 737280, 1, np.uint8) 
      
    # check type of array 
    #print(type(array)) 
      
    # our array will be of width  
    # 737280 pixels That means it  
    # will be a long dark line 
    #print(array.b) 
      
    # Reshape the array into a  
    # familiar resoluition 
    #array = np.reshape(b, (1024, 720)) 
      
    # show the shape of the array 
    print(b.shape) 
  
    # show the array 
    print(b) 
      
    # creating image object of 
    # above array 
    data = im.fromarray(b) 
      
    # saving the final output  
    # as a PNG file 
    data.save('gfg_dummy_pic.png') 
  
# driver code 
if __name__ == "__main__": 
    
  # function call 
  main()
297/40:
from PIL import Image
import numpy as np

img = Image.fromarray(b, 'RGB')
img.save('my.png')
img.show()
297/41:
from PIL import Image
import numpy as np

img = Image.fromarray(b, 'Grey')
img.save('my.png')
img.show()
297/42:
from PIL import Image
import numpy as np

img = Image.fromarray(b)
img.save('my.png')
img.show()
297/43: b = mst.iloc[0]
297/44:
b = mst.iloc[0] 
b.flatten()
297/45:
b = mst.iloc[0] 
b.
297/46:
b = mst.iloc[0] 
b
297/47: b = mst.iloc[0]
297/48: mst.corr()
298/1:
import os
import modin.pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
298/2: mst = pd.read_csv('mnist_train.csv')
298/3: a = mst.T
298/4: a.corr()
298/5: mst.corr()
298/6: mst.corr()
298/7: dtale.show(pd.read_csv('mnist_train.csv'))
299/1:
import os
import modin.pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
299/2: mst = pd.read_csv('mnist_train.csv')
299/3: dtale.show(pd.read_csv('mnist_train.csv'))
299/4: dtale.show(pd.read_csv('mnist_train.csv'))
299/5: dtale.show(pd.read_csv('mnist_train.csv'))
299/6:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
299/7: dtale.show(pd.read_csv('mnist_train.csv'))
299/8: dtale.show(pd.read_csv('mnist_train.csv'))
299/9: mst = pd.read_csv('mnist_train.csv')
299/10: a = mst.T
299/11: a
299/12: dtale.show(pd.read_csv(a))
299/13: dtale.show(a)
299/14: dtale.show(pd.read_csv('mst'))
299/15: mst = pd.read_csv('mnist_train.csv')
299/16: mst.describe()
299/17: dtale.show(pd.read_csv('mst'))
299/18:
from PIL import Image
import numpy as np

img = Image.fromarray(b)
img.save('my.png')
img.show()
299/19: b = mst.iloc[0]
299/20:
from PIL import Image
import numpy as np

img = Image.fromarray(b)
img.save('my.png')
img.show()
299/21: sns.heatmap(data=a)
299/22:
import os
import modin.pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
299/23:
std_scaler = StandardScaler()
X_s = StandardScaler().fit_transform(mst)
std_X=pd.DataFrame(X_s)
std_X.coloums=X.coloums
std_X.head()
299/24:
c = pd.drop("label",1)
d = df["label"]
299/25:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
299/26:
c = pd.drop("label",1)
d = df["label"]
299/27:
c = df.drop("label",1)
d = df["label"]
299/28:
c = mst.drop("label",1)
d = mst["label"]
299/29:
std_scaler = StandardScaler()
X_s = StandardScaler().fit_transform(c)
std_X=pd.DataFrame(X_s)
std_X.coloums=X.coloums
std_X.head()
299/30:
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_x=pd.DataFrame(x_s)
std_x.coloums=X.coloums
std_x.head()
299/31:
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_x=pd.DataFrame(x_s)
std_x.coloums=x.coloums
std_x.head()
299/32:
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_x=pd.DataFrame(x_s)
std_x.coloums=c.coloums
std_x.head()
299/33:
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.coloums=c.coloums
std_c.head()
299/34:
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
299/35:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
import threading as th
299/36: th.Timer()
299/37:
T = th.Timer (Delay Duration, function, args = None, kwargs = None)
T.start()
299/38:
T = th.Timer (10, function, args = None, kwargs = None)
T.start()
299/39:
import threading as th
## Creating a function
def prnt():
print("EDU CBA \n")
T = th.Timer(3.0, prnt)
T.start()
print("Exit Program\n")
T.cancel()
299/40:
import threading as th
## Creating a function
def prnt():
    print("EDU CBA \n")
    T = th.Timer(3.0, prnt)
    T.start()
    print("Exit Program\n")
    T.cancel()
299/41:
import time

def procedure():
    time.sleep(2.5)

# measure process time
t0 = time.clock()
procedure()
print time.clock(), "seconds process time"

# measure wall time
t0 = time.time()
procedure()
print time.time() - t0, "seconds wall time"
299/42:
import time

def procedure():
    time.sleep(2.5)

# measure process time
    t0 = time.clock()
    procedure()
print time.clock(), "seconds process time"

# measure wall time
     t0 = time.time()
     procedure()
print time.time() - t0, "seconds wall time"
299/43:
import time

def procedure():
    time.sleep(2.5)

# measure process time
t0 = time.clock()
procedure()
print time.clock(), "seconds process time"

# measure wall time
t0 = time.time()
procedure()
print time.time() - t0, "seconds wall time"
299/44:
# import the time module 
import time 

# define the countdown func. 
def countdown(t): 
    
    while t: 
        mins, secs = divmod(t, 60) 
        timer = '{:02d}:{:02d}'.format(mins, secs) 
        print(timer, end="\r") 
        time.sleep(1) 
        t -= 1
    
    print('Fire in the hole!!') 


# input time in seconds 
t = input("Enter the time in seconds: ") 

# function call 
countdown(int(t)) 
import time

def procedure():
    time.sleep(2.5)

# measure process time
t0 = time.clock()
procedure()
print time.clock(), "seconds process time"

# measure wall time
t0 = time.time()
procedure()
print time.time() - t0, "seconds wall time"
299/45:
# import the time module 
import time 

# define the countdown func. 
def countdown(t): 
    
    while t: 
        mins, secs = divmod(t, 60) 
        timer = '{:02d}:{:02d}'.format(mins, secs) 
        print(timer, end="\r") 
        time.sleep(1) 
        t -= 1
    
    print('Fire in the hole!!') 


# input time in seconds 
t = input("Enter the time in seconds: ") 

# function call 
countdown(int(t))
299/46:
import time
time.perf_counter()
299/47:
import time
time.perf_counter()


time.perf_counter()
299/48:
>>> t = Timer()
>>> t.start()

>>> t.stop()
299/49:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
from timer import timer
299/50:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
from timer import timer
299/51: !pip install timer
299/52:
>>> t = Timer()
>>> t.start()

>>> t.stop()
299/53:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
from timer import timer
299/54:
>>> t = Timer()
>>> t.start()

>>> t.stop()
299/55:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
from timer import timer
299/56:
>>> t = Timer()
>>> t.start()

>>> t.stop()
299/57:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
from timer import Timer
299/58:
>>> t = timer()
>>> t.start()

>>> t.stop()
299/59:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
from timer import Timer
299/60:
>>> t = _Timerr()
>>> t.start()

>>> t.stop()
299/61:
>>> t = _Timer()
>>> t.start()

>>> t.stop()
299/62:
t = _Timer()
t.start()
t.stop()
299/63:
t = timer()
t.start()
t.stop()
299/64:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
from timer import timer
299/65:
t = timer()
t.start()
t.stop()
299/66:
import logging
import time

from timer import timer

# timer would print nothing without this line or logging level is info or higher
logging.basicConfig(level=logging.DEBUG)


# explicit the timer's name and it's time unit
@timer('function:add', unit='s')
def add(a, b):
    time.sleep(.1)
    return a + b


# function name is timer's name for default
@timer
def sub(a, b):
    time.sleep(.1)
    return a - b


if __name__ == '__main__':
    # 'timer' would be timer's name by default
    with timer('time.sleep(2)') as t:
        print(3)
        time.sleep(1)
        print(f'after time.sleep(1) once, t.elapse = {t.elapse}')
        time.sleep(1)
        print(f'after time.sleep(1) twice, t.elapse = {t.elapse}')
    print(f'after with, t.elapse = {t.elapse}')

    print(add(1, 1))
    print(sub(2, 1))
299/67:
os.listdir()
%%t
299/68: %time
299/69: %timeit
299/70: %time
299/71:
%time
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
299/72:
%timeit
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
299/73:
%timeit
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
%timeit
299/74:
%timeit
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
%time
299/75:

std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
%time
299/76:
mst.describe()
%time
299/77: %time mst.describe()
299/78:
%time 
mst.describe()
%it
299/79:
%time 
mst.describe()
%time
299/80:
%time 
mst.describe()
%timeit
299/81:
%timeit
mst.describe()
%timeit
299/82:
%%timeit
mst.describe()
%timeit
299/83:
%%timeit
mst.describe()
299/84:
%%time
mst.describe()
299/85:
%%time
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
%time
299/86:
%%timeit
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
%time
299/87:
%%time
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
%time
299/88:
%%time
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
299/89: os.listdir()
299/90:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import pca
299/91:
%%time
pca = PCA.fit(std_c)
299/92:
%%time
pca = pca.fit(std_c)
299/93:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
299/94:
%%time
pca = PCA.fit(std_c)
299/95:
%%time
pca = PCA.fit(std_c)
299/96:
%%time
pca = PCA.fit(std_c)
299/97:
%%time
pca = PCA().fit(std_c)
299/98:
%%time
pca = PCA().fit(std_c)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,8,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/99:
%%timeit
pca = PCA().fit(std_c)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,8,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/100:
%%time
pca = PCA().fit(std_c)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,8,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/101:

pca = PCA().fit(std_c)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,8,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
%%time
299/102:
%%timeit
pca = PCA().fit(std_c)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,8,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/103:
from sklearn.decomposition import PCA
pca1 = PCA(n_components=5)
df3 = pca1().fit_transform(Std_x)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,4,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/104: x = StandardScaler().fit_transform(c)
299/105:
x = StandardScaler().fit_transform(c)
x
299/106:
x = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x)
std_c.columns=c.columns
std_c.head()
299/107:
x = StandardScaler().fit_transform(c)
x=pd.DataFrame(x)
x.columns=c.columns
x.head()
299/108:
x = StandardScaler().fit_transform(c)
x=pd.DataFrame(x)
x.head()
299/109:
x = StandardScaler().fit_transform(c)
x=pd.DataFrame(x)
x.columns=c.columns
x.head()
299/110:
%%time
x = StandardScaler().fit_transform(c)
x=pd.DataFrame(x)
x.columns=c.columns
x.head()
299/111:
%%time
std_scaler = StandardScaler()
x_s = StandardScaler().fit_transform(c)
std_c=pd.DataFrame(x_s)
std_c.columns=c.columns
std_c.head()
299/112:

pca = PCA(n_components=3)
principalComponents = pca.fit_transform(x)
print('Duration: {} seconds'.format(time.time() - start))
principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])
299/113:

pca = PCA(n_components=3)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])
299/114:

pca = PCA(n_components=3)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

principal
299/115:

pca = PCA(n_components=10)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

principal
299/116:
pca = PCA(n_components=9)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

principal
299/117:
pca = PCA(n_components=8)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

principal
299/118:
pca = PCA(n_components=5)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

principal
299/119:
pca = PCA(n_components=4)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

principal
299/120:
pca = PCA(n_components=3)
principalComponents = pca.fit_transform(x)

principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

principal
299/121:
from sklearn.decomposition import PCA
pca1 = PCA(n_components=5)
df3 = pca1().fit_transform(Std_x)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,4,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/122:
%%timeit
from sklearn.decomposition import PCA

pca = PCA().fit(Std_x)

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,784,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/123:
%%timeit
from sklearn.decomposition import PCA

pca = PCA().fit(x)

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,784,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/124: plot_2d(principalComponents[:, 0],principalComponents[:, 1])
299/125:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld
from sklearn.manifold import TSNE
import umap
299/126: !pip install umap
299/127:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld
from sklearn.manifold import TSNE
import umap
299/128:
from sklearn.decomposition import PCA
pca1 = PCA(n_components=5)
df3 = pca1.fit_transform(Std_x)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,4,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/129:
from sklearn.decomposition import PCA
pca1 = PCA(n_components=5)
df3 = pca1.fit_transform(x)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,4,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/130:
%%time
ld = ld().fit(x)
299/131:
%%time
lda = ld().fit(x)
299/132:
%%time
lda = ld().fit(x,y)
299/133:
%%time
lda = ld().fit(x,d)
299/134: lda
299/135:

plt.plot(np.cumsum(lda.explained_variance_ratio_))
plt.xlim(0,784,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/136:

plt.plot(np.cumsum(lda.explained_variance_ratio_))
plt.xlim(0,70,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
299/137:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld
from sklearn.manifold import TSNE
import umap

# For plotting
import plotly.io as plt_io
import plotly.graph_objects as go
%matplotlib inline
299/138:
def plot_2d(component1, component2):
    
    fig = go.Figure(data=go.Scatter(
        x = component1,
        y = component2,
        mode='markers',
        marker=dict(
            size=20,
            color=y, #set color equal to a variable
            colorscale='Rainbow', # one of plotly colorscales
            showscale=True,
            line_width=1
        )
    ))
    fig.update_layout(margin=dict( l=100,r=100,b=100,t=100),width=2000,height=1200)                 
    fig.layout.template = 'plotly_dark'
    
    fig.show()
299/139:
def plot_3d(component1,component2,component3):
fig = go.Figure(data=[go.Scatter3d(
        x=component1,
        y=component2,
        z=component3,
        mode='markers',
        marker=dict(
            size=10,
            color=y,                # set color to an array/list of desired values
            colorscale='Rainbow',   # choose a colorscale
            opacity=1,
            line_width=1
        )
    )])
# tight layout
    fig.update_layout(margin=dict(l=50,r=50,b=50,t=50),width=1800,height=1000)
    fig.layout.template = 'plotly_dark'
    
    fig.show()
299/140:
def plot_3d(component1,component2,component3):
    fig = go.Figure(data=[go.Scatter3d(
        x=component1,
        y=component2,
        z=component3,
        mode='markers',
        marker=dict(
            size=10,
            color=y,                # set color to an array/list of desired values
            colorscale='Rainbow',   # choose a colorscale
            opacity=1,
            line_width=1
        )
    )])
# tight layout
    fig.update_layout(margin=dict(l=50,r=50,b=50,t=50),width=1800,height=1000)
    fig.layout.template = 'plotly_dark'
    
    fig.show()
299/141: plot_2d(lda[:, 0],lda[:, 1])
299/142: X_LDA = LDA(n_components=3).fit_transform(x,y)
299/143: X_LDA = lda(n_components=3).fit_transform(x,y)
299/144: plot_2d(lda)
299/145: plot_2d(lda,lda)
299/146: plot_2d(lda[:, 0],lda[:, 1])
299/147:
%%time
lda = ld().fit_transform(x,d)
299/148: plot_2d(lda[:, 0],lda[:, 1])
299/149: lda.iplot()
299/150:
%%time
lda = ld(n_components=3).fit_transform(x,d)
299/151: plot_2d(lda[:, 0],lda[:, 1])
299/152:
%%time
lda = ld(n_components=3).fit_transform(x,d)
299/153: plot_2d(lda[:, 0],lda[:, 1])
299/154: plot_2d(lda[:, 0],lda[:, 3])
299/155: plot_2d(lda[:, 0],lda[:, 1])
299/156: plot_2d(lda[1:, 0],lda[:, 1])
299/157: plot_2d(lda[1:, 1],lda[1:, 1])
299/158: plot_2d(lda[1:, 0],lda[:, 3])
299/159: plot_2d(lda[1:, 0],lda[:, 2])
299/160: plot_2d(lda[1:, 0],lda[:, 3])
299/161: plot_2d(lda[3:, 0],lda[:, 3])
299/162: plot_2d(lda[3:, 0],lda[:, 4])
299/163:
%%time
lda = ld(n_components=4).fit_transform(x,d)
299/164: plot_2d(lda[:, 0],lda[:, 4])
299/165: plot_2d(lda[:, 0],lda[:, 3])
299/166: plot_2d(lda[1:, 0],lda[:, 3])
299/167: plot_2d(lda[1:, 1],lda[:, 3])
299/168: plot_2d(lda[1:, 1],lda[1:, 3])
299/169: plot_2d(lda[1:, 0],lda[1:, 3])
299/170: plot_2d(lda[:,],lda[1:, 3])
299/171: plot_2d(lda[:,2],lda[1:, 3])
299/172:
lda = LatentDirichletAllocation(n_components=2)
X_feature_reduced = lda.fit(x).transform(x)
plt.scatter(X_feature_reduced[:,0],X_feature_reduced[:,1],c=target)
plt.title('LDA')
plt.show()
299/173:
lda = LatentDirichletAllocation(n_components=2)
X_feature_reduced = lda.fit(x).transform(x)
plt.scatter(X_feature_reduced[:,0],X_feature_reduced[:,1],c=target)
plt.title('LDA')
plt.show()
299/174:
from sklearn.decomposition import PCA,LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=2)
X_feature_reduced = lda.fit(x).transform(x)
plt.scatter(X_feature_reduced[:,0],X_feature_reduced[:,1],c=target)
plt.title('LDA')
plt.show()
299/175:
from sklearn.decomposition import PCA,LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=2)
X_feature_reduced = lda.fit_transform(x)
plt.scatter(X_feature_reduced[:,0],X_feature_reduced[:,1],c=target)
plt.title('LDA')
plt.show()
299/176:
lda = LDA(n_components=5)
# Taking in as second argument the Target as labels
X_LDA_2D = lda.fit_transform(x, d.values )
299/177:
lda = LDA(n_components=5)
# Taking in as second argument the Target as labels
X_LDA_2D = ld.fit_transform(x, d.values )
299/178:
lda = LD(n_components=5)
# Taking in as second argument the Target as labels
X_LDA_2D = ld.fit_transform(x, d.values )
299/179:
lda = ld(n_components=5)
# Taking in as second argument the Target as labels
X_LDA_2D = ld.fit_transform(x, d.values )
299/180:
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=5)
# Taking in as second argument the Target as labels
X_LDA_2D = ld.fit_transform(x, d.values )
299/181:
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=5)
# Taking in as second argument the Target as labels
X_LDA_2D = LDA.fit_transform(x, d.values )
299/182:
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=5)
# Taking in as second argument the Target as labels
X_LDA_2D = lda.fit_transform(x, d.values )
299/183:
# Using the Plotly library again
traceLDA = go.Scatter(
    x = X_LDA_2D[:,0],
    y = X_LDA_2D[:,1],
#     name = Target,
#     hoveron = Target,
    mode = 'markers',
    text = Target,
    showlegend = True,
    marker = dict(
        size = 8,
        color = Target,
        colorscale ='Jet',
        showscale = False,
        line = dict(
            width = 2,
            color = 'rgb(255, 255, 255)'
        ),
        opacity = 0.8
    )
)
data = [traceLDA]

layout = go.Layout(
    title= 'Linear Discriminant Analysis (LDA)',
    hovermode= 'closest',
    xaxis= dict(
         title= 'First Linear Discriminant',
        ticklen= 5,
        zeroline= False,
        gridwidth= 2,
    ),
    yaxis=dict(
        title= 'Second Linear Discriminant',
        ticklen= 5,
        gridwidth= 2,
    ),
    showlegend= False
)

fig = dict(data=data, layout=layout)
py.iplot(fig, filename='styled-scatter')
299/184:
# Using the Plotly library again
traceLDA = go.Scatter(
    x = X_LDA_2D[:,0],
    y = X_LDA_2D[:,1],
#     name = Target,
#     hoveron = Target,
    mode = 'markers',
    text = Target,
    showlegend = True,
    marker = dict(
        size = 8,
        color = d,
        colorscale ='Jet',
        showscale = False,
        line = dict(
            width = 2,
            color = 'rgb(255, 255, 255)'
        ),
        opacity = 0.8
    )
)
data = [traceLDA]

layout = go.Layout(
    title= 'Linear Discriminant Analysis (LDA)',
    hovermode= 'closest',
    xaxis= dict(
         title= 'First Linear Discriminant',
        ticklen= 5,
        zeroline= False,
        gridwidth= 2,
    ),
    yaxis=dict(
        title= 'Second Linear Discriminant',
        ticklen= 5,
        gridwidth= 2,
    ),
    showlegend= False
)

fig = dict(data=data, layout=layout)
py.iplot(fig, filename='styled-scatter')
299/185:
# Using the Plotly library again
traceLDA = go.Scatter(
    x = X_LDA_2D[:,0],
    y = X_LDA_2D[:,1],
#     name = Target,
#     hoveron = Target,
    mode = 'markers',
    
    showlegend = True,
    marker = dict(
        size = 8,
        color = d,
        colorscale ='Jet',
        showscale = False,
        line = dict(
            width = 2,
            color = 'rgb(255, 255, 255)'
        ),
        opacity = 0.8
    )
)
data = [traceLDA]

layout = go.Layout(
    title= 'Linear Discriminant Analysis (LDA)',
    hovermode= 'closest',
    xaxis= dict(
         title= 'First Linear Discriminant',
        ticklen= 5,
        zeroline= False,
        gridwidth= 2,
    ),
    yaxis=dict(
        title= 'Second Linear Discriminant',
        ticklen= 5,
        gridwidth= 2,
    ),
    showlegend= False
)

fig = dict(data=data, layout=layout)
py.iplot(fig, filename='styled-scatter')
299/186:
# Using the Plotly library again
traceLDA = go.Scatter(
    x = X_LDA_2D[:,0],
    y = X_LDA_2D[:,1],
#     name = Target,
#     hoveron = Target,
    mode = 'markers',
    
    showlegend = True,
    marker = dict(
        size = 8,
        color = d,
        colorscale ='Jet',
        showscale = False,
        line = dict(
            width = 2,
            color = 'rgb(255, 255, 255)'
        ),
        opacity = 0.8
    )
)
data = [traceLDA]

layout = go.Layout(
    title= 'Linear Discriminant Analysis (LDA)',
    hovermode= 'closest',
    xaxis= dict(
         title= 'First Linear Discriminant',
        ticklen= 5,
        zeroline= False,
        gridwidth= 2,
    ),
    yaxis=dict(
        title= 'Second Linear Discriminant',
        ticklen= 5,
        gridwidth= 2,
    ),
    showlegend= False
)

fig = dict(data=x, layout=layout)
py.iplot(fig, filename='styled-scatter')
299/187:
# Using the Plotly library again
traceLDA = go.Scatter(
    x = X_LDA_2D[:,0],
    y = X_LDA_2D[:,1],
#     name = Target,
#     hoveron = Target,
    mode = 'markers',
    
    showlegend = True,
    marker = dict(
        size = 8,
        color = d,
        colorscale ='Jet',
        showscale = False,
        line = dict(
            width = 2,
            color = 'rgb(255, 255, 255)'
        ),
        opacity = 0.8
    )
)
data = [traceLDA]

layout = go.Layout(
    title= 'Linear Discriminant Analysis (LDA)',
    hovermode= 'closest',
    xaxis= dict(
         title= 'First Linear Discriminant',
        ticklen= 5,
        zeroline= False,
        gridwidth= 2,
    ),
    yaxis=dict(
        title= 'Second Linear Discriminant',
        ticklen= 5,
        gridwidth= 2,
    ),
    showlegend= False
)

fig = dict(data=data, layout=layout)
py.iplot(fig, filename='styled-scatter')
299/188:
# Using the Plotly library again
import plotly.offline as py
traceLDA = go.Scatter(
    x = X_LDA_2D[:,0],
    y = X_LDA_2D[:,1],
#     name = Target,
#     hoveron = Target,
    mode = 'markers',
    
    showlegend = True,
    marker = dict(
        size = 8,
        color = d,
        colorscale ='Jet',
        showscale = False,
        line = dict(
            width = 2,
            color = 'rgb(255, 255, 255)'
        ),
        opacity = 0.8
    )
)
data = [traceLDA]

layout = go.Layout(
    title= 'Linear Discriminant Analysis (LDA)',
    hovermode= 'closest',
    xaxis= dict(
         title= 'First Linear Discriminant',
        ticklen= 5,
        zeroline= False,
        gridwidth= 2,
    ),
    yaxis=dict(
        title= 'Second Linear Discriminant',
        ticklen= 5,
        gridwidth= 2,
    ),
    showlegend= False
)

fig = dict(data=data, layout=layout)
py.iplot(fig, filename='styled-scatter')
299/189:
from sklearn.decomposition import PCA,LatentDirichletAllocation
lda = LatentDirichletAllocation(n_components=2)
X_feature_reduced = lda.fit_transform(x)[11:19 PM, 2/4/2021] Amoolya G: pca1 = PCA(n_components=10)
df5=pca1.fit_transform(Std_x)

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,10,1)
plt.xlabel('Number of components')
plt.ylabel('cumulative explained variance')
explained_variance = pca1.explained_variance_ratio_
explained_variance
plt.bar(range(10),explained_variance, alpha=0.5, align='center', label='abc')
plt.legend(loc='best')
plt.tight_layout()
plt.scatter(X_feature_reduced[:,0],X_feature_reduced[:,1],c=target)
plt.title('LDA')
plt.show()
300/1:
# I# Invoking the t-SNE method
tsne = TSNE(n_components=2)
tsne_results = tsne.fit_transform(X_std)
300/2:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld
from sklearn.manifold import TSNE
import umap

# For plotting
import plotly.io as plt_io
import plotly.graph_objects as go
%matplotlib inline
300/3: mst = pd.read_csv('mnist_train.csv')
300/4:
c = mst.drop("label",1)
d = mst["label"]
300/5:
# I# Invoking the t-SNE method
tsne = TSNE(n_components=2)
tsne_results = tsne.fit_transform(x)
300/6:
# I# Invoking the t-SNE method
tsne = TSNE(n_components=2)
tsne_results = tsne.fit_transform(x)
300/7:
%%time
x = StandardScaler().fit_transform(c)
x=pd.DataFrame(x)
x.columns=c.columns
x.head()
300/8:
# I# Invoking the t-SNE method
tsne = TSNE(n_components=2)
tsne_results = tsne.fit_transform(x)
301/1:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld
from sklearn.manifold import TSNE
import umap

# For plotting
import plotly.io as plt_io
import plotly.graph_objects as go
%matplotlib inline
301/2: mst = pd.read_csv('mnist_train.csv')
301/3:
c = mst.drop("label",1)
d = mst["label"]
301/4:
%%time
x = StandardScaler().fit_transform(c)
x=pd.DataFrame(x)
x.columns=c.columns
x.head()
301/5:
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(x.head())
301/6: embedding = umap.UMAP(densmap=True).fit_transform(x)
301/7:
import umap
import umap.plot
from sklearn.datasets import load_digits

digits = load_digits()

mapper = umap.UMAP().fit(x.data)
umap.plot.points(mapper, labels=digits.target)
301/8: !pip install umap.plot
301/9:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/10:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.get(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/11: !pip install umap-learn
301/12:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/13:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/14:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/15:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/16: !pip install umap-learn[plot]
301/17:
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(c.head())
df6= c.head()
df6['tsne-2d-one'] = tsne_results[:,0]
df6['tsne-2d-two'] = tsne_results[:,1]
plt.figure(figsize=(16,10))
sns.scatterplot(
    x="tsne-2d-one", y="tsne-2d-two",
    hue="tsne-2d-two",
    
    data=df6,
    legend="full",


   
)
301/18:
tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)
tsne_results = tsne.fit_transform(x.head())
df6= c.head()
df6['tsne-2d-one'] = tsne_results[:,0]
df6['tsne-2d-two'] = tsne_results[:,1]
plt.figure(figsize=(16,10))
sns.scatterplot(
    x="tsne-2d-one", y="tsne-2d-two",
    hue="tsne-2d-two",
    
    data=df6,
    legend="full",


   
)
301/19:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/20:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld
from sklearn.manifold import TSNE

# For plotting
import plotly.io as plt_io
import plotly.graph_objects as go
%matplotlib inline
301/21:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/22:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/23:
import umap
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.umap_(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/24:
from umap import UMAP
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/25:
from umap import UMAP
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/26:
from umap import UMAP
from sklearn.datasets import load_digits

digits = load_digits()

embedding = umap.UMAP(n_neighbors=5,
                      min_dist=0.3,
                      metric='correlation').fit_transform(x.data)
301/27:
import umap
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(context="paper", style="white")

mnist = fetch_openml("mnist_784", version=1)

reducer = umap.UMAP(random_state=42)
embedding = reducer.fit_transform(x.data)

fig, ax = plt.subplots(figsize=(12, 10))
color = mnist.target.astype(int)
plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap="Spectral", s=0.1)
plt.setp(ax, xticks=[], yticks=[])
plt.title("MNIST data embedded into two dimensions by UMAP", fontsize=18)

plt.show()
301/28:
import umap
reducer = umap.UMAP()
302/1:
import umap
reducer = umap.UMAP()
302/2:
import umap
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(context="paper", style="white")

mnist = fetch_openml("mnist_784", version=1)

reducer = umap.UMAP(random_state=42)
embedding = reducer.fit_transform(mnist.data)

fig, ax = plt.subplots(figsize=(12, 10))
color = mnist.target.astype(int)
plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap="Spectral", s=0.1)
plt.setp(ax, xticks=[], yticks=[])
plt.title("MNIST data embedded into two dimensions by UMAP", fontsize=18)

plt.show()
302/3:
import umap
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(context="paper", style="white")

#mnist = fetch_openml("mnist_784", version=1)

#reducer = umap.UMAP(random_state=42)
embedding = reducer.fit_transform(x.head().data)

fig, ax = plt.subplots(figsize=(12, 10))
color = mnist.target.astype(int)
plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap="Spectral", s=0.1)
plt.setp(ax, xticks=[], yticks=[])
plt.title("MNIST data embedded into two dimensions by UMAP", fontsize=18)

plt.show()
302/4: mst = pd.read_csv('mnist_train.csv')
302/5:
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import cufflinks as cf
cf.go_offline()
import dtale
from sklearn.preprocessing import StandardScaler 
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as ld
from sklearn.manifold import TSNE

# For plotting
import plotly.io as plt_io
import plotly.graph_objects as go
%matplotlib inline
302/6: mst = pd.read_csv('mnist_train.csv')
302/7:
c = mst.drop("label",1)
d = mst["label"]
302/8:
%%time
x = StandardScaler().fit_transform(c)
x=pd.DataFrame(x)
x.columns=c.columns
x.head()
302/9:
import umap
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(context="paper", style="white")

#mnist = fetch_openml("mnist_784", version=1)

#reducer = umap.UMAP(random_state=42)
embedding = reducer.fit_transform(x.head().data)

fig, ax = plt.subplots(figsize=(12, 10))
color = mnist.target.astype(int)
plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap="Spectral", s=0.1)
plt.setp(ax, xticks=[], yticks=[])
plt.title("MNIST data embedded into two dimensions by UMAP", fontsize=18)

plt.show()
302/10:
import umap
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(context="paper", style="white")

#mnist = fetch_openml("mnist_784", version=1)

#reducer = umap.UMAP(random_state=42)
embedding = reducer.fit_transform(x.head())

fig, ax = plt.subplots(figsize=(12, 10))
color = mnist.target.astype(int)
plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap="Spectral", s=0.1)
plt.setp(ax, xticks=[], yticks=[])
plt.title("MNIST data embedded into two dimensions by UMAP", fontsize=18)

plt.show()
302/11:
import umap
from sklearn.datasets import fetch_openml
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(context="paper", style="white")

#mnist = fetch_openml("mnist_784", version=1)

#reducer = umap.UMAP(random_state=42)
embedding = reducer.fit_transform(x.head())

fig, ax = plt.subplots(figsize=(12, 10))
#color = mnist.target.astype(int)
plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap="Spectral", s=0.1)
plt.setp(ax, xticks=[], yticks=[])
plt.title("MNIST data embedded into two dimensions by UMAP", fontsize=18)

plt.show()
302/12:
import umap
reducer = umap.UMAP()
embedding = reducer.fit_transform(x.head())
302/13: embedding
302/14: import umap.plot
302/15: umap.plot.points(embedding)
302/16: q= pd.DataFrame(embedding)
302/17: umap.plot.points(q)
302/18: q.iplot()
302/19: q.iplot(kind ='bar')
302/20: q.iplot(kind ='bubble')
302/21: q.iplot(kind ='bubble3d')
302/22: q.iplot(kind ='choroplet')
302/23: q.iplot(kind ='spread')
302/24: q.iplot(kind ='scatter3d')
302/25: q.iplot(kind ='bar')
302/26: q.iplot(kind ='scattergeo')
302/27: q.iplot(kind ='ohlc')
302/28: q.iplot(kind ='candle')
302/29: q.iplot(kind ='scatter',mode='markers')
302/30:
import umap
reducer = umap.UMAP()
embedding = reducer.fit_transform(x)
302/31: q= pd.DataFrame(embedding)
302/32: q.iplot(kind ='scatter',mode='markers')
302/33:
plt.scatter(
    embedding[:, 0],
    embedding[:, 1],
plt.gca().set_aspect('equal', 'datalim')
plt.title('UMAP projection of the Penguin dataset', fontsize=24)
302/34:
plt.scatter(
    embedding[:, 0],
    embedding[:, 1],
plt.gca().set_aspect('equal', 'datalim')
302/35:
plt.scatter(
    embedding[:, 0],
    embedding[:, 1],
plt.gca().set_aspect('equal', 'datalim'))
304/1:
import pandas as pd
import cufflinks as cf
cf.go_offline()
304/2:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
 if(keyword.iskeyword(word)):
 print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
 print("You Won")
else:
 print("better luck next time, The number is:",num)
304/3:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word)):
    print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/4:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word)):
    print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/5:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word))
    print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/6:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word))
    print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/7:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word))
    print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/8:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word))
    print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/9:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word)):
    print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/10:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word)):
        print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/11:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word)):
        print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
304/12:
import keyword
array = ["sub" , "yes" , "if" , "no", "while", "ccd", "dowhile"]
for word in array:
    if(keyword.iskeyword(word)):
        print(word)
import random
num = random.randint(0,6)
choice = int(input("Guess The Score: "))
if num ==choice:
    print("You Won")
else:
    print("better luck next time, The number is:",num)
306/1:
import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile

from distutils.version import StrictVersion
from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image

# This is needed since the notebook is stored in the object_detection folder.
sys.path.append("..")
from object_detection.utils import ops as utils_ops

if StrictVersion(tf.__version__) < StrictVersion('1.9.0'):
  raise ImportError('Please upgrade your TensorFlow installation to v1.9.* or later!')
308/1:
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from matplotlib import style
# setting style for graphs
style.use('ggplot')
plt.rcParams['figure.figsize'] = (20,10)
308/2:
df = pd.read_excel('Canada.xlsx',1, skiprows = range(20), skipfooter = 2)

df.drop(['AREA','REG','DEV','Type','Coverage','DevName'], axis=1, inplace=True)
df.rename(columns = {'OdName':'country','AreaName':'continent','RegName':'region'}, inplace = True)
df['total'] = df.sum(axis = 1)
df = df.set_index('country')
df.rename(index = {'United Kingdom of Great Britain and Northern Ireland':'UK & Ireland'}, inplace = True)
df.columns = df.columns.astype(str)
# Useful for upcoming visualizations
years = list(map(str, range(1980,2013)))
309/1:
import pandas as pd
import numpy as np
import seaborn as sb
import matplotlib.pyplot as plt
from matplotlib import style
# setting style for graphs
style.use('ggplot')
plt.rcParams['figure.figsize'] = (20,10)
309/2:
df = pd.read_excel('Canada.xlsx',1, skiprows = range(20), skipfooter = 2)

df.drop(['AREA','REG','DEV','Type','Coverage','DevName'], axis=1, inplace=True)
df.rename(columns = {'OdName':'country','AreaName':'continent','RegName':'region'}, inplace = True)
df['total'] = df.sum(axis = 1)
df = df.set_index('country')
df.rename(index = {'United Kingdom of Great Britain and Northern Ireland':'UK & Ireland'}, inplace = True)
df.columns = df.columns.astype(str)
# Useful for upcoming visualizations
years = list(map(str, range(1980,2013)))
309/3:
df = pd.read_excel('Canada.xlsx',1, skiprows = range(20), skipfooter = 2)

df.drop(['AREA','REG','DEV','Type','Coverage','DevName'], axis=1, inplace=True)
df.rename(columns = {'OdName':'country','AreaName':'continent','RegName':'region'}, inplace = True)
df['total'] = df.sum(axis = 1)
df = df.set_index('country')
df.rename(index = {'United Kingdom of Great Britain and Northern Ireland':'UK & Ireland'}, inplace = True)
df.columns = df.columns.astype(str)
# Useful for upcoming visualizations
years = list(map(str, range(1980,2013)))
309/4:
df1=pd.read_excel(
     os.path.join(APP_PATH, "Data", "Canada.xlsx"),
     engine='openpyxl',
)

#df = pd.read_excel('Canada.xlsx',1, skiprows = range(20), skipfooter = 2)

df.drop(['AREA','REG','DEV','Type','Coverage','DevName'], axis=1, inplace=True)
df.rename(columns = {'OdName':'country','AreaName':'continent','RegName':'region'}, inplace = True)
df['total'] = df.sum(axis = 1)
df = df.set_index('country')
df.rename(index = {'United Kingdom of Great Britain and Northern Ireland':'UK & Ireland'}, inplace = True)
df.columns = df.columns.astype(str)
# Useful for upcoming visualizations
years = list(map(str, range(1980,2013)))
309/5:
import os
df1=pd.read_excel(
     os.path.join(APP_PATH, "Data", "Canada.xlsx"),
     engine='openpyxl',
)

#df = pd.read_excel('Canada.xlsx',1, skiprows = range(20), skipfooter = 2)

df.drop(['AREA','REG','DEV','Type','Coverage','DevName'], axis=1, inplace=True)
df.rename(columns = {'OdName':'country','AreaName':'continent','RegName':'region'}, inplace = True)
df['total'] = df.sum(axis = 1)
df = df.set_index('country')
df.rename(index = {'United Kingdom of Great Britain and Northern Ireland':'UK & Ireland'}, inplace = True)
df.columns = df.columns.astype(str)
# Useful for upcoming visualizations
years = list(map(str, range(1980,2013)))
309/6:
import os
df1=pd.read_excel(
     os.path.join(APP_PATH, "Data", "Canada.xlsx"),
     engine='openpyxl',
)

#df = pd.read_excel('Canada.xlsx',1, skiprows = range(20), skipfooter = 2)

df.drop(['AREA','REG','DEV','Type','Coverage','DevName'], axis=1, inplace=True)
df.rename(columns = {'OdName':'country','AreaName':'continent','RegName':'region'}, inplace = True)
df['total'] = df.sum(axis = 1)
df = df.set_index('country')
df.rename(index = {'United Kingdom of Great Britain and Northern Ireland':'UK & Ireland'}, inplace = True)
df.columns = df.columns.astype(str)
# Useful for upcoming visualizations
years = list(map(str, range(1980,2013)))
309/7:
import os
df1=pd.read_excel(
     os.path.join(APP_PATH, "Data", "Canada.xlsx"),
     engine='openpyxl',)

#df = pd.read_excel('Canada.xlsx',1, skiprows = range(20), skipfooter = 2)

df.drop(['AREA','REG','DEV','Type','Coverage','DevName'], axis=1, inplace=True)
df.rename(columns = {'OdName':'country','AreaName':'continent','RegName':'region'}, inplace = True)
df['total'] = df.sum(axis = 1)
df = df.set_index('country')
df.rename(index = {'United Kingdom of Great Britain and Northern Ireland':'UK & Ireland'}, inplace = True)
df.columns = df.columns.astype(str)
# Useful for upcoming visualizations
years = list(map(str, range(1980,2013)))
314/1:
import numpy as np 
data = [1, 2, 2, 2, 3, 1, 1, 15, 2, 2, 2, 3, 1, 1, 2] 
mean = np.mean(data) 
std = np.std(data) 
print('mean of the dataset is', mean) 
print('std. deviation is', std) 
threshold = 3
outlier = [] 
for i in data: 
    z = (i-mean)/std 
    if z > threshold: 
        outlier.append(i) 
print('outlier in dataset is', outlier)
314/2:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import matplotlib
import dataprep 
from dataprep.eda import plot
from dataprep.eda import plot_correlation
cf.go_offline()
314/3:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import matplotlib
cf.go_offline()
314/4: sp=pd.read_csv('Stock_price_Train.csv')
314/5: sp.head()
314/6: sp.describe()
314/7: sp.isnull().sum()
314/8: sp.iplot(kind='box',boxpoints='suspectedoutliers')
314/9: sp.corr()
314/10:
import numpy as np 
data = [1, 2, 2, 2, 3, 1, 1, 15, 2, 2, 2, 3, 1, 1, 2] 
mean = np.mean(data) 
std = np.std(data) 
print('mean of the dataset is', mean) 
print('std. deviation is', std) 
threshold = 3
outlier = [] 
for i in data: 
    z = (i-mean)/std 
    if z > threshold: 
        outlier.append(i) 
print('outlier in dataset is', outlier)
314/11:
#find Q1, Q3, and interquartile range for each column
Q1 = sp.quantile(q=.25)
Q3 = sp.quantile(q=.75)
IQR = sp.apply(stats.iqr)

#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
sp_clean = sp[~((sp < (Q1-1.5*IQR)) | (sp > (Q3+1.5*IQR))).any(axis=1)]

#find how many rows are left in the dataframe 
sp_clean.shape
314/12:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import scipy.stats as stats
import matplotlib
cf.go_offline()
314/13:

#find Q1, Q3, and interquartile range for each column
Q1 = sp.quantile(q=.25)
Q3 = sp.quantile(q=.75)
IQR = sp.apply(stats.iqr)

#only keep rows in dataframe that have values within 1.5*IQR of Q1 and Q3
sp_clean = sp[~((sp < (Q1-1.5*IQR)) | (sp > (Q3+1.5*IQR))).any(axis=1)]

#find how many rows are left in the dataframe 
sp_clean.shape
314/14: sp.type
314/15: sp.Description
314/16: sp.inf0()
314/17: sp.info()
314/18:
#find absolute value of z-score for each observation
z = np.abs(stats.zscore(sp))

#only keep rows in dataframe with all z-scores less than absolute value of 3 
data_clean = sp[(z<3).all(axis=1)]

#find how many rows are left in the dataframe 
data_clean.shape

(99,3)
314/19:
df1 = sp[~sp.groupby('unitprice').transform( lambda x: abs(x-x.mean()) > 1.96*x.std()).values]
print (df1)
314/20:
df1 = sp[~sp.groupby('UnitPrice').transform( lambda x: abs(x-x.mean()) > 1.96*x.std()).values]
print (df1)
314/21: df1.iplot(kind='box',boxpoints='suspectedoutliers')
316/1:
import pandas as pd
import numpy as np
import cufflinks as cf
import seaborn as sns
import scipy.stats as stats
import matplotlib
cf.go_offline()
316/2: sp=pd.read_csv('Stock_price_Train.csv')
316/3: sp.shape()
316/4: sp.shape
316/5: sp.head()
316/6: sp.describe()
316/7: sp.isnull().sum()
316/8: sp.iplot(kind='box',boxpoints='suspectedoutliers')
316/9: sp.corr()
316/10: sp.info()
   1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import cufflinks as cf
cf.go_offline()
   2: dataset = sns.load_dataset("iris")
   3: dataset.value_counts()
   4: dataset.iplot(x='species',y='petal_length',kind ="box",categories='myGroup')
   5: _ih[-15:]
   6: _ih[-15:]
   7: _ih[-14:]
   8: _ih[-13:]
   9: _ih[-11:]
  10: -z
  11: %history -g -f notebook_file.ipynb
